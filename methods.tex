%In order to evaluate an update summarization system, we adopt the simulation-based approach used in the TREC Temporal Summarization (TS) Track.  We provide a brief overview of the problem.  Details on the formulation can be found in the track overview paper \cite{aslam2013trec}.  

An update summarization system takes as input 
\begin{enumerate*}[label=\itshape\alph*\upshape)]
  \item a short query defining the event to be tracked (e.g. `Hurricane Sandy'), 
  \item an event category defining the type of event to be tracked (e.g. `hurricane'), 
  \item a stream of time-stamped documents %, $(\doc_0, \doc_1,\ldots,\doc_T)$,
  presented in temporal order, and \item an evaluation time period of interest.
    %  $(\stime,\etime)$.  
      \end{enumerate*} While processing documents
      throughout the time period of interest, the system must output sentences,
      known as updates.
      Updates should not only be relevant to the event but also salient,
      i.e. worthy of
      reporting.
      For most domains, updates also need to be 
      timely and novel, i.e. users need to be informed of changes in the 
      event as quickly as possible.
      Finally, in aggregate the updates must comprehensively summarize the 
      event.

In order to quantify the salience of a sentence with respect to an event,
we assume the event can be represented as a set of atomic facts, or nuggets.
Nuggets are brief and important text snippets that represent sub-event that should be conveyed
by an ideal update summary (see Figure~\ref{fig:nuggets} for examples).
\setlength{\fboxsep}{10pt}
\begin{figure}
    \fbox{\parbox{6.9cm}{
        $\cdot$ hurricane force wind warnings are in effect from Rhode Island Sound to Chincoteague Bay\\
        
$\cdot$ Obama declared an emergency in Maryland and signed an order authorized the Federal Emergency Management Agency to aid in disaster relief\\

        $\cdot$ over 5000 commercial airline flights scheduled for October 28 and October 29 were cancelled
 }}
 \caption{Example nuggets from Hurricane Sandy.\label{fig:nuggets}}
\end{figure}

Given an update $u$ and a set of nuggets $N$ we define the salience of 
$u$ as,
\[\operatorname{salience}(u) = \operatorname{max}_{n \in N} 
\operatorname{sim}(u, n) \]
where $\operatorname{sim}(\cdot)$ is the semantic similarity such as
the cosine similarity of latent vectors associated with the update and 
nugget text \cite{?}. % We train
% a regression model to predict salience using features derived from
% the updates themselves
% and the document stream.



     % \ref{sec:methods}.  In our version of this problem, we assume that that
     % the system receives one batch of new sentence-segmented documents every
     % hour throughout the period of interest.
\subsection{Update Summarization}

Our system architecture follows a simple pipeline design where each
stage provides an additional level of processing or filtering of the input
sentences.
At the highest level, we begin with an empty update summary $U$.
At each hour we receive a new batch of sentences $b_t$ from the stream
and perform the following actions:
\begin{enumerate}
    \item predict the salience of sentences in $b_t$ (Section~\ref{sec:salpred}),
  \item select a set of exemplar sentences (by combining clustering with 
      salience predictions) (Section~\ref{sec:exsel}),
  \item add the most novel and salient exemplars to $U$ (Section~\ref{sec:upsel}).
\end{enumerate}

The resultant list of updates $U$ is our summary of the event.
Step 2 uses the affinity propagation algorithm to incorporate the salience
predictions from step 1. We refer to this summarization 
system as the \textbf{AP+Salience} model.


%\kmcomment{This was as best as I could do on this. These are the example
%    categories from the wikipedia page. However you refer in the language model
%    description to the event type earthquake which is more specific. Also, in
%    the TREC data you refer to event types that are specified in the metadata.
%So I'm wondering how many different categories you have and where they come
%from.}

%\fdcomment{include description of nuggets here.} 
%%\kmcomment{We should jointly
%    discuss what goes in data and what goes in problem definition. I have moved
%    event types here because I think knowing how many types and examples of
%    them would be helpful upfront. I think where nuggets and wikipedia pages go
%is questionable.}

%Figure \ref{alg:temporal-summarization} outlines our general update
%summarization algorithm.  At each hour, the system processes each input 
%sentence batch $S_t$.
%We predict the salience $P$ for all input sentences $s\in S_t$.
%Next, we cluster $S_t$ using the AP clustering algorithm, biased by $P$,
%obtaining a set of exemeplar sentences $E$. Finally we select a subset of $E$
%to be updates and add those to our set of summary updates $U$.
%
%\begin{algorithm}%[H]
% \KwData{ 
%     $S_{\stime}, S_{\stime+1},\ldots,S_{\etime} $ --- time ordered sentence
%     batches\\
% \KwResult{U --- a list of updates, i.e. the update summary} 
%}
% ~\\
% Initialize empty list $U$ of updates\\
%    \For{$t \gets \stime,\ldots,\etime $}{
%        $P \gets \operatorname{PredictSalience}(S_t)$\\
%        $E \gets \operatorname{APCluster}(P, S_t)$ \\
%        $U_t \gets \operatorname{SelectUpdates}(E,P,U)$\\
%        $U \gets U \cup U_t$
%}
% \caption{Temporal Summarization Algorithm}\label{alg:temporal-summarization}
%\end{algorithm}

\subsection{Salience Prediction}
\label{sec:salpred}

\subsubsection{Model}

Nuggets represent ground truth data which would not be available at test time;
we need to find some function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ 
such that 
\[ \operatorname{salience}(s) = f(x_s) \]
where $x_s$ is a $d$-dimensional feature vector for sentence $s$. 
Given that we have many thousands of sentences of varying relevance to each
event in our stream and salience scores are derived in an unsupervised fashion
(assuming event nuggets are known), we can generate large amounts of training
data for our model by simply sampling sentences 
from the stream and computing the maximum
semantic similarity to the nugget set.

To learn $f$ and predict salience,
we adopt Gaussian process (GP) regression 
\cite{rasmussen:gaussian-process-book}.  GP regressors are a
class of data-driven, non-parametric model generalizing the multi-variate
Gaussian to the infinite dimensional setting.  GP's are general
and are state of the art for many regression tasks.  
%A full treatment of
%Gaussian process regression is beyond the scope of this article and can be
%found in standard textbooks.


GP's are non-parametric and rely on a covariance matrix $\kernelMatrix$, measuring the similarity between pairs of instances, in our 
case sentences.  In our experiments, we used a radial basis 
function (RBF) kernel.  Given two featurized sentences, 
$\features$ and $\features^\prime$, the RBF kernel is defined as,
\begin{align*}
        \kernelMatrixij{\features}{\features^\prime}&= \sigma^2 \exp\left(- \frac{1}{2} 
\sum_{i=1}^{\numfeatures} \frac{ (\featuresi{i}-\featuresi{i}^\prime)^2}{\ell_i^2} \right)
\end{align*}
where $\sigma$ and the $\ell_i$ are parameters we fit to our observed training data. The $\ell_i$ are feature dependent scaling parameters; once learned, they not only improve the accuracy of the model, but give us some introspection  into which features are more important.
Kernel matrices are also closed under addition, i.e. the sum of kernels
is also a kernel; we exploit this fact by organizing our features into 
meaningful groups with their own RBF kernel using the sum of their kernels 
to parameterize the model.


\begin{figure}[t!]
\begin{tabular}{| l |} 
\hline
\textbf{Basic Features}\\
$\cdot$ Sent. position (normalized by doc. length) \\
$\cdot$ Sent. length \\
$\cdot$ Ratio of punc. to non-punc. chars. \\
$\cdot$ Ratio of caps. to non-caps. chars. \\
$\cdot$ Ratio of lowercase to other chars. \\
$\cdot$ Ratio of uppercase to other chars. \\
$\cdot$ \# of caps. words (normalized by \# of words)\\
$\cdot$ \# of Person, Location, Org. Date, Number,\\
$\;\;$ Ordinal, Percent, Money, Set, Misc  N.E. \\
$\;\;$ tags (normalized by \# of words)\\
\hline
\textbf{Query Features}\\
$\cdot$ \% of query words covered by sent.\\
$\cdot$ Total query matches.\\
$\cdot$ Total event-type synonyms/hypernyms/ \\
$\;\;$ hyponyms coverage.\\
$\cdot$ Total event-type synonyms/hypernyms/ \\
$\;\;$ hyponyms matches.\\
\hline
\textbf{Language Model Features}\\
$\cdot$ Avg. token log probability (domain lang. \\
$\;\;$ model)\\
$\cdot$ Avg. token log probability (background \\
$\;\;$ lang. model)\\
\hline
\textbf{Geo-tag Features}\\
$\cdot$ Median document distance to nearest  \\
$\;\;$ location cluster (current hour).\\
$\cdot$ Median document distance to nearest  \\
$\;\;$ location clusters (previous hour).\\
$\cdot$ Distance of first location in doc. to nearest \\
$\;\;$ location cluster (current hour).\\
$\cdot$ Distance of first location in doc. to nearest \\
$\;\;$ location cluster (previous hour).\\
\hline
\textbf{Temporal Features}\\
$\cdot$ Avg. tf-idf at current time.\\
$\cdot$ Change in avg. tf-idf since previous hour \\
$\;\;$  (up to 24 hours).\\
$\cdot$ Time since query/event start.\\
\hline
\end{tabular}
\caption{Salience model features.}
\end{figure}

\subsubsection{Features}
We want our model to be predictive across different kinds of events so we avoid lexical features.  Instead, we extract a variety of features including language model scores, geographic relevance, and temporal relevance from each sentence.  
%These features are used to fit a Gaussian process regression model that can predict the similarity of a sentence to a gold summary \cite{preotiuc2013temporal}.  

\paragraph{Basic Features}
%KM - Would be good to have quick justification of these features. I added a
%sentence. Feel free to edit or remove.

We employ several basic features that have been used previously in supervised models to rank sentence salience \cite{kupiec1995trainable,conroy2001using}. These include sentence length, the number of capitalized words normalized by sentence length, document position, number of named entities.  
Since training is done on grammatical English, some of these features help
to downweight sentences that are ungrammatical (e.g., have too many capitalized words or are too short).
Others help to more heavily weight important sentences (e.g., that appear in
prominent positions such as paragraph initial or article initial).

\paragraph{Query Features}

Query features measure the relationship between the sentence and the event query and type.  These include the number of query words present in the sentence in addition to the number of event type synonyms, hypernyms, and hyponyms as found in WordNet \cite{miller1995wordnet}.  For example, for event type \emph{earthquake},  we match sentence terms ``quake'', ``temblor'', ``seism'', and ``aftershock''.
\paragraph{Language Model Features}\label{subsubsec:lm}
Language models allow us to measure the likelihood of a sentence having been produced from a particular source.  We consider two types of language model features.  The first model is estimated from a corpus of generic news articles.  This model is intended to assess the general writing quality (grammaticality, word usage) of an input sentence and helps us to filter out text snippets which are not sentences (e.g., web page titles).  The second model is estimated from text specific to our event types.  For example, the language model for event type `earthquake' is estimated from Wikipedia pages under the category \emph{Category:Earthquakes}.  These models are intended to detect sentences similar to those appearing in summaries of other events in the same category (e.g. most earthquake summaries are likely to include higher probability for ngrams including the token `magnitude').  
%KM - Note: Someplace the exact list of event types should appear. Probably not
%here.
%KM - I note you have it in a later section but it is labeled as data you use
%to train language models and semantic similarity. I think it would be good to
%have up front in definition of task.


%For both models, we Finally, we extract the percentage of capitalized words,
%and sentence length as features. These last two features also help to
%identify sentences that are less likely to contain relevant content-- overly
%long and heavily capitalized sentences in our corpus were likely to be long
%strings of web-page headlines, section headers, and other irrelevant page
%structure. 

\paragraph{Geographic Relevance Features}

Locations are identified using a named entity tagger. For each location in a sentence, we obtain its latitude and longitude using the a publicly available geolocation service.  We then compute its distance to that of the event location.  It is possible for a sentence and an event to have multiple locations so we take as features the minimum, maximum, and average distance of all sentence-event location pairs.  Distances are calculated using the Vincenty distance. 
%KM - Probably should say how you determine event location. Some events move.
%KM - In your figure you only include one distance.

\paragraph{Temporal Relevance Features}

Our data consists of hourly crawls of online content and so we exploit the temporality of corpus by capturing the burstiness of a sentence, i.e.  the change in word frequency from one hour to the next.``Bursty'' sentences often indicate new and important data. 

Let $D_t$ be the set of web pages at time $t$ and let $s = \{w_1,\ldots,w_n\}$ be a sentence from a page $d \in D_t$.  We calculate the 1-hour burstiness of sentence $s$ from document $d$ at hour $t$  as 
%\begin{align*}
%\operatorname{b}_1(s,d,t) = \frac{1}{|s|} \sum_{w \in s} \Bigg( &
%\operatorname{tf-idf}_t(w,d)  \\ & \left. - \frac{\sum_{d^\prime \in D_{t-1}:
%w \in d^\prime } \operatorname{tf-idf}_{t-1}(w,d^\prime)}{|\{d^\prime \in
%D_{t-1}: w \in d^\prime\}|} \right) \end{align*}

%where \begin{align*} \operatorname{tf-idf}_t(w,d) =&
%\log\left(1+\sum_{w^\prime \in d}1\{w=w^\prime\}  \right)\\ & \times
%\log\left(\frac{|D_t|}{1 + \sum_{d^\prime \in D_t}1\{w \in d^\prime\}}\right).
%\end{align*}

[\textrm{Simpler explanation goes here}]
% 1\{w = w^\prime} %- \operatorname{avg-tf-idf}_{t_{i-1}}(w).
%\end{align*}


We similarly find the sentence's 5-hour burstiness.  In addition to burstiness, we also include the sentence's average tf-idf and hours since the event in question started as features.


% Formally, let $p(f)$ be a distribution over functions where $f$ is any mapping
% of an input space $\mathcal{X}$ to the reals,
%
% $$f: \mathcal{X} \rightarrow \mathcal{R}.$$
% Let the random variable $\mathbf{f} = (f(x_1),\ldots,f(x_n) )$ be
%  an $n$-dimensional vector whose elements are evaluations of the function $f$
% at points $x_i \in \mathcal{X}$.
% We say $p(f)$ is a Gaussian process if for any finite subset
% $\{x_1,\ldots,x_n\} \subset \mathcal{X}$, the marginal distribution over
% that finite subset $p(\mathbf{f})$ has a multivariate Gaussian distribution.
% A GP is parameterized by a mean function $\mu(\mathbf{x})$ and a
% covariance function $K(x,x^\prime)$. Generally, the mean function is simply
% set to 0, leaving the distribution to be completely characterized by the
% kernel function on the data.
%
% In the regression setting, we typically have a response variable $y$ that
% is the sum of our model prediction  and
% some Gaussian noise, i.e. $y = f(x) + \epsilon$ with
% $\epsilon \sim \mathcal{N}(0, \sigma^2)$. When
% $f \sim \operatorname{GP}(\mathbf{0}, \mathbf{K})$, the
% two distributions
% of principal interest are the marginal likelihood
% $p(\mathbf{y}|\mathbf{X}) =
% \mathcal{N}(\mathbf{0},\mathbf{K} + \sigma^2\mathbf{I})$ and the predictive
% distribution,
%
% $$p(\mathbf{y_*}|\mathbf{x_*},\mathbf{X},\mathbf{y}) =
% \mathcal{N}(\boldsymbol{\mu}_*, \boldsymbol{\sigma}^2_*) $$
%
% where $\mathbf{x_*}$ is a new or unseen input, $\mathbf{y_*}$ our predicted
% response, and
% \begin{align*}
% \boldsymbol{\mu}_* & = \mathbf{K_*}(\mathbf{K} + \sigma^2\mathbf{I})^{-1}\mathbf{y} \\
% \boldsymbol{\sigma}^2_* &
% = \mathbf{K}_{**} - \mathbf{K}_*(\mathbf{K} + \sigma^2\mathbf{I})^{-1}
% \mathbf{K}_*^T + \sigma^2\\
% \end{align*}.
%
% Here $\mathbf{K}_* = K(\mathbf{x}_*, \mathbf{X})$, and
% $\mathbf{K}_{**} = K(\mathbf{x}_*, \mathbf{x}_*)$.
%
%

\subsection{Exemplar Selection}
\label{sec:exsel}

We combine the output of our salience prediction model with the affinity
propagation algorithm to identify a set of exemplar sentences 
for each input batch. 
Affinity propagation (AP) is a clustering algorithm
that identifies a subset of datapoints as exemplars and forms clusters
by assigning the remaining points to one of the exemplars. AP can be seen
as a generalization of other $k$-centers algorithms that attempts to 
maximize the net similarity objective 
\[ \mathcal{S} = \sum_{i : i \neq e_i}^n \operatorname{sim}(i,e_i) 
+ \sum_{i : i = e_i}^n \operatorname{pref}(e_i)  \]
where $e_i$ is the exemplar of the $i$-th data point, and functions
$\operatorname{sim}$ and $\operatorname{pref}$ express the pairwise similarity
of data points and the apriori preference of a data point to be an exemplar
respectively. There are very 
few restrictions on the nature of the similarities and preferences
other than that they be real-valued.

AP has several useful properties and interpretations. Chiefly, the number
of clusters $k$ is not a model hyper-parameter. Given that our task requires
clustering many batches of streaming data, searching for an optimal $k$ 
would be computationally prohibitive. With AP, $k$ is determined by the
similarities and preferences of the data. Generally lower preferences will
result in fewer clusters.  


In our summarization system we use the output of our salience model as 
the preferences, i.e.
$\operatorname{salience}(s) = \operatorname{pref}(s)$; for the similarity 
function we use semantic similarity. Recall that $\operatorname{salience}(s)$
is a prediction of the semantic similarity of $s$ to information about the 
event be summarized, i.e. the set of event nuggets.
Intuitively, when maximizing objective function $\mathcal{S}$, AP must balance
between best representing the input data and representing the most salient
input. Additionally, when the level of input is high but the salience
predictions are low, the preference term will guide AP toward a solution
with fewer clusters; vice-versa when input is very salient on average but
the volume of input is low. The adaptive nature of our model differentiates
our method from most out update summarization systems.




\subsection{Update Selection}
\label{sec:upsel}

After exemplar sentences have been identified, two more filters are applied 
before updating the summary. The exemplars are examined
sequentially in order of highest salience. First, a salience threshold 
is applied, and any exemplars whose salience is less than
$\lambda_{sal}$ are discarded. Second, a similarity threshold 
is applied, where the exemplar is discarded if its
maximum semantic similarity to any previous updates in the summary is
greater than $\lambda_{sim}$. If the exemplar passes these two filters,
it is selected as an update and added to the summary.




