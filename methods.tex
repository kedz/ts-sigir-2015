%In order to evaluate an update summarization system, we adopt the simulation-based approach used in the TREC Temporal Summarization (TS) Track.  We provide a brief overview of the problem.  Details on the formulation can be found in the track overview paper \cite{aslam2013trec}.  

Our update summarization system takes as input 
\begin{enumerate*}[label=\itshape\alph*\upshape)]
  \item a short query defining the event to be tracked (e.g. `Hurricane Sandy'), 
  \item an event category defining the type of event to be tracked (e.g. `hurricane'), 
  \item a stream of time-stamped documents %, $(\doc_0, \doc_1,\ldots,\doc_T)$,
  presented in temporal order, and \item an evaluation time period of interest.
    %  $(\stime,\etime)$.  
\end{enumerate*} 
While processing documents
throughout the time period of interest, the system outputs sentences
from these documents likely to be useful to query issuer.  We refer
to these selected sentences as \emph{updates}.

In order to measure the usefulness of a system's updates, we consider
the degree to which the system output reflects the different
aspects of an event.  Events are often composed of a variety of sub-events.  
For example, the Hurricane Sandy
event includes sub-events related to the storm making landfall,
the ensuing flooding, the many transportation issues, amongst many
others.  An ideal system would update the user about each of these
sub-events as they occur, with low latency.  In order to use
language consistent with previous literature, we refer to these
sub-events as the \emph{nuggets} associated with an event.  Nuggets are
defined as a fine-grained atomic sub-event associated with an event.  
We present several example nuggets associated with the Hurricane
Sandy event in Figure \ref{fig:nuggets}.  We describe how these 
nuggets are found in Section \ref{sec:data}.

% Updates should not only be relevant to the event but also salient,
% i.e. worthy of
% reporting.
% For most domains, updates also need to be
% timely and novel, i.e. users need to be informed of changes in the
% event as quickly as possible.
% Finally, in aggregate the updates must comprehensively summarize the
% event.

\begin{figure}
\setlength{\fboxsep}{10pt}
    \fbox{\parbox{6.9cm}{
        -- hurricane force wind warnings are in effect from Rhode Island Sound to Chincoteague Bay

        -- Obama declared an emergency in Maryland and signed an order authorized the Federal Emergency Management Agency to aid in disaster relief

        -- over 5000 commercial airline flights scheduled for October 28 and October 29 were cancelled
 }}
 \caption{Example nuggets from Hurricane Sandy.\label{fig:nuggets}}
\end{figure}


Throughout our treatment of our algorithm, the \emph{salience} 
of an update captures the degree to which it reflects an 
event's nuggets.  Assuming that we have a text representation 
of our nuggets, the salience of an update $u$  with respect to a set of nuggets $N$ is
defined as,
\begin{align}
\operatorname{salience}(u) = \operatorname{max}_{n \in N} 
\operatorname{sim}(u, n) \ref{eq:salience}
\end{align}
where $\operatorname{sim}(\cdot)$ is the semantic similarity such as
the cosine similarity of latent vectors associated with the update and 
nugget text \cite{?}. % We train
% a regression model to predict salience using features derived from
% the updates themselves
% and the document stream.



     % \ref{sec:methods}.  In our version of this problem, we assume that that
     % the system receives one batch of new sentence-segmented documents every
     % hour throughout the period of interest.
\subsection{Update Summarization}

Our system architecture follows a simple pipeline design where each
stage provides an additional level of processing or filtering of the input
sentences.
We begin with an empty update summary $U$.
At each hour we receive a new batch of sentences $b_t$ from the stream
and perform the following actions:
\begin{enumerate}
    \item predict the salience of sentences in $b_t$ (Section~\ref{sec:salpred}),
  \item select a set of exemplar sentences  in $b_t$ by combining clustering with 
      salience predictions (Section~\ref{sec:exsel}),
  \item add the most novel and salient exemplars to $U$ (Section~\ref{sec:upsel}).
\end{enumerate}

The resultant list of updates $U$ is our summary of the event.
% Step 2 uses the affinity propagation algorithm to incorporate the salience
% predictions from step 1. We refer to this summarization
% system as the \textbf{AP+Salience} model.


%\kmcomment{This was as best as I could do on this. These are the example
%    categories from the wikipedia page. However you refer in the language model
%    description to the event type earthquake which is more specific. Also, in
%    the TREC data you refer to event types that are specified in the metadata.
%So I'm wondering how many different categories you have and where they come
%from.}

%\fdcomment{include description of nuggets here.} 
%%\kmcomment{We should jointly
%    discuss what goes in data and what goes in problem definition. I have moved
%    event types here because I think knowing how many types and examples of
%    them would be helpful upfront. I think where nuggets and wikipedia pages go
%is questionable.}

%Figure \ref{alg:temporal-summarization} outlines our general update
%summarization algorithm.  At each hour, the system processes each input 
%sentence batch $S_t$.
%We predict the salience $P$ for all input sentences $s\in S_t$.
%Next, we cluster $S_t$ using the AP clustering algorithm, biased by $P$,
%obtaining a set of exemeplar sentences $E$. Finally we select a subset of $E$
%to be updates and add those to our set of summary updates $U$.
%
%\begin{algorithm}%[H]
% \KwData{ 
%     $S_{\stime}, S_{\stime+1},\ldots,S_{\etime} $ --- time ordered sentence
%     batches\\
% \KwResult{U --- a list of updates, i.e. the update summary} 
%}
% ~\\
% Initialize empty list $U$ of updates\\
%    \For{$t \gets \stime,\ldots,\etime $}{
%        $P \gets \operatorname{PredictSalience}(S_t)$\\
%        $E \gets \operatorname{APCluster}(P, S_t)$ \\
%        $U_t \gets \operatorname{SelectUpdates}(E,P,U)$\\
%        $U \gets U \cup U_t$
%}
% \caption{Temporal Summarization Algorithm}\label{alg:temporal-summarization}
%\end{algorithm}

\subsection{Salience Prediction}
\label{sec:salpred}

\subsubsection{Model}

When evaluating our summarization system
we do not have access to event nuggets. However, we can use the nuggets
in our training data to learn a model to predict salience for sentences
in new events.
For each event in our training data, we sample a set of sentences and  each 
sentence's salience is computed according to Equation \ref{eq:salience}.
This results in training set of set sentences and the target values 
(their salience) to predict.

In order to fit a regression model, the sentences are represented as feature
vectors (described in the next section).
We fit a Gaussian process (GP) regression model to this data.
\cite{rasmussen:gaussian-process-book}.  GP regressors are a
class of data-driven, non-parametric model generalizing the multi-variate
Gaussian to the infinite dimensional setting.  
%GP's are general
%and are state of the art for many regression tasks.  
GP's are non-parametric and rely on a covariance matrix $\kernelMatrix$, measuring the similarity between pairs of instances, in our 
case sentences.  In our experiments, we used a radial basis 
function (RBF) kernel.  
Our features fall naturally into five groups and we use a separate RBF kernel
for each, using the sum of each feature group kernel as the final input
to the GP model.

%include feature table
\input{feature_table.tex}

\subsubsection{Features}
We want our model to be predictive across different event instances so we avoid lexical features.  Instead, we extract a variety of features including language model scores, geographic relevance, and temporal relevance from each sentence.  

\paragraph{Basic Features}
%KM - Would be good to have quick justification of these features. I added a
%sentence. Feel free to edit or remove.

We employ several basic features that have been used previously in supervised models to rank sentence salience \cite{kupiec1995trainable,conroy2001using}. These include sentence length, the number of capitalized words normalized by sentence length, document position, number of named entities.  
The data stream comprises text extracted from raw html documents;
these features help to downweight sentences that are not actually article 
content (e.g. web page titles, links to other content) or
more heavily weight important sentences (e.g., that appear in
prominent positions such as paragraph initial or article initial).

\paragraph{Query Features}

Query features measure the relationship between the sentence and the event query and type.  These include the number of query words present in the sentence in addition to the number of event type synonyms, hypernyms, and hyponyms as found in WordNet \cite{miller1995wordnet}.  
For example, for event type \emph{earthquake},  we match sentence terms 
``quake'', ``temblor'', ``seism'', and ``aftershock''.

\paragraph{Language Model Features}\label{subsubsec:lm}
Language models allow us to measure the likelihood of a sentence having been 
produced from a particular source.  We consider two types of language model 
features.  The first model is estimated from a corpus of generic news 
articles \ckcomment{(we used the 199?-200? Associated Press and New York Times sections of the Gigaword corpus)}.  
This model is intended to assess the general writing quality (grammaticality, word usage) of an input sentence and helps our model to select sentences
written in the newswire style.  

The second model is estimated from text specific to our event types.  
For each event type we create a corpus of related documents using pages
and subcategories listed under a related Wikipedia category.
For example, the language model for event type `earthquake' is estimated 
from Wikipedia pages under the category \emph{Category:Earthquakes}.  

\fdcomment{stress here that this technique can be applied in situations beyond crisis; any situation where the
expected summary is similar to some previously seen target or some side information.}

These models are intended to detect sentences similar to those appearing in 
summaries of other events in the same category 
(e.g. most earthquake summaries are likely to include higher probability for 
ngrams including the token `magnitude').  

We use the SRILM toolkit to implement a 5-gram Kneser-Ney model for both
the background language and model and the event specific language models.
For each sentence we use the average token log probability under each model
as a feature.


%KM - Note: Someplace the exact list of event types should appear. Probably not
%here.
%KM - I note you have it in a later section but it is labeled as data you use
%to train language models and semantic similarity. I think it would be good to
%have up front in definition of task.


%For both models, we Finally, we extract the percentage of capitalized words,
%and sentence length as features. These last two features also help to
%identify sentences that are less likely to contain relevant content-- overly
%long and heavily capitalized sentences in our corpus were likely to be long
%strings of web-page headlines, section headers, and other irrelevant page
%structure. 

\paragraph{Geographic Relevance Features}
\fdcomment{need introduction here.  what are geographic features?}
There are two challenges to using geographic features. First we do not 
know where the event is and second most sentences do not contain references
to a location.
We address the first issue by extracting all locations from sentences at the
current hour and looking up their latitude and 
longitude using a publicly available geolocation service. 
\fdcomment{is this all documents in the bucket or the filtered set?  is it a biased sample?}
Since the document
stream contains documents that are at least somewhat relevant to the event,
we assume in aggregate the locations should give us a rough area of interest.
The locations are clustered (using affinity propagation and uniform salience)
\fdcomment{affinity propagation undefined}
and we treat the resulting cluster centers
as the event locations for the current time.

To address the second issue, 
we compute geographic relevance features for the document as a whole and all
sentences from that document receive the same feature value.
Using the locations in each document, we compute the median distance to the 
nearest event location. Because document position is a good indicator 
of importance we also compute the distance of the first mentioned
location to the nearest event location. Because some events can move, we also
compute these distances to event locations from the previous hour.



\paragraph{Temporal Relevance Features}
\fdcomment{motivate burstiness; explain what it is before using the expression.}
Our data consists of hourly crawls of online content and so we exploit the temporality of corpus by capturing the burstiness of a sentence, i.e.  the change in word frequency from one hour to the next.``Bursty'' sentences often indicate new and important data. 

We compute the IDF for each hour in our data stream. 
For each sentence, the average TFIDF for the current hour $t$ is taken as a 
feature. Additionally, we use the difference in average TFIDF from time $t$
to $t-i$ for $i = \{1, \ldots, 24\}$ to measure how the TFIDF scores for the 
sentence have changed over the last 24 hours. \fdcomment{unclear.  is it just changing the IDF part of the 
items in the average?}

The final temporal feature is the hours since the event started.
%difference 
%Using these IDF 
%calculations, the avg TFIDF is calculated for
%Let $D_t$ be the set of web pages at time $t$ and let $s = \{w_1,\ldots,w_n\}$ be a sentence from a page $d \in D_t$.  We calculate the 1-hour burstiness of sentence $s$ from document $d$ at hour $t$  as 
%\begin{align*}
%\operatorname{b}_1(s,d,t) = \frac{1}{|s|} \sum_{w \in s} \Bigg( &
%\operatorname{tf-idf}_t(w,d)  \\ & \left. - \frac{\sum_{d^\prime \in D_{t-1}:
%w \in d^\prime } \operatorname{tf-idf}_{t-1}(w,d^\prime)}{|\{d^\prime \in
%D_{t-1}: w \in d^\prime\}|} \right) \end{align*}

%where \begin{align*} \operatorname{tf-idf}_t(w,d) =&
%\log\left(1+\sum_{w^\prime \in d}1\{w=w^\prime\}  \right)\\ & \times
%\log\left(\frac{|D_t|}{1 + \sum_{d^\prime \in D_t}1\{w \in d^\prime\}}\right).
%\end{align*}

%[\textrm{Simpler explanation goes here}]
% 1\{w = w^\prime} %- \operatorname{avg-tf-idf}_{t_{i-1}}(w).
%\end{align*}


%We similarly find the sentence's 5-hour burstiness.  In addition to burstiness, we also include the sentence's average tf-idf and 


% Formally, let $p(f)$ be a distribution over functions where $f$ is any mapping
% of an input space $\mathcal{X}$ to the reals,
%
% $$f: \mathcal{X} \rightarrow \mathcal{R}.$$
% Let the random variable $\mathbf{f} = (f(x_1),\ldots,f(x_n) )$ be
%  an $n$-dimensional vector whose elements are evaluations of the function $f$
% at points $x_i \in \mathcal{X}$.
% We say $p(f)$ is a Gaussian process if for any finite subset
% $\{x_1,\ldots,x_n\} \subset \mathcal{X}$, the marginal distribution over
% that finite subset $p(\mathbf{f})$ has a multivariate Gaussian distribution.
% A GP is parameterized by a mean function $\mu(\mathbf{x})$ and a
% covariance function $K(x,x^\prime)$. Generally, the mean function is simply
% set to 0, leaving the distribution to be completely characterized by the
% kernel function on the data.
%
% In the regression setting, we typically have a response variable $y$ that
% is the sum of our model prediction  and
% some Gaussian noise, i.e. $y = f(x) + \epsilon$ with
% $\epsilon \sim \mathcal{N}(0, \sigma^2)$. When
% $f \sim \operatorname{GP}(\mathbf{0}, \mathbf{K})$, the
% two distributions
% of principal interest are the marginal likelihood
% $p(\mathbf{y}|\mathbf{X}) =
% \mathcal{N}(\mathbf{0},\mathbf{K} + \sigma^2\mathbf{I})$ and the predictive
% distribution,
%
% $$p(\mathbf{y_*}|\mathbf{x_*},\mathbf{X},\mathbf{y}) =
% \mathcal{N}(\boldsymbol{\mu}_*, \boldsymbol{\sigma}^2_*) $$
%
% where $\mathbf{x_*}$ is a new or unseen input, $\mathbf{y_*}$ our predicted
% response, and
% \begin{align*}
% \boldsymbol{\mu}_* & = \mathbf{K_*}(\mathbf{K} + \sigma^2\mathbf{I})^{-1}\mathbf{y} \\
% \boldsymbol{\sigma}^2_* &
% = \mathbf{K}_{**} - \mathbf{K}_*(\mathbf{K} + \sigma^2\mathbf{I})^{-1}
% \mathbf{K}_*^T + \sigma^2\\
% \end{align*}.
%
% Here $\mathbf{K}_* = K(\mathbf{x}_*, \mathbf{X})$, and
% $\mathbf{K}_{**} = K(\mathbf{x}_*, \mathbf{x}_*)$.
%
%

\subsection{Exemplar Selection}
\label{sec:exsel}

We combine the output of our salience prediction model with the affinity
propagation algorithm to identify a set of exemplar sentences 
for each input batch. 
Affinity propagation (AP) is a clustering algorithm
that identifies a subset of datapoints as exemplars and forms clusters
by assigning the remaining points to one of the exemplars. AP attempts to 
maximize the net similarity objective 
\[ \mathcal{S} = \sum_{i : i \neq e_i}^n \operatorname{sim}(i,e_i) 
+ \sum_{i : i = e_i}^n \operatorname{pref}(e_i)  \]
where $e_i$ is the exemplar of the $i$-th data point, and functions
$\operatorname{sim}$ and $\operatorname{salience}$ express the pairwise 
similarity
of data points and the apriori preference of a data point to be an exemplar
respectively. 
AP differs from other $k$-centers algorithms in that it simultaneously 
considers all data points as exemplars, making it less prone to finding
local optima as a result of poor initialization. Furthermore, the 
second term in $\mathcal{S}$ incorporates the individual importance of 
data points as candidate exemplars; most other clustering algorithms only make
use of the first term, i.e. the pairwise similarities between data points.
 
%There are very 
%few restrictions on the nature of the similarities and preferences
%other than that they be real-valued.

AP has several useful properties and interpretations. Chiefly, the number
of clusters $k$ is not a model hyper-parameter. Given that our task requires
clustering many batches of streaming data, searching for an optimal $k$ 
would be computationally prohibitive. With AP, $k$ is determined by the
similarities and preferences of the data. Generally lower preferences will
result in fewer clusters.  


%In our summarization system we use the output of our salience model as 
%the preferences, i.e.
%$\operatorname{salience}(s) = \operatorname{pref}(s)$; for the similarity 
%function we use semantic similarity. 
Recall that $\operatorname{salience}(s)$
is a prediction of the semantic similarity of $s$ to information about the 
event be summarized, i.e. the set of event nuggets.
Intuitively, when maximizing objective function $\mathcal{S}$, AP must balance
between best representing the input data and representing the most salient
input. Additionally, when the level of input is high but the salience
predictions are low, the preference term will guide AP toward a solution
with fewer clusters; vice-versa when input is very salient on average but
the volume of input is low. The adaptive nature of our model differentiates
our method from most out update summarization systems.




\subsection{Update Selection}
\label{sec:upsel}

\textbf{Salience Filter } To 
ensure that only the most salient updates are selected we apply a minimum
salience threshold;
after exemplar sentences have been identified, any exemplars whose salience is 
less than $\lambda_{sal}$ are removed from consideration. 

\textbf{Novelty Filter } Next,
to prevent adding updates that are redundant, we filter out exemplars
that are too similar to previous updates.
The exemplars are examined
sequencially in order of decreasing salience and  a similarity threshold 
is applied, where the exemplar is ignored if its
maximum semantic similarity to any previous updates in the summary is
greater than $\lambda_{sim}$.

Exemplars that pass these thresholds are added selected as updates and added
to the summary.




