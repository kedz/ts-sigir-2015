\label{sec:methods}

Our update summarization system takes as input 
\begin{enumerate*}[label=\itshape\alph*\upshape)]
  \item a short query defining the event to be tracked 
  (e.g. `Hurricane Sandy'), 
  \item an event category defining the type of event to be tracked 
  (e.g. `hurricane'), 
  \item a stream of time-stamped documents presented in temporal order, 
  and \item an evaluation time period of interest.
\end{enumerate*} 
While processing documents throughout the time period of interest, the system 
outputs sentences from these documents likely to be useful to the query 
issuer.  We refer to these selected sentences as \emph{updates}.

In order to measure the usefulness of a system's updates, we consider the 
degree to which the system output reflects the different aspects of an event.
Events are often composed of a variety of sub-events.  For example, the 
Hurricane Sandy event includes sub-events related to the storm making 
landfall, the ensuing flooding, the many transportation issues, among many
others.  An ideal system would update the user about each of these
sub-events as they occur. We refer to these sub-events as the \emph{nuggets} 
associated with an event.  A nugget is defined as a fine-grained atomic 
sub-event associated with an event.  We present 2 example nuggets associated 
with the Hurricane Sandy event in Figure \ref{fig:nuggets}. Each event has 
anywhere from 50 to several hundred nuggets in total in our gold dataset.
We describe how these nuggets are found in Section \ref{sec:data}.


\input{nugget_examples.tex}


Throughout our treatment of our algorithm, the \emph{salience} of an update 
captures the degree to which it reflects an event's unobserved nuggets.  
Assuming that we have a text representation for each of our nuggets, the 
salience of an update $u$  with respect to a set of nuggets $N$ is defined as,
\begin{align}
\operatorname{salience}(u) = \operatorname{max}_{n \in N} 
\operatorname{sim}(u, n) \label{eq:salience}
\end{align}
where $\operatorname{sim}(\cdot)$ is the semantic similarity such as the 
cosine similarity of latent vectors associated with the update and nugget 
text \cite{guo2012simple}. 


\subsection{Update Summarization}


Our system architecture follows a simple pipeline design where each stage 
provides an additional level of processing or filtering of the input 
sentences. We begin with an empty update summary $U$. At each hour we receive 
a new batch of sentences $b_t$ from the stream of event relevant documents
and perform the following actions:
\begin{enumerate}
  \item predict the salience of sentences in $b_t$ (Section~\ref{sec:salpred}),
  \item select a set of exemplar sentences in $b_t$ by combining 
      clustering with 
      salience predictions (Section~\ref{sec:exsel}),
  \item add the most novel and salient exemplars 
      to $U$ (Section~\ref{sec:upsel}).
\end{enumerate}
The resultant list of updates $U$ is our summary of the event.


\subsection{Salience Prediction}
\label{sec:salpred}


\subsubsection{Features}
\label{sec:features}


We want our model to be predictive of sentence salience across different event instances so we avoid event-specific lexical features.  Instead, we extract features such as language model scores, geographic relevance, and temporal relevance from each sentence.  


\textbf{Basic Features}
We employ several basic features that have been used previously in supervised 
models to rank sentence salience \cite{kupiec1995trainable,conroy2001using}. 
These include sentence length, the number of capitalized words normalized by 
sentence length, document position, and number of named entities.  The data 
stream comprises text extracted from raw html documents; these features help 
to downweight sentences that are not content (e.g. web page titles, links to 
other content) or more heavily weight important sentences (e.g., that appear 
in prominent positions such as paragraph initial or article initial).


\textbf{Query Features}
Query features measure the relationship between the sentence and the event 
query and type.  These include the number of query words present in the 
sentence in addition to the number of event type synonyms, hypernyms, and 
hyponyms using WordNet \cite{miller1995wordnet}.  For example, for event type 
\emph{earthquake},  we match sentence terms ``quake'', ``temblor'', ``seism'',
and ``aftershock''.


\textbf{Language Model Features}\label{subsubsec:lm}
Language models allow us to measure the likelihood of producing a sentence 
from a particular source.  We consider two types of language model features.  
The first model is estimated from a corpus of generic news articles (we used 
the 1995-2010 Associated Press section of the Gigaword corpus 
\cite{graff2003english}).  
This model is intended to assess the general writing quality (grammaticality, 
word usage) of an input sentence and helps our model to select sentences
written in the newswire style.  


\begin{figure}
    \center
    \begin{tabular}{|l l l|}
    %Event Types \\
    \hline
    Storm & Earthquake & Meteor Impact\\
    Accident & Riot & Protest\\
    Hostages & Shooting &Bombing\\
    \hline
    \end{tabular}
    \caption{TREC TS event types.}
    \label{fig:eventtypes}
\end{figure}

The second model is estimated from text specific to our event types.  For each
event type we create a corpus of related documents using pages and 
subcategories listed under a related Wikipedia category. For example, the 
language model for event type `earthquake' is estimated from Wikipedia pages 
under the category \emph{Category:Earthquakes}.  Fig.~\ref{fig:eventtypes} 
lists the event types found in our dataset. These models are intended to 
detect sentences similar to those appearing in summaries of other events in 
the same category (e.g. most earthquake summaries are likely to include 
higher probability for ngrams including the token `magnitude'). While we focus
our system on the language of news and disaster, we emphasize that the use of 
language modeling can be an effective feature for multi-document summarization
for other domains that have related text corpora.


We use the SRILM toolkit \cite{stolcke2002srilm} to implement a 5-gram 
Kneser-Ney model for both the background language model and the event specific
language models. For each sentence we use the average token log probability 
under each model as a feature.


\textbf{Geographic Relevance Features}
The disasters in our corpus are all phenomena that affect some part of the 
world. Where possible, we would like to capture a sentence's proximity to the 
event, i.e. when a sentence references a location, it should be close to the 
area of the disaster. 


There are two challenges to using geographic features. First, we do not know 
where the event is, and second, most sentences do not contain references to a 
location. We address the first issue by extracting all locations from 
documents relevant to the event at the current hour and looking up their 
latitude and longitude using a publicly available geo-location service. 
Since the documents that are at least somewhat relevant to the event, we 
assume in aggregate the locations should give us a rough area of interest.
The locations are clustered and we treat the resulting cluster centers as the 
event locations for the current time.


The second issue arises from the fact that the majority of sentences in our 
data do not contain explicit references to locations, i.e. a sequence of 
tokens tagged as location named entities. Our intuition is that geographic 
relevance is important in the disaster domain, and we would like to take 
advantage of the sentences that do have location information present. To make 
up for this imbalance, we instead compute an overall location for the document
and derive geographic features based on the document's proximity to the event
in question. These features are assigned to all sentences in the document.


Our method of computing document-level geographic relevance features is as 
follows. Using the locations in each document, we compute the median distance 
to the nearest event location. Because document position is a good indicator 
of importance we also compute the distance of the first mentioned location to 
the nearest event location. All sentences in the document take as features 
these two distance calculations. Because some events can move, we also compute
these distances to event locations from the previous hour.


\textbf{Temporal Relevance Features}
As we track events over time, it is likely that the coverage of the event 
may die down, only to spike back up when there is a breaking development.
Identifying terms that are ``bursty,'' i.e. suddenly peaking in usage,
can help to locate novel sentences that are part of the most recent reportage
and have yet to fall into the background.

We compute the IDF for each hour in our data stream. For each sentence, the 
average TF*IDF for the current hour $t$ is taken as a feature. Additionally, 
we use the difference in average TF*IDF from time $t$ to $t-i$ for 
$i = \{1, \ldots, 24\}$ to measure how the TF*IDF scores for the sentence have
changed over the last 24 hours, i.e. we keep the sentence term frequencies 
fixed and compute the difference in IDF. Large changes in IDF value indicate 
the sentence contains bursty terms.
We also use the time (in hours) since the event started as a feature.


\subsubsection{Model}


Given our feature representation of the input sentences, we need only target 
salience values for model learning. For each event in our training data, we 
sample a set of sentences and  each sentence's salience is computed according 
to Equation \ref{eq:salience}. This results in a training set of sentences, 
their feature representations, and their target salience values to predict.


We opt to use a Gaussian process (GP) regression model
\cite{rasmussen:gaussian-process-book} with a Radial Basis Function (RBF) 
kernel for the salience prediction task. Our features fall naturally into 
five groups and we use a separate RBF kernel for each, using the sum of each 
feature group RBF kernel as the final input to the GP model.


\subsection{Exemplar Selection}
\label{sec:exsel}


Once we have predicted the salience for a batch of sentences, we must
now select a set of update candidates, i.e. sentences that are both salient
and representative of the current batch. To accomplish this, we combine the 
output of our salience prediction model with the affinity propagation 
algorithm.
Affinity propagation (AP) is a clustering algorithm
that identifies a subset of data points as exemplars and forms clusters
by assigning the remaining points to one of the exemplars 
\cite{frey2007clustering}. AP attempts to maximize the net similarity 
objective 
\[ \mathcal{S} = \sum_{i : i \neq e_i}^n \operatorname{sim}(i,e_i) 
+ \sum_{i : i = e_i}^n \operatorname{salience}(e_i)  \]
where $e_i$ is the exemplar of the $i$-th data point, and functions
$\operatorname{sim}$ and $\operatorname{salience}$ express the pairwise 
similarity of data points and our predicted \textit{apriori} preference of a 
data point to be an exemplar respectively. 
AP differs from other $k$-centers algorithms in that it simultaneously 
considers all data points as exemplars, making it less prone to finding local 
optima as a result of poor initialization. Furthermore, the second term in 
$\mathcal{S}$ incorporates the individual importance of data points as 
candidate exemplars; most other clustering algorithms only make use of the 
first term, i.e. the pairwise similarities between data points.
 

AP has several useful properties and interpretations. Chiefly, the number
of clusters $k$ is not a model hyper-parameter. Given that our task requires
clustering many batches of streaming data, searching for an optimal $k$ 
would be computationally prohibitive. With AP, $k$ is determined by the
similarities and preferences of the data. Generally lower preferences will
result in fewer clusters.  


Recall that $\operatorname{salience}(s)$ is a prediction of the semantic 
similarity of $s$ to information about the event be summarized, i.e. the set 
of event nuggets. Intuitively, when maximizing objective function 
$\mathcal{S}$, AP must balance between best representing the input data and 
representing the most salient input. Additionally, when the level of input is 
high but the salience predictions are low, the preference term will guide AP 
toward a solution with fewer clusters; \textit{vice-versa} when input is very 
salient on average but the volume of input is low. The adaptive nature of our 
model differentiates our method from most other update summarization systems.


\subsection{Update Selection}
\label{sec:upsel}


The exemplar sentences from the exemplar selection stage are the most salient 
and representative of the input for the current hour. However, we need to 
reconcile these sentences with updates from the previous hour to ensure that 
the most salient and least redundant  updates are selected. To ensure that 
only the most salient updates are selected we apply a minimum salience 
threshold; after exemplar sentences have been identified, any exemplars whose 
salience is less than $\lambda_{sal}$ are removed from consideration. 


Next, to prevent adding updates that are redundant with previous output 
updates, we filter out exemplars that are too similar to previous updates.
The exemplars are examined sequentially in order of decreasing salience and  a similarity threshold is applied, where the exemplar is ignored if its maximum 
semantic similarity to any previous updates in the summary is greater than 
$\lambda_{sim}$.


Exemplars that pass these thresholds are selected as updates and added
to the summary.
