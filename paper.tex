% This is "sig-alternate.tex" V2.0 May 2012 This file should be compiled with
% V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls' V2.5
% LaTeX2e document class file. It is for those submitting articles to ACM
% Conference Proceedings WHO DO NOT WISH TO STRICTLY ADHERE TO THE SIGS
% (PUBS-BOARD-ENDORSED) STYLE.  The 'sig-alternate.cls' file will produce a
% similar-looking, albeit, 'tighter' paper resulting in, invariably, fewer
% pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces: 1) The Permission
% Statement 2) The Conference (location) Info information 3) The Copyright
% Line with ACM data 4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which DOES NOT produce 1) thru'
% 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within the source
% .tex file, over both the CopyrightYear (defaulted to 200X) and the ACM
% Copyright Data (defaulted to X-XXXXX-XX-X/XX/XX).  e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the
% copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use the .bib file (from which
% the .bbl file % is produced).  REMEMBER HOWEVER: After having produced the
% .bbl file, and prior to final submission, you *NEED* to 'insert' your .bbl
% file into your source .tex file so as to provide ONE 'self-contained' source
% file.
%
% ================= IF YOU HAVE QUESTIONS ======================= Questions
% regarding the SIGS styles, SIGS policies and procedures, Conferences etc.
% should be sent to Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate} 
\usepackage{url} 
\usepackage{color}
% \documentclass[10pt]{article} \usepackage{url} \usepackage{color}
\usepackage[utf8]{inputenc}
% \usepackage[margin=1in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[]{algorithm2e}
\DeclareMathOperator{\corpus}{\mathcal{C}}
\DeclareMathOperator{\doc}{\mathnormal{d}}
\DeclareMathOperator{\sent}{\mathnormal{s}}
\DeclareMathOperator{\order}{\pi}
\DeclareMathOperator{\dtime}{\mathnormal{t}}
\DeclareMathOperator{\hour}{\mathnormal{h}}
\DeclareMathOperator{\hours}{\mathcal{H}}
\DeclareMathOperator{\Sim}{\mathbf{K}}
\DeclareMathOperator{\SMat}{\mathbf{X}}
\DeclareMathOperator{\Pref}{\boldsymbol{\pi}}
\DeclareMathOperator{\Exemp}{\mathnormal{Exemplars}}
\DeclareMathOperator{\Updates}{\mathnormal{Updates}}
\usepackage{cleveref}
\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}

\usepackage{authblk}
\newcommand{\fdadd}[1]{\textcolor{red}{#1}}
\newcommand{\fdcomment}[1]{\textbf{\textcolor{red}{[FD: #1]}}}

\begin{document}

%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden
%- IF NEED BE.  \crdata{0-12345-67-8/90/01}  % Allows default copyright data
%(0-89791-88-6/97/05) to be over-ridden - IF NEED BE.  --- End of Author
%Metadata ---

\title{The Anatomy of a Temporal Summarization System}

\numberofauthors{1} %  in this sample file, there are a *total*
\author{}
% \author[1]{Chris Kedzie}
% \author[1]{Kathleen McKeown}
% \author[2]{Fernando Diaz}
% \affil[1]{Columbia University, Department of Computer Science}
% \affil[2]{Microsoft Research}
%    Alternate {\ttlit ACM} SIG Proceedings Paper in LaTeX
%Format\titlenote{(Produces the permission block, and copyright information).
%For use with SIG-ALTERNATE.CLS. Supported by ACM.}} \subtitle{[Extended
%Abstract] \titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement and
% alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time' i.e. three
% 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of "name/affiliations" may
% appear. We just ask that you restrict the number of 'columns' to three.
%
% Because of the available 'opening page real-estate' we ask you to refrain
% from putting more than six authors (two rows with three columns) beneath the
% article title.  More than six makes the first-page appear very cluttered
% indeed.
%
% Use the \alignauthor commands to handle the names and affiliations for an
% 'aesthetic maximum' of six authors.  Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the \additionalauthors
% command.  These 'additional authors' will be output/set for you without
% further effort on your part as the last section in the body of your article
% BEFORE References or any Appendices.

%\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting reasons)
% and the remaining two appear in the \additionalauthors section.
%
%\author{
% You can go ahead and credit any number of authors here, e.g. one 'row of
% three' or two rows (consisting of one row of three and a second row of one,
% two or three).
%
% The command \alignauthor (no curly braces needed) should precede each author
% name, affiliation/snail-mail address and e-mail address. Additionally, tag
% each line of affiliation/address with \affaddr, and tag the e-mail address
% with \email.
%
% 1st. author

%\alignauthor Chris Kedzie\\ \affaddr{Columbia University}\\
%\affaddr{Department of Computer Science} \email{kedzie@cs.columbia.edu}

% 2nd. author
%\alignauthor Kathleen McKeown\\ \affaddr{Columbia University}\\
%\affaddr{Department of Computer Science}\\ \email{kathy@cs.columbia.edu}

% 3rd. author
%\alignauthor Fernando Diaz\\ \affaddr{Microsoft Research}\\
%\email{fdiaz@microsoft.com}

% There's nothing stopping you putting the seventh, eighth, etc.  author on
% the opening page (as the 'third row') but we ask, for aesthetic reasons that
% you place these 'additional authors' in the \additional authors block, viz.
%} 
\date{29 October 2014}
% Just remember to make sure that the TOTAL number of authors is the number
% that will appear on the first page PLUS the number that will appear in the
% \additionalauthors section.

\maketitle \begin{abstract} 
Information need is highest during times of crisis; both officials and 
civilians need effective access to make informed decisions about their 
safety and the safety of others. However, these situations pose many 
interesting challenges for the IR systems that support these decisions.
Specifically, we address event monitoring and updating, a.k.a. the
temporal summarization task (TS). Given an event query, we monitor streams
of online news and select important text to present to the user. Updates
are selected by predicting the salience of the text w.r.t. to the event.
Our model utilizes a variety of features at the query, document, and corpus
level. Additionally, these predictions are used to directly bias a clustering
algorithm for sentence selection, 
increasing the novelty of the updates. We evaluate our system
on a varied set of retrospective events using a combination of human 
judgements
and automatic evaluation measures, as compared to a baseline system.
We demonstrate the effect of the various feature groups, and the importance
of salience prediction w.r.t. update precision when compared to a clustering
only baseline. 


\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures,
%performance measures]

%\terms{Summarization}

%\keywords{Extractive Summarization, Affinity Propagation}


\input{introduction.tex}

\input{relatedwork.tex}


\section{Background}\label{sec:background}

\subsection{Affinity Propagation}

Affinity propagation (AP) is a message passing algorithm that identifies both
exemplar data points and assignments of each point to an exemplar.  This is
done iteratively by passing \emph{responsibility} and \emph{availability}
messages between data points that quantify the fitness of one data point to
represent another, and the fitness of a data point to be represented based on
the choices of other data points respectively \cite{dueck2007non}.

AP is parameterized by an $n\times n$ similarity matrix $S$ and an $n\times 1$
preference vector $\pi$.  $S$ is a real-valued matrix where $S(i,j)$ is the
similarity of the $i$-th data point to the $j$-th data point.  $S$ does not
need to be symmetric.  $\pi$ is a real-valued vector where $\pi(i)$ expresses
our preference that the $i$-th data point can serve as an exemplar a priori of
other data points. 




AP has several useful properties that comport well with the TS track 
requirements. First, the number of clusters $k$ is not a hyper-parameter
of the model. Since we will be running this algorithm many times in per run,
the usual methods of hyperparameter search become infeasible 
(?). The number of clusters falls out of the algorithm
organically-- lower overall preference values will result in fewer clusters. 

Secondly, the arbitrary nature of the preferences
allow us to incorporate a variety of signals for
identifying the best exemplars, i.e. salience and redundancy signals. 
The preferences can be thought of as self-similarities (similarities that
pairwise-comparison based algorithms would ignore) that we can exploit to 
incorporate our prior beliefs about a data point.

Finally, cluster exemplars are guaranteed to be actual data points. Many 
clustering algorithms group data around mathematical objects (e.g., the
mean) that are not necessarily observed in the data. The extractive nature of
this TS task requires that we emit actual data points (i.e. sentences).
We are able to sidestep 
the additional requirement of selecting a most
representative cluster member, as this is computed explicitly in the AP 
algorithm. 




%Each element $R(i,k)$ expresses the fitness of the $k^{th}$ point to serve as
%the exemplar of the $i^{th}$ point relative to other potential exemplars.
%Each element $A(i, k)$ represents the $k^{th}$ point's ``availability'' to
%serve as an exemplar of the $i^{th}$ point, taking into account other points'
%preference for the $k^{th}$ point as an exemplar.



%While, AP does not set the number of exemplars before-hand, a lower overall
%preference values will result in a smaller number of exemplars.


%arbitrary pair-wise similarity function $S: \mathcal{R}^d \rightarrow
%\mathcal{R}$, where $d$ is the dimension of the data being clustered and a
%real-valued preference $\pi_i$ quantifying our belief a priori of the
%$i^{th}$ element's ability to serve as an exemplar.




\subsection{Semantic Similarity}\label{subsec:semsim}

Whenever we make a pairwise comparison between sentences, we use the weighted
textual matrix factorization (WTMF) model of \cite{guo2012simple}. This 
model can be thought of as a variant of latent semantic analysis (?), 
where words that are not present in a sentence are explicitly modeled.
More formally, we have a term-sentence matrix 
$\mathbf{X}\in\mathcal{R}^{v \times n}$ representing $n$ sentences with a 
vocabulary of $v$ words; $\mathbf{X}_{i,j}$ indicates is non-zero if sentence
$j$ contains word $i$. In the WTMF regime, we want to find an approximation
of $\mathbf{X} \approx \mathbf{P}^T\mathbf{Q}$, where 
$\mathbf{P} \in \mathcal{R}^{k \times v}$ is a latent word vector space and
$\mathbf{Q} \in \mathcal{R}^{k \times n}$ is a latent sentence vector
space. These matrices are found by minimizing the objective function

$$\sum_i^v \sum_j^n \mathbf{W}_{i,j}(\mathbf{P}_{\cdot,i}^T
\mathbf{Q}_{\cdot,j} 
- \mathbf{X}_{i,j})^2 
 + \lambda ||\mathbf{P}||_2^2 + \lambda ||\mathbf{Q}||_2^2$$

where $\mathbf{W}_{i,j} = 
\begin{cases} 1, & \textrm{if $\mathbf{X}_{i,j} \ne 0$ } \\
w_m, & \textrm{if $\mathbf{X}_{i,j} = 0$ }\\
\end{cases}$
and $\lambda$ is a hyperparameter controlling the regularization terms.

The $w_m$ term is another model hyperparameter that is set to a small constant
($\le .01$). The weight matrix $\mathbf{W}$ has the effect of discounting the
reconstruction error of missing terms (words that did not occur a sentence).

Given an unseen sentence $\hat{i}$ we can project its term vector into the
latent sentence vector space with 

$$
\mathbf{Q}_{\cdot,\hat{i}} = (\mathbf{P}\mathbf{\tilde{W}}^{(\hat{i})}
\mathbf{P}^T  + \lambda\mathbf{I} )^{-1} 
\mathbf{P}\mathbf{\tilde{W}}^{(\hat{i})} \mathbf{X}_{\cdot, \hat{i}}
$$  

where $\mathbf{\tilde{W}}^{(\hat{i})}$ is an $v\times v$ diagonal matrix
where $\mathbf{\tilde{W}}^{(\hat{i})}_{j,j}$ is equal to $1$ or $w_m$ 
depending on whether or not the $j$-th term occurs in sentence $\hat{i}$.


The WTMF model is used extensively throughout our TS system. When making any 
pairwise comparison between sentence $i$ and $j$, we first construct
their latent sentence vectors $\mathbf{Q}_{\cdot,i}$ and
$\mathbf{Q}_{\cdot,j}$ and then find the cosine similarity 
$\displaystyle \operatorname{cos-sim}
(\mathbf{Q}_{\cdot,i}, \mathbf{Q}_{\cdot,j}) = 
\frac{\mathbf{Q}_{\cdot,i}^T\mathbf{Q}_{\cdot,j}}{||\mathbf{Q}_{\cdot,i}||_2
||\mathbf{Q}_{\cdot,j}||_2   }$.


Because the events for the TS task come from different domains, we construct
domain specific latent word vector spaces for each domain using in-domain 
Wikipedia pages (see~\cref{sec:data} for more details).

\subsection{Gaussian Processes}

A Gaussian process (GP) is a distribution over functions and is a 
generalization of the multi-variate Gaussian to the infinite dimensional
setting. That is, we use the observed data to define a distribution over 
possible functions that generated this data, without having to explicitly 
parameterize the function---in this sense GPs are considered 
a non-parametric model.

Formally, let $p(f)$ be a distribution over functions where $f$ is any mapping
of an input space $\mathcal{X}$ to the reals,

$$f: \mathcal{X} \rightarrow \mathcal{R}.$$ 
Let the random variable $\mathbf{f} = (f(x_1),\ldots,f(x_n) )$ be
 an $n$-dimensional vector whose elements are evaluations of the function $f$
at points $x_i \in \mathcal{X}$.
We say $p(f)$ is a Gaussian process if for any finite subset 
$\{x_1,\ldots,x_n\} \subset \mathcal{X}$, the marginal distribution over 
that finite subset $p(\mathbf{f})$ has a multivariate Gaussian distribution.
A GP is parameterized by a mean function $\mu(\mathbf{x})$ and a 
covariance function $K(x,x^\prime)$. Generally, the mean function is simply
set to 0, leaving the distribution to be completely characterized by the
kernel function on the data.

In the regression setting, we typically have a response variable $y$ that
is the sum of our model prediction  and 
some Gaussian noise, i.e. $y = f(x) + \epsilon$ with 
$\epsilon \sim \mathcal{N}(0, \sigma^2)$. When
$f \sim \operatorname{GP}(\mathbf{0}, \mathbf{K})$, the
two distributions
of principal interest are the marginal likelihood
$p(\mathbf{y}|\mathbf{X}) = 
\mathcal{N}(\mathbf{0},\mathbf{K} + \sigma^2\mathbf{I})$ and the predictive
distribution,

$$p(\mathbf{y_*}|\mathbf{x_*},\mathbf{X},\mathbf{y}) =
\mathcal{N}(\boldsymbol{\mu}_*, \boldsymbol{\sigma}^2_*) $$

where $\mathbf{x_*}$ is a new or unseen input, $\mathbf{y_*}$ our predicted
response, and
\begin{align*}
\boldsymbol{\mu}_* & = \mathbf{K_*}(\mathbf{K} + \sigma^2\mathbf{I})^{-1}\mathbf{y} \\
\boldsymbol{\sigma}^2_* & 
= \mathbf{K}_{**} - \mathbf{K}_*(\mathbf{K} + \sigma^2\mathbf{I})^{-1}
\mathbf{K}_*^T + \sigma^2\\
\end{align*}.

Here $\mathbf{K}_* = K(\mathbf{x}_*, \mathbf{X})$, and 
$\mathbf{K}_{**} = K(\mathbf{x}_*, \mathbf{x}_*)$.


GP's are incredibly general, and are state of the art for many regression 
tasks (?). The reliance on the covariance matrix 
$\mathbf{K}$ for parameterization opens up the wide world of kernel methods
for regression, and many varieties of similarity functions can be used.
In our experiments we used a radial basis function kernel 
$$K(\mathbf{x},\mathbf{x}^\prime) = \sigma^2 \exp\bigg(- \frac{1}{2} 
\sum_{i=1}^d \frac{ (x_i-x^\prime_i)^2}{\ell_i^2} \bigg)$$ where 
$\sigma$ and the $\ell_i$ are parameters we fit to our observed training data.
The $\ell_i$ are feature dependent scaling parameters; once learned, they not
only improve the accuracy of the model, but give us some introspection 
into which features are more important.

%the formation of sentence clusters around more salient sentences. We
%introduce the affinity propagation algorithm as as an elegant way to
%incorporate our salience predictions into a

%What follows is a description of our ongoing event summarization efforts.  We
%briefly situate our approach to summarization within the broader field of
%multi-document summarization, and then introduce the affinity propagation
%algorithm which we use for clustering. This algorithm allows us to elegantly
%address the salient sentence selection problem by incorporating our prior
%beliefs about sentence quality. Next, we describe out method for modeling
%summary sentence quality, and the features used in this model.  Finally, we
%address future features and system improvements that we are incorporating
%into our summarizer.

\section{Data}\label{sec:data}

Our primary dataset is the TREC 2014 Stream Corpus, a ?tb corpus of hourly 
web crawls from October 2011
through mid February 2013 \cite{frank2012building}.\footnote{\url{http://trec-kba.org/kba-stream-corpus-2014.shtml}}
As per the TS specifications, all summary sentences come from processing this
corpus in a time aligned manner. Additionally, we use the query terms 
provided by the track organizers for document retrieval.

For our semantic similarity component (\cref{subsec:semsim}),
domain specific term-sentence matrices were built using subsections of
Wikipedia. For each event type we identified one or more related Wikipedia
categories and collected all pages that were listed under that category.
For all pages collected, we used the latest possible revision date before the
start of the Stream Corpus.

We use the same Wikipedia corpora to construct the lanuage models used
in our salience prediction model (\cref{subsubsec:lm}). 

\begin{figure*}
	\begin{center}
\begin{tabular}{| p{5cm} c c |}
\hline
Event & WP Categories & No. Docs/Sents/Words\\
\hline \hline
Boston\_Marathon\_bombings \newline
Christopher\_Dorner\_shootings\_and\_manhunt \newline 
 In\_Amenas\_hostage\_crisis
& Terrorism, Mass Shootings & 33,732/1,139,588/26,201,659  \\
\hline
 Cyclone\_Oswald \newline
Early\_2012\_European\_cold\_wave \newline
February\_2013\_noreaster \newline & Weather events & 35,554/591,850/12,794,438  \\
\hline
Costa\_Concordia\_disaster & Accidents & 22,874/732,945/16,520,242 \\
\hline
Chelyabinsk\_meteor & Earthquakes & 14,515/283,509/6,135,803  \\
\hline
Port\_Said\_Stadium\_riot \newline
2012\_Afghanistan\_Quran\_burning\_protests \newline
2011-13\_Russian\_protests \newline
2012\_Romanian\_protests \newline
2012-13\_Egyptian\_protests \newline
2013\_Bulgarian\_protests\_against\_the\_Borisov\_cabinet \newline
2013\_Shahbag\_protests & Activism\_by\_type & 464,657/11,254,122/250,172,896  \\
\hline
\end{tabular}
\caption{Domain specific Wikipedia corpora for semantic similarity and 
language models. }
\end{center}
\end{figure*} 


We use the TREC 2013 Stream Corpus and TS 2013 event/gold nugget data to 
train our salience models. See section~\cref{subsec:Predict} 
for a detailed description of our sampling procedure to generate labeled
data.




%Our documents for summarization come from the online news portion of the TREC
%Stream Corpus, a 6.45tb corpus obtained by hourly web crawls from October 2011
%through mid February 2013
%
%Summary events come from the TREC Temporal Summarization track, and include
%natural disasters like Hurricane Sandy as well as man-made events like a 2012
%train accident in Buenos
%Aires.\footnote{\url{http://trec.nist.gov/data/tempsumm2013.html}} The track
%organizers also provide a search query for each event \cite{aslam2013trec}.
%For each event, we collect the documents that contain all query words and
%stratify them by the hour they were collected. 
%
%For evaluation purposes, the track organizers also provided gold nugget
%information (i.e. important pieces of information, usually the length of a
%short clause or sentence). These gold nuggets come from the event's related
%Wikipedia article and also include the timestamp of when they were added to
%the page.
%
%%KM _ Ask Chris: is this true?
%To create hourly gold summaries to evaluate our system, we simply take the set
%of gold nugget information from the start of the event up to the current hour. 
%



\section{Approach}\label{sec:approach}


We first begin with some notation. For a given event, let $\corpus$ be the set
of retrieved documents. A document $\doc \in \corpus$ is an ordered sequence
of sentences $\{\sent_{1,\doc},\ldots,\sent_{|\doc|,\doc} \}$. 
Additionally, each document has a timestamp $\dtime(\doc)$. Finally, let 
$\corpus_{\hour_i}$ be the set of retrieved documents such that 
$\hour_i \le \dtime(\doc) < \hour_{i+1}$ for all $\doc \in \corpus_{\hour_i}$.

%Our TS system involves ? phases involving sentence level classification, 
%regression, and clustering. 
Figure ? outlines our general temporal summarization algorithm. The description
of our approach is as follows.
For each event, we iterate over the retrieved 
documents in hourly chunks, emitting 0 or more updates at each hour.
At each hour $\hour$, we process each document
$\doc \in \corpus_{\hour}$. First, we identify where the document content
actually is; Second, we predict the salience of all content sentences.
To account for redundancy, the predicted salience is penalized based on the 
distance of the sentence in question to the previous summary updates.

Finally, we cluster all content sentences 
for the current hour using the penalized salience predictions to bias the 
formation
of clusters around the most salient sentences. For each cluster center, or 
exemplar, that results, we check that the salience is above a threshold and 
that it does not belong to a singleton cluster; exemplars that satisfy these
conditions are emitted as an update.
Additionally, we maintain the complete set of updates in order to penalize
salience predictions in the subsequent time steps. 

% \begin{figure}
% \centering
\begin{algorithm}%[H]
 \KwData{$Query$ --- the set of event query words\\ 
        $SC$ --- the stream corpus\\

         
}
 ~\\
 Initialize empty list $\mathbf{U}$ of updates\\
 Initialize empty list $\boldsymbol{\Pref}^{(U)}$ of update preferences\\
 $\corpus \gets $ RetrieveDocuments($Query$, $SC$) (See \cref{subsec:Document Retrieval})\\
 \For{$i \gets 1,\ldots,t $}{

  Initialize empty lists $\SMat, \Pref$ \;
      

  \For{$\doc \in \corpus_{\hour_i}$}{
   \For{$\sent \in \operatorname{getContent}(\doc, Query)$ (See \cref{subsec:Content Detection})\\}{
     $\SMat.\operatorname{append}(\sent)$\;
     $\sigma \gets \operatorname{PredictSalience}(\sent)$ (See \cref{subsec:Predict})\\     
     $\Pref.\operatorname{append}(\sigma)$\;
     
   }        
  }
  %$\Sim \gets \operatorname{ComputeSimilarityMatrix}(X)$\;
  %$\operatorname{}$
  ~\\ 
  $\mathbf{U}_{h_i}, \Pref^{(U)}_{h_i} \gets \operatorname{SentenceSelection}
    (\SMat, \Pref)$ (See \cref{subsec:SentenceSelection})\\
  ~\\
  $\operatorname{Emit}(\mathbf{U}_{h_i})$ \\ 
  $\mathbf{U}\operatorname{.append}(\mathbf{U}_{h_i})$ \\
  $\mathbf{\Pref}^{(U)}\operatorname{.append}(\Pref^{(U)}_{h_i})$ \\
%gets \Updates_{cache} \cup \Updates_{\hour}$\;

 } 
 \caption{Temporal Summarization Algorithm}
\end{algorithm}
% \end{figure}

\subsection{Document Retrieval}\label{subsec:Document Retrieval}

The focus of our system was on salience prediction and clustering stages, and
so we relied heavily on the pre-filtered corpus provided by the track 
organizers. The TREC Temporal Summarization 2014 (TREC-TS-2014F) corpus is
a subset of the full TREC 2014 StreamCorpus and is
intended to be a high recall retrieval of documents related to all 15 of 
the 2014 TS events. 

Track participants were provided with a set of query words for each event.
For example, the event ``Costa Concordia disaster and recovery'' had query
words [``costa'', ``concordia''].
In order to construct an event specific corpus,
we retrieve all documents whose timestamps fall within the event start/stop 
times and whose raw html content contains at least one keyword from the 
event's query words. We further restricted our document set to only those 
articles from the news domain.


\subsection{Content Detection}\label{subsec:Content Detection}

In early versions of our summarization system, we found that structural html
artifacts and sentence tokenization errors
were negatively effecting the performance of later stages. Examples of the 
former include strings of link text like ``World Politics Sports ...'', while
examples of the latter included concatenations of various article headlines,
i.e. headlines pertaining to the event in question, as well as headlines from
other non-related events.
Both types of ``sentences'' were problematic for computing sentence similarity
as they were more likely to have higher average similarity to all input 
sentences. In turn they would be more likely to appear as cluster exemplars
in our clustering stage. From the clustering algorithm's point of view, this
is the correct decision to make---such multi-topic sentences are more general 
than a single topic, and better able to represent all aspects contained in a 
cluster. They make poor choices as updates, however, as they contain 
irrelevant information.

In order to filter out these problematic inputs, we trained a classifier to 
identify which sentences came from inside a document's main article and which
came from various headers, titles, menus, and links to other content. We collected ? random sentences and manually labeled whether the sentence came from
inside or outside the document's main article. We then trained a logistic 
regression classifier using the following features :

\begin{itemize}
 \item the position of the sentence
 \item word counts
 \item the last token in the sentence
 \item the last two tokens in the sentence
 \item the last three tokens in the sentence.
\end{itemize} 

These features were sufficient to capture 
the main difference between content and non-content sentences,
which 
was that content sentences generally ended with sentence final punctuation, 
i.e. periods or a closing quotation mark. 

Within our larger summarization framework, we process a document at a time,
identifying the subset of its sentences that are content sentences.
We then check to make sure \emph{all} event
query terms can be found within the document's content sentences. If so,
we send the content sentences on to the next stage of our pipeline; otherwise
we ignore all sentences in this document.

\subsection{Predicting Sentence Salience}\label{subsec:Predict}

In order to use AP clustering for summarization, we need to assign a
preference value to each input sentence.  In our approach, we equate a
sentence's salience with its preference.  A good model of sentence salience
should predict higher values for sentences that are more likely to appear in a
human generated summary of the event. 

We do not have such human judgments, but we do have last year's gold nugget
sentences.
The response variable we try to predict is a sentence's semantic similarity
(see ?) to the gold nugget sentences, i.e. we want to predict the similarity
to a gold nugget when the gold nugget is not known.


To build training data for this regression task, 
we use the TREC 2013 event/gold nuggets. For each event, we retrieve all 
sentences using the document retrieval and content selection steps outlined
in the previous selection. We then sample with replacement 200 sentences
from this collection, and extract non-lexical features (described in more 
detail below) for each sentence
to construct our design matrix $\mathbf{X}$. To build our response variables
$\mathbf{y}$ we compute the maximum semantic similarity of each input
sentence to the gold nugget sentences. We fit a $\operatorname{GP}$ with
an RBF kernel to this data, optimizing kernel parameters with the scaled 
conjugate gradient method. This sampling procedure is repeated 100 times
for each of the ? 2013 TS events, yielding ? total models. In our salience 
predictions for the 2014 events, we take the mean prediction of all models.

 
%we take a subset of sentences
%relevant to the TREC events (approximately 1000) and match them to the gold
%nugget sentence with highest similarity as determined by the sentence
%similarity system of \cite{guo2012simple}. 
%\cite{} have used this system previously to correlate sentences to meaningful
%units of information in human generated summaries. 
%We use the real-valued similarity scores as our salience scores for the
%training sentences.

We want our model to be predictive across different kinds of events so we
avoid lexical features.  Instead, we extract a variety of features including
language model scores, geographic relevance, and temporal relevance from each
sentence.  These features are used to fit a Gaussian process regression model
that can predict the similarity of a sentence to a gold summary
\cite{preotiuc2013temporal}.  
%We use the model predicted salience of each
%sentence as it's preference value in the AP clustering. 

\subsubsection{Basic Features}

We employ several basic features that have been used previously in supervised
models to rank sentence salience \cite{kupiec1995trainable,conroy2001using}.
These include sentence length, the number of capitalized words normalized by
sentence length, and the number of query words present in the sentence.  Query
words include the event's type (e.g., \emph{earthquake}) and are expanded with
the event type's WordNet \cite{miller1995wordnet} synset, hypernyms, and
hyponyms.  For \emph{earthquake}, e.g., we obtain ``quake,'' ``temblor,''
``seism,'' ``aftershock,'' etc.   


\subsubsection{Language Model Features}\label{subsubsec:lm}

%Because the data in our experiments is scraped from the web, it is common to
%find sentences that contain both salient informantion and two kinds of noise:
%noisey fragments of web page structure (e.g. section titles, \emph{News},
%\emph{Sports}, etc.) and references to other news not relevant to the topic
%summary.
%
We use two trigram language models, trained using the SRILM toolkit
\cite{stolcke2002srilm}, taking as features the average log probability (i.e.
the sentence's total log probability normalized by sentence length) from each
model.  This first model is trained on 4 years (2005-2009) of articles from
the Gigaword corpus.  Specifically, we use articles from the Associated Press
and the New York Times. This model is intended to assess the general writing
quality (grammaticality, word usage) of an input sentence and helps us to
filter out text snippets which are not sentences (e.g., web page titles).  The
second model is a domain specific language model. We build a corpus of
Wikipedia articles for each event type, consisting of documents from a related
Wikipedia category. E.g. for earthquakes, we collect pages under the category
\emph{Category:Earthquakes}. This model assigns higher probability to
sentences that are focused on the given domain.

%For both models, we Finally, we extract the percentage of capitalized words,
%and sentence length as features. These last two features also help to
%identify sentences that are less likely to contain relevant content-- overly
%long and heavily capitalized sentences in our corpus were likely to be long
%strings of web-page headlines, section headers, and other irrelevant page
%structure. 

\subsubsection{Geographic Relevance Features}

Locations are identified using a named entity tagger. For each location in a
sentence, we obtain its latitude and longitude using the Google Maps API.  We
then compute its distance to that of the event location.  It is possible for a
sentence and an event to have multiple locations so we take as features the
minimum, maximum, and average distance of all sentence-event location pairs.
Distances are calculated using the Vincenty distance.

\subsubsection{Temporal Relevance Features}

Our data consists of hourly crawls of online content and so we exploit the
temporality of corpus by capturing the burstiness of a sentence, i.e.  the
change in word frequency from one hour to the next.``Bursty'' sentences often
indicate new and important data. 

Let $D_t$ be the set of web pages at time $t$ and let $s = \{w_1,\ldots,
w_n\}$ be a sentence from a page $d \in D_t$.  We calculate the 1-hour
burstiness of sentence $s$ from document $d$ at hour $t$  as \begin{align*}
\operatorname{b}_1(s,d,t) = \frac{1}{|s|} \sum_{w \in s} \Bigg( &
\operatorname{tf-idf}_t(w,d)  \\ & \left. - \frac{\sum_{d^\prime \in D_{t-1}:
w \in d^\prime } \operatorname{tf-idf}_{t-1}(w,d^\prime)}{|\{d^\prime \in
D_{t-1}: w \in d^\prime\}|} \right) \end{align*}

where \begin{align*} \operatorname{tf-idf}_t(w,d) =&
\log\left(1+\sum_{w^\prime \in d}1\{w=w^\prime\}  \right)\\ & \times
\log\left(\frac{|D_t|}{1 + \sum_{d^\prime \in D_t}1\{w \in d^\prime\}}\right).
\end{align*}
% 1\{w = w^\prime} %- \operatorname{avg-tf-idf}_{t_{i-1}}(w).
%\end{align*}


We similarly find the sentence's 5-hour burstiness.  In addition to
burstiness, we also include the sentence's average tf-idf and hours since the
event in question started as features.


\subsection{Sentence Selection}\label{subsec:SentenceSelection}

In the sentence selection stage, we use the salience predictions from our GP
model as preferences in the AP clustering algorithm. The AP algorithm is 
parameterized by a similarity matrix $\mathbf{S}$ and a vector of 
preferences $\boldsymbol{\pi}$; we found AP to be very sensitive to these
parameters, and did not perform robustly on our range of inputs.
In order to improve the quality of the clusters and exemplar selection,
we re-scaled both the raw inputs $\mathbf{S}$ and $\boldsymbol{\pi}$. 
The raw preferences are scaled to lie within the  range $(-3, -2)$

The initial matrix $\mathbf{S}$ is computed by finding the pairwise semantic
similarity between input sentences. Self-similarities and similarities below 
a threshold $\lambda$ were masked and the remaining values scaled to the range
$(-3, -1)$.

\subsubsection{Preferences}

Before rescaling, we first penalize each $\boldsymbol{\pi}_i$ based on
the aggregate similarity of input sentence $\mathbf{s}_i$ to the set of previous updates
$\mathbf{U}$. We call this the redundancy penalty 

$$\rho_i = \sum_j \frac{\boldsymbol{\pi}^{(U)}_{j}\operatorname{cos-sim}(\mathbf{s}_i, \mathbf{U}_j)}
{\sum_{j^\prime}\operatorname{cos-sim}(\mathbf{s}_i, \mathbf{U}_{j^\prime})} $$   
where $\boldsymbol{\pi}^{(U)}_{j}$ is the salience prediction of the $j$ 
previous update. We calculate our new penalized preferences 
$\boldsymbol{\pi}^{(\rho)}$ where $\boldsymbol{\pi}^{(\rho)}_i = \boldsymbol{\pi}_i - \rho_i$.
Finally, we rescale $\boldsymbol{\pi}^{(\rho)}$ such that all values lie 
within the range $(-3,-2)$.

\subsubsection{Filtering}

We run AP clustering with our rescaled and penalized sentence similarities
and preferences. When the clustering has converged we emit all exemplar
sentences (cluster centers) whose preference is $\lambda$ standard deviations
above the mean preference for the current time period and who do not belong
to singleton clusters as updates. These updates and their preferences are 
retained for penalty calculations in subsequent time steps.


\section{Run Submissions}

We submitted three different runs for official evaluation. The first system,
(AP+) used AP clustering without salience predictions or redundancy penalty.
The clustering was run every hour, and all non-singleton exemplars were 
taken as the updates.

Our second submission (AP+Sal+) uses our salience prediction model, with the
AP clustering to select sentences. We do not penalize for redundancy.

Our final run (AP+Sal+Red-) is the same as the previous run but the 
redundancy penalty is applied.

\section{Results}

Overall it appears that (AP+Sal+) is our best performing model of the three
submissions. One of the notable differences is in the average number of 
updates per event. (AP+Sal+) was by far the most terse system, with 
383.2 updates/event compared to 5997.8 and 1070.7 for (AP+) and (AP+Sal+Red-)
respectively.
Despite not returning many updates, $\mathbb{E}$[Latency Gain] was high enough
on average to outperform on the F1 score compared to the other two systems.
The differences in F1 from the baseline run for both (AP+Sal+) and 
(AP+Sal+Red-) are statistically significant; 
this is empirical validation of our salience modeling efforts.

\begin{figure}
\centering
\begin{tabular}{| c | c | c | c |}
\hline
\textbf{Run} & \textbf{nE[Lat. Gain]} & \textbf{Lat. Comp} & \textbf{F1} \\
\hline
AP+ & 0.0222  & 0.5777 & 0.0403\\
\hline
AP+Sal+ & 0.0751 &  0.4139 & $\mathbf{0.1162}$\\
\hline
AP+Sal+Red- & 0.0375 & 0.3413 & 0.0602\\
\hline
\end{tabular}
\caption{Submission results, averaged over 2014 TREC TS events.}
\end{figure}

Unfortunately, there is still much work to be done with our redundancy 
component. (AP+Sal+Red-) returned an order of magnitude more updates on 
average than (AP+Sal+) but still achieved comparatively lower comprehensiveness
scores. The redundancy penalty is perhaps too aggressive, driving the 
clustering to focus on very novel input spaces---so novel as to be 
irrelevant. 

A related issue with our current model is that our salience predictions
are relative from the point of view of the clustering algorithm. If all inputs
have low salience predictions, the AP clustering will still find exemplars, 
albeit from the most salient inputs. This becomes especially problematic 
with the redundancy component, as with every timestep, we are evaluating 
riskier and riskier sets of more novel sentences, and being forced to return
something. This behavior can be observed especially in the updates from the
Chelyabinsk meteor event, where our document retrieval stage was 
under-performing and the predicted salience of all updates was quite low.
This would suggest implementing some minimum threshold of salience. We
initially experimented with such a threshold but found tuning it across 
events of different magnitudes to be difficult. 


\bibliographystyle{abbrv}
%\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this
%case
\bibliography{cites.bib}
% You must have a proper ".bib" file and remember to run: latex bibtex latex
% latex to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional \balancecolumns \balancecolumns % GM June 2007 That's
%all folks!
\end{document}
