% This is "sig-alternate.tex" V2.0 May 2012 This file should be compiled with
% V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls' V2.5
% LaTeX2e document class file. It is for those submitting articles to ACM
% Conference Proceedings WHO DO NOT WISH TO STRICTLY ADHERE TO THE SIGS
% (PUBS-BOARD-ENDORSED) STYLE.  The 'sig-alternate.cls' file will produce a
% similar-looking, albeit, 'tighter' paper resulting in, invariably, fewer
% pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces: 1) The Permission
% Statement 2) The Conference (location) Info information 3) The Copyright
% Line with ACM data 4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which DOES NOT produce 1) thru'
% 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within the source
% .tex file, over both the CopyrightYear (defaulted to 200X) and the ACM
% Copyright Data (defaulted to X-XXXXX-XX-X/XX/XX).  e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the
% copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use the .bib file (from which
% the .bbl file % is produced).  REMEMBER HOWEVER: After having produced the
% .bbl file, and prior to final submission, you *NEED* to 'insert' your .bbl
% file into your source .tex file so as to provide ONE 'self-contained' source
% file.
%
% ================= IF YOU HAVE QUESTIONS ======================= Questions
% regarding the SIGS styles, SIGS policies and procedures, Conferences etc.
% should be sent to Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate} 
\usepackage{url} 
\usepackage{color}
% \documentclass[10pt]{article} \usepackage{url} \usepackage{color}
\usepackage[utf8]{inputenc}
% \usepackage[margin=1in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[]{algorithm2e}
\DeclareMathOperator{\corpus}{\mathcal{C}}
\DeclareMathOperator{\doc}{\mathnormal{d}}
\DeclareMathOperator{\sent}{\mathnormal{s}}
\DeclareMathOperator{\order}{\pi}
\DeclareMathOperator{\dtime}{\mathnormal{t}}
\DeclareMathOperator{\hour}{\mathnormal{h}}
\DeclareMathOperator{\hours}{\mathcal{H}}
\DeclareMathOperator{\Sim}{\mathbf{K}}
\DeclareMathOperator{\SMat}{\mathbf{X}}
\DeclareMathOperator{\Pref}{\boldsymbol{\pi}}
\DeclareMathOperator{\Exemp}{\mathnormal{Exemplars}}
\DeclareMathOperator{\Updates}{\mathnormal{Updates}}
\usepackage{cleveref}
\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}

\usepackage{authblk}
\newcommand{\fdadd}[1]{\textcolor{red}{#1}}
\newcommand{\fdcomment}[1]{\textbf{\textcolor{red}{[FD: #1]}}}

\begin{document}

%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden
%- IF NEED BE.  \crdata{0-12345-67-8/90/01}  % Allows default copyright data
%(0-89791-88-6/97/05) to be over-ridden - IF NEED BE.  --- End of Author
%Metadata ---

\title{The Anatomy of a Temporal Summarization System}

\numberofauthors{1} %  in this sample file, there are a *total*
\author{}
% \author[1]{Chris Kedzie}
% \author[1]{Kathleen McKeown}
% \author[2]{Fernando Diaz}
% \affil[1]{Columbia University, Department of Computer Science}
% \affil[2]{Microsoft Research}
%    Alternate {\ttlit ACM} SIG Proceedings Paper in LaTeX
%Format\titlenote{(Produces the permission block, and copyright information).
%For use with SIG-ALTERNATE.CLS. Supported by ACM.}} \subtitle{[Extended
%Abstract] \titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement and
% alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time' i.e. three
% 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of "name/affiliations" may
% appear. We just ask that you restrict the number of 'columns' to three.
%
% Because of the available 'opening page real-estate' we ask you to refrain
% from putting more than six authors (two rows with three columns) beneath the
% article title.  More than six makes the first-page appear very cluttered
% indeed.
%
% Use the \alignauthor commands to handle the names and affiliations for an
% 'aesthetic maximum' of six authors.  Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the \additionalauthors
% command.  These 'additional authors' will be output/set for you without
% further effort on your part as the last section in the body of your article
% BEFORE References or any Appendices.

%\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting reasons)
% and the remaining two appear in the \additionalauthors section.
%
%\author{
% You can go ahead and credit any number of authors here, e.g. one 'row of
% three' or two rows (consisting of one row of three and a second row of one,
% two or three).
%
% The command \alignauthor (no curly braces needed) should precede each author
% name, affiliation/snail-mail address and e-mail address. Additionally, tag
% each line of affiliation/address with \affaddr, and tag the e-mail address
% with \email.
%
% 1st. author

%\alignauthor Chris Kedzie\\ \affaddr{Columbia University}\\
%\affaddr{Department of Computer Science} \email{kedzie@cs.columbia.edu}

% 2nd. author
%\alignauthor Kathleen McKeown\\ \affaddr{Columbia University}\\
%\affaddr{Department of Computer Science}\\ \email{kathy@cs.columbia.edu}

% 3rd. author
%\alignauthor Fernando Diaz\\ \affaddr{Microsoft Research}\\
%\email{fdiaz@microsoft.com}

% There's nothing stopping you putting the seventh, eighth, etc.  author on
% the opening page (as the 'third row') but we ask, for aesthetic reasons that
% you place these 'additional authors' in the \additional authors block, viz.
%} 
\date{29 October 2014}
% Just remember to make sure that the TOTAL number of authors is the number
% that will appear on the first page PLUS the number that will appear in the
% \additionalauthors section.

\maketitle \begin{abstract} 
Information need is highest during times of crisis; both officials and 
civilians need effective access to make informed decisions about their 
safety and the safety of others. However, these situations pose many 
interesting challenges for the IR systems that support these decisions.
Specifically, we address event monitoring and updating, a.k.a. the
temporal summarization task (TS). Given an event query, we monitor streams
of online news and select important text to present to the user. Updates
are selected by predicting the salience of the text w.r.t. to the event.
Our model utilizes a variety of features at the query, document, and corpus
level. Additionally, these predictions are used to directly bias a clustering
algorithm for sentence selection, 
increasing the novelty of the updates. We evaluate our system
on a varied set of retrospective events using a combination of human 
judgements
and automatic evaluation measures, as compared to a baseline system.
We demonstrate the effect of the various feature groups, and the importance
of salience prediction w.r.t. update precision when compared to a clustering
only baseline. 


\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures,
%performance measures]

%\terms{Summarization}

%\keywords{Extractive Summarization, Affinity Propagation}

\section{Introduction}

During crises, information is critical for first responders and those caught
in the event.  When the event is significant, as in the case of Hurricane
Sandy, the amount of information produced by traditional news outlets,
government agencies, relief organizations, and social media can vastly
overwhelm those trying to monitor the situation. Methods for identifying,
tracking, and summarizing events from text based input have been explored
extensively  (e.g.,
\cite{allan1998topic,Filatova&Hatzivassiloglou.04a,Wang&al.11}). However,
these experiments were not performed in the large and heterogeneous
environment of the modern web; there is still a need for robust and scalable 
methods for automatic summarization.



%Previous work on generating event descriptions and/or multi-document
%summarization has relied on clustering algorithms to find representative
%sentences appropriate for an event summary.  These methods impose a metric
%space on the text data that can make it difficult to incorporate external
%sources of information elegantly -- in this paper we argue that centroid
%sentences are not a priori the best candidates for inclusion in an event
%summary.

In this paper, we detail our participation in the second year of the TREC
Temporal Summarization (TS) track. Broadly, TS systems are required to monitor
a time ordered corpus for information about a specific event and ``emit'' 
updates about that event. Under the current TREC formulation of this task,
successful TS systems must synthesize techniques from information retrieval,
 extractive multi-document summarization, and update summarization.
Specifically, we must select updates to emit based on three primary criterion:
relevance, salience, and redundancy.

As this was our first time participating in TS track, we focused mostly on 
second and third criterion, relying on the TREC supplied resources for 
document retrieval and filtering heuristics for relevance.
Our system predicts sentence salience in the context of a
large-scale event and integrates these predictions into
a clustering based multi-document summarization system. We fit a Gaussian 
Process (GP) regression model to predict sentence salience and use 
these predictions to bias the
formation of sentence clusters around more salient regions in the input space
using affinity propagation (AP) clustering.  AP uses the salience predictions
as well as pairwise similarities among input sentences to identify
\emph{exemplar} sentences, which we use as our summary output.  Our approach
differs from other methods of summarization that compute salience by pairwise
comparisons alone, ignoring features of importance that are intrinsic to the
sentences themselves.

The remainder of the paper is organized as follows. In section
~\ref{sec:background} we give an overview of the semantic similarity, 
Gaussian process, and affinity propagation algorithms that make up the bulk of
our TS system. 
Next we describe the data with which we build our various models.
Then in section~\ref{sec:approach} we give a high level sketch
of our system, and then explain each component in detail. 
We finish with the presentation of our results and related works.


\section{Background}\label{sec:background}

\subsection{Affinity Propagation}

Affinity propagation (AP) is a message passing algorithm that identifies both
exemplar data points and assignments of each point to an exemplar.  This is
done iteratively by passing \emph{responsibility} and \emph{availability}
messages between data points that quantify the fitness of one data point to
represent another, and the fitness of a data point to be represented based on
the choices of other data points respectively \cite{dueck2007non}.

AP is parameterized by an $n\times n$ similarity matrix $S$ and an $n\times 1$
preference vector $\pi$.  $S$ is a real-valued matrix where $S(i,j)$ is the
similarity of the $i$-th data point to the $j$-th data point.  $S$ does not
need to be symmetric.  $\pi$ is a real-valued vector where $\pi(i)$ expresses
our preference that the $i$-th data point can serve as an exemplar a priori of
other data points. 




AP has several useful properties that comport well with the TS track 
requirements. First, the number of clusters $k$ is not a hyper-parameter
of the model. Since we will be running this algorithm many times in per run,
the usual methods of hyperparameter search become infeasible 
(?). The number of clusters falls out of the algorithm
organically-- lower overall preference values will result in fewer clusters. 

Secondly, the arbitrary nature of the preferences
allow us to incorporate a variety of signals for
identifying the best exemplars, i.e. salience and redundancy signals. 
The preferences can be thought of as self-similarities (similarities that
pairwise-comparison based algorithms would ignore) that we can exploit to 
incorporate our prior beliefs about a data point.

Finally, cluster exemplars are guaranteed to be actual data points. Many 
clustering algorithms group data around mathematical objects (e.g., the
mean) that are not necessarily observed in the data. The extractive nature of
this TS task requires that we emit actual data points (i.e. sentences).
We are able to sidestep 
the additional requirement of selecting a most
representative cluster member, as this is computed explicitly in the AP 
algorithm. 




%Each element $R(i,k)$ expresses the fitness of the $k^{th}$ point to serve as
%the exemplar of the $i^{th}$ point relative to other potential exemplars.
%Each element $A(i, k)$ represents the $k^{th}$ point's ``availability'' to
%serve as an exemplar of the $i^{th}$ point, taking into account other points'
%preference for the $k^{th}$ point as an exemplar.



%While, AP does not set the number of exemplars before-hand, a lower overall
%preference values will result in a smaller number of exemplars.


%arbitrary pair-wise similarity function $S: \mathcal{R}^d \rightarrow
%\mathcal{R}$, where $d$ is the dimension of the data being clustered and a
%real-valued preference $\pi_i$ quantifying our belief a priori of the
%$i^{th}$ element's ability to serve as an exemplar.




\subsection{Semantic Similarity}\label{subsec:semsim}

Whenever we make a pairwise comparison between sentences, we use the weighted
textual matrix factorization (WTMF) model of \cite{guo2012simple}. This 
model can be thought of as a variant of latent semantic analysis (?), 
where words that are not present in a sentence are explicitly modeled.
More formally, we have a term-sentence matrix 
$\mathbf{X}\in\mathcal{R}^{v \times n}$ representing $n$ sentences with a 
vocabulary of $v$ words; $\mathbf{X}_{i,j}$ indicates is non-zero if sentence
$j$ contains word $i$. In the WTMF regime, we want to find an approximation
of $\mathbf{X} \approx \mathbf{P}^T\mathbf{Q}$, where 
$\mathbf{P} \in \mathcal{R}^{k \times v}$ is a latent word vector space and
$\mathbf{Q} \in \mathcal{R}^{k \times n}$ is a latent sentence vector
space. These matrices are found by minimizing the objective function

$$\sum_i^v \sum_j^n \mathbf{W}_{i,j}(\mathbf{P}_{\cdot,i}^T
\mathbf{Q}_{\cdot,j} 
- \mathbf{X}_{i,j})^2 
 + \lambda ||\mathbf{P}||_2^2 + \lambda ||\mathbf{Q}||_2^2$$

where $\mathbf{W}_{i,j} = 
\begin{cases} 1, & \textrm{if $\mathbf{X}_{i,j} \ne 0$ } \\
w_m, & \textrm{if $\mathbf{X}_{i,j} = 0$ }\\
\end{cases}$
and $\lambda$ is a hyperparameter controlling the regularization terms.

The $w_m$ term is another model hyperparameter that is set to a small constant
($\le .01$). The weight matrix $\mathbf{W}$ has the effect of discounting the
reconstruction error of missing terms (words that did not occur a sentence).

Given an unseen sentence $\hat{i}$ we can project its term vector into the
latent sentence vector space with 

$$
\mathbf{Q}_{\cdot,\hat{i}} = (\mathbf{P}\mathbf{\tilde{W}}^{(\hat{i})}
\mathbf{P}^T  + \lambda\mathbf{I} )^{-1} 
\mathbf{P}\mathbf{\tilde{W}}^{(\hat{i})} \mathbf{X}_{\cdot, \hat{i}}
$$  

where $\mathbf{\tilde{W}}^{(\hat{i})}$ is an $v\times v$ diagonal matrix
where $\mathbf{\tilde{W}}^{(\hat{i})}_{j,j}$ is equal to $1$ or $w_m$ 
depending on whether or not the $j$-th term occurs in sentence $\hat{i}$.


The WTMF model is used extensively throughout our TS system. When making any 
pairwise comparison between sentence $i$ and $j$, we first construct
their latent sentence vectors $\mathbf{Q}_{\cdot,i}$ and
$\mathbf{Q}_{\cdot,j}$ and then find the cosine similarity 
$\displaystyle \operatorname{cos-sim}
(\mathbf{Q}_{\cdot,i}, \mathbf{Q}_{\cdot,j}) = 
\frac{\mathbf{Q}_{\cdot,i}^T\mathbf{Q}_{\cdot,j}}{||\mathbf{Q}_{\cdot,i}||_2
||\mathbf{Q}_{\cdot,j}||_2   }$.


Because the events for the TS task come from different domains, we construct
domain specific latent word vector spaces for each domain using in-domain 
Wikipedia pages (see~\cref{sec:data} for more details).

\subsection{Gaussian Processes}

A Gaussian process (GP) is a distribution over functions and is a 
generalization of the multi-variate Gaussian to the infinite dimensional
setting. That is, we use the observed data to define a distribution over 
possible functions that generated this data, without having to explicitly 
parameterize the function---in this sense GPs are considered 
a non-parametric model.

Formally, let $p(f)$ be a distribution over functions where $f$ is any mapping
of an input space $\mathcal{X}$ to the reals,

$$f: \mathcal{X} \rightarrow \mathcal{R}.$$ 
Let the random variable $\mathbf{f} = (f(x_1),\ldots,f(x_n) )$ be
 an $n$-dimensional vector whose elements are evaluations of the function $f$
at points $x_i \in \mathcal{X}$.
We say $p(f)$ is a Gaussian process if for any finite subset 
$\{x_1,\ldots,x_n\} \subset \mathcal{X}$, the marginal distribution over 
that finite subset $p(\mathbf{f})$ has a multivariate Gaussian distribution.
A GP is parameterized by a mean function $\mu(\mathbf{x})$ and a 
covariance function $K(x,x^\prime)$. Generally, the mean function is simply
set to 0, leaving the distribution to be completely characterized by the
kernel function on the data.

In the regression setting, we typically have a response variable $y$ that
is the sum of our model prediction  and 
some Gaussian noise, i.e. $y = f(x) + \epsilon$ with 
$\epsilon \sim \mathcal{N}(0, \sigma^2)$. When
$f \sim \operatorname{GP}(\mathbf{0}, \mathbf{K})$, the
two distributions
of principal interest are the marginal likelihood
$p(\mathbf{y}|\mathbf{X}) = 
\mathcal{N}(\mathbf{0},\mathbf{K} + \sigma^2\mathbf{I})$ and the predictive
distribution,

$$p(\mathbf{y_*}|\mathbf{x_*},\mathbf{X},\mathbf{y}) =
\mathcal{N}(\boldsymbol{\mu}_*, \boldsymbol{\sigma}^2_*) $$

where $\mathbf{x_*}$ is a new or unseen input, $\mathbf{y_*}$ our predicted
response, and
\begin{align*}
\boldsymbol{\mu}_* & = \mathbf{K_*}(\mathbf{K} + \sigma^2\mathbf{I})^{-1}\mathbf{y} \\
\boldsymbol{\sigma}^2_* & 
= \mathbf{K}_{**} - \mathbf{K}_*(\mathbf{K} + \sigma^2\mathbf{I})^{-1}
\mathbf{K}_*^T + \sigma^2\\
\end{align*}.

Here $\mathbf{K}_* = K(\mathbf{x}_*, \mathbf{X})$, and 
$\mathbf{K}_{**} = K(\mathbf{x}_*, \mathbf{x}_*)$.


GP's are incredibly general, and are state of the art for many regression 
tasks (?). The reliance on the covariance matrix 
$\mathbf{K}$ for parameterization opens up the wide world of kernel methods
for regression, and many varieties of similarity functions can be used.
In our experiments we used a radial basis function kernel 
$$K(\mathbf{x},\mathbf{x}^\prime) = \sigma^2 \exp\bigg(- \frac{1}{2} 
\sum_{i=1}^d \frac{ (x_i-x^\prime_i)^2}{\ell_i^2} \bigg)$$ where 
$\sigma$ and the $\ell_i$ are parameters we fit to our observed training data.
The $\ell_i$ are feature dependent scaling parameters; once learned, they not
only improve the accuracy of the model, but give us some introspection 
into which features are more important.

%the formation of sentence clusters around more salient sentences. We
%introduce the affinity propagation algorithm as as an elegant way to
%incorporate our salience predictions into a

%What follows is a description of our ongoing event summarization efforts.  We
%briefly situate our approach to summarization within the broader field of
%multi-document summarization, and then introduce the affinity propagation
%algorithm which we use for clustering. This algorithm allows us to elegantly
%address the salient sentence selection problem by incorporating our prior
%beliefs about sentence quality. Next, we describe out method for modeling
%summary sentence quality, and the features used in this model.  Finally, we
%address future features and system improvements that we are incorporating
%into our summarizer.

\section{Data}\label{sec:data}

Our primary dataset is the TREC 2014 Stream Corpus, a ?tb corpus of hourly 
web crawls from October 2011
through mid February 2013 \cite{frank2012building}.\footnote{\url{http://trec-kba.org/kba-stream-corpus-2014.shtml}}
As per the TS specifications, all summary sentences come from processing this
corpus in a time aligned manner. Additionally, we use the query terms 
provided by the track organizers for document retrieval.

For our semantic similarity component (\cref{subsec:semsim}),
domain specific term-sentence matrices were built using subsections of
Wikipedia. For each event type we identified one or more related Wikipedia
categories and collected all pages that were listed under that category.
For all pages collected, we used the latest possible revision date before the
start of the Stream Corpus.

We use the same Wikipedia corpora to construct the lanuage models used
in our salience prediction model (\cref{subsubsec:lm}). 

\begin{figure*}
	\begin{center}
\begin{tabular}{| p{5cm} c c |}
\hline
Event & WP Categories & No. Docs/Sents/Words\\
\hline \hline
Boston\_Marathon\_bombings \newline
Christopher\_Dorner\_shootings\_and\_manhunt \newline 
 In\_Amenas\_hostage\_crisis
& Terrorism, Mass Shootings & 33,732/1,139,588/26,201,659  \\
\hline
 Cyclone\_Oswald \newline
Early\_2012\_European\_cold\_wave \newline
February\_2013\_noreaster \newline & Weather events & 35,554/591,850/12,794,438  \\
\hline
Costa\_Concordia\_disaster & Accidents & 22,874/732,945/16,520,242 \\
\hline
Chelyabinsk\_meteor & Earthquakes & 14,515/283,509/6,135,803  \\
\hline
Port\_Said\_Stadium\_riot \newline
2012\_Afghanistan\_Quran\_burning\_protests \newline
2011-13\_Russian\_protests \newline
2012\_Romanian\_protests \newline
2012-13\_Egyptian\_protests \newline
2013\_Bulgarian\_protests\_against\_the\_Borisov\_cabinet \newline
2013\_Shahbag\_protests & Activism\_by\_type & 464,657/11,254,122/250,172,896  \\
\hline
\end{tabular}
\caption{Domain specific Wikipedia corpora for semantic similarity and 
language models. }
\end{center}
\end{figure*} 


We use the TREC 2013 Stream Corpus and TS 2013 event/gold nugget data to 
train our salience models. See section~\cref{subsec:Predict} 
for a detailed description of our sampling procedure to generate labeled
data.




%Our documents for summarization come from the online news portion of the TREC
%Stream Corpus, a 6.45tb corpus obtained by hourly web crawls from October 2011
%through mid February 2013
%
%Summary events come from the TREC Temporal Summarization track, and include
%natural disasters like Hurricane Sandy as well as man-made events like a 2012
%train accident in Buenos
%Aires.\footnote{\url{http://trec.nist.gov/data/tempsumm2013.html}} The track
%organizers also provide a search query for each event \cite{aslam2013trec}.
%For each event, we collect the documents that contain all query words and
%stratify them by the hour they were collected. 
%
%For evaluation purposes, the track organizers also provided gold nugget
%information (i.e. important pieces of information, usually the length of a
%short clause or sentence). These gold nuggets come from the event's related
%Wikipedia article and also include the timestamp of when they were added to
%the page.
%
%%KM _ Ask Chris: is this true?
%To create hourly gold summaries to evaluate our system, we simply take the set
%of gold nugget information from the start of the event up to the current hour. 
%



\section{Approach}\label{sec:approach}


We first begin with some notation. For a given event, let $\corpus$ be the set
of retrieved documents. A document $\doc \in \corpus$ is an ordered sequence
of sentences $\{\sent_{1,\doc},\ldots,\sent_{|\doc|,\doc} \}$. 
Additionally, each document has a timestamp $\dtime(\doc)$. Finally, let 
$\corpus_{\hour_i}$ be the set of retrieved documents such that 
$\hour_i \le \dtime(\doc) < \hour_{i+1}$ for all $\doc \in \corpus_{\hour_i}$.

%Our TS system involves ? phases involving sentence level classification, 
%regression, and clustering. 
Figure ? outlines our general temporal summarization algorithm. The description
of our approach is as follows.
For each event, we iterate over the retrieved 
documents in hourly chunks, emitting 0 or more updates at each hour.
At each hour $\hour$, we process each document
$\doc \in \corpus_{\hour}$. First, we identify where the document content
actually is; Second, we predict the salience of all content sentences.
To account for redundancy, the predicted salience is penalized based on the 
distance of the sentence in question to the previous summary updates.

Finally, we cluster all content sentences 
for the current hour using the penalized salience predictions to bias the 
formation
of clusters around the most salient sentences. For each cluster center, or 
exemplar, that results, we check that the salience is above a threshold and 
that it does not belong to a singleton cluster; exemplars that satisfy these
conditions are emitted as an update.
Additionally, we maintain the complete set of updates in order to penalize
salience predictions in the subsequent time steps. 

% \begin{figure}
% \centering
\begin{algorithm}%[H]
 \KwData{$Query$ --- the set of event query words\\ 
        $SC$ --- the stream corpus\\

         
}
 ~\\
 Initialize empty list $\mathbf{U}$ of updates\\
 Initialize empty list $\boldsymbol{\Pref}^{(U)}$ of update preferences\\
 $\corpus \gets $ RetrieveDocuments($Query$, $SC$) (See \cref{subsec:Document Retrieval})\\
 \For{$i \gets 1,\ldots,t $}{

  Initialize empty lists $\SMat, \Pref$ \;
      

  \For{$\doc \in \corpus_{\hour_i}$}{
   \For{$\sent \in \operatorname{getContent}(\doc, Query)$ (See \cref{subsec:Content Detection})\\}{
     $\SMat.\operatorname{append}(\sent)$\;
     $\sigma \gets \operatorname{PredictSalience}(\sent)$ (See \cref{subsec:Predict})\\     
     $\Pref.\operatorname{append}(\sigma)$\;
     
   }        
  }
  %$\Sim \gets \operatorname{ComputeSimilarityMatrix}(X)$\;
  %$\operatorname{}$
  ~\\ 
  $\mathbf{U}_{h_i}, \Pref^{(U)}_{h_i} \gets \operatorname{SentenceSelection}
    (\SMat, \Pref)$ (See \cref{subsec:SentenceSelection})\\
  ~\\
  $\operatorname{Emit}(\mathbf{U}_{h_i})$ \\ 
  $\mathbf{U}\operatorname{.append}(\mathbf{U}_{h_i})$ \\
  $\mathbf{\Pref}^{(U)}\operatorname{.append}(\Pref^{(U)}_{h_i})$ \\
%gets \Updates_{cache} \cup \Updates_{\hour}$\;

 } 
 \caption{Temporal Summarization Algorithm}
\end{algorithm}
% \end{figure}

\subsection{Document Retrieval}\label{subsec:Document Retrieval}

The focus of our system was on salience prediction and clustering stages, and
so we relied heavily on the pre-filtered corpus provided by the track 
organizers. The TREC Temporal Summarization 2014 (TREC-TS-2014F) corpus is
a subset of the full TREC 2014 StreamCorpus and is
intended to be a high recall retrieval of documents related to all 15 of 
the 2014 TS events. 

Track participants were provided with a set of query words for each event.
For example, the event ``Costa Concordia disaster and recovery'' had query
words [``costa'', ``concordia''].
In order to construct an event specific corpus,
we retrieve all documents whose timestamps fall within the event start/stop 
times and whose raw html content contains at least one keyword from the 
event's query words. We further restricted our document set to only those 
articles from the news domain.


\subsection{Content Detection}\label{subsec:Content Detection}

In early versions of our summarization system, we found that structural html
artifacts and sentence tokenization errors
were negatively effecting the performance of later stages. Examples of the 
former include strings of link text like ``World Politics Sports ...'', while
examples of the latter included concatenations of various article headlines,
i.e. headlines pertaining to the event in question, as well as headlines from
other non-related events.
Both types of ``sentences'' were problematic for computing sentence similarity
as they were more likely to have higher average similarity to all input 
sentences. In turn they would be more likely to appear as cluster exemplars
in our clustering stage. From the clustering algorithm's point of view, this
is the correct decision to make---such multi-topic sentences are more general 
than a single topic, and better able to represent all aspects contained in a 
cluster. They make poor choices as updates, however, as they contain 
irrelevant information.

In order to filter out these problematic inputs, we trained a classifier to 
identify which sentences came from inside a document's main article and which
came from various headers, titles, menus, and links to other content. We collected ? random sentences and manually labeled whether the sentence came from
inside or outside the document's main article. We then trained a logistic 
regression classifier using the following features :

\begin{itemize}
 \item the position of the sentence
 \item word counts
 \item the last token in the sentence
 \item the last two tokens in the sentence
 \item the last three tokens in the sentence.
\end{itemize} 

These features were sufficient to capture 
the main difference between content and non-content sentences,
which 
was that content sentences generally ended with sentence final punctuation, 
i.e. periods or a closing quotation mark. 

Within our larger summarization framework, we process a document at a time,
identifying the subset of its sentences that are content sentences.
We then check to make sure \emph{all} event
query terms can be found within the document's content sentences. If so,
we send the content sentences on to the next stage of our pipeline; otherwise
we ignore all sentences in this document.

\subsection{Predicting Sentence Salience}\label{subsec:Predict}

In order to use AP clustering for summarization, we need to assign a
preference value to each input sentence.  In our approach, we equate a
sentence's salience with its preference.  A good model of sentence salience
should predict higher values for sentences that are more likely to appear in a
human generated summary of the event. 

We do not have such human judgments, but we do have last year's gold nugget
sentences.
The response variable we try to predict is a sentence's semantic similarity
(see ?) to the gold nugget sentences, i.e. we want to predict the similarity
to a gold nugget when the gold nugget is not known.


To build training data for this regression task, 
we use the TREC 2013 event/gold nuggets. For each event, we retrieve all 
sentences using the document retrieval and content selection steps outlined
in the previous selection. We then sample with replacement 200 sentences
from this collection, and extract non-lexical features (described in more 
detail below) for each sentence
to construct our design matrix $\mathbf{X}$. To build our response variables
$\mathbf{y}$ we compute the maximum semantic similarity of each input
sentence to the gold nugget sentences. We fit a $\operatorname{GP}$ with
an RBF kernel to this data, optimizing kernel parameters with the scaled 
conjugate gradient method. This sampling procedure is repeated 100 times
for each of the ? 2013 TS events, yielding ? total models. In our salience 
predictions for the 2014 events, we take the mean prediction of all models.

 
%we take a subset of sentences
%relevant to the TREC events (approximately 1000) and match them to the gold
%nugget sentence with highest similarity as determined by the sentence
%similarity system of \cite{guo2012simple}. 
%\cite{} have used this system previously to correlate sentences to meaningful
%units of information in human generated summaries. 
%We use the real-valued similarity scores as our salience scores for the
%training sentences.

We want our model to be predictive across different kinds of events so we
avoid lexical features.  Instead, we extract a variety of features including
language model scores, geographic relevance, and temporal relevance from each
sentence.  These features are used to fit a Gaussian process regression model
that can predict the similarity of a sentence to a gold summary
\cite{preotiuc2013temporal}.  
%We use the model predicted salience of each
%sentence as it's preference value in the AP clustering. 

\subsubsection{Basic Features}

We employ several basic features that have been used previously in supervised
models to rank sentence salience \cite{kupiec1995trainable,conroy2001using}.
These include sentence length, the number of capitalized words normalized by
sentence length, and the number of query words present in the sentence.  Query
words include the event's type (e.g., \emph{earthquake}) and are expanded with
the event type's WordNet \cite{miller1995wordnet} synset, hypernyms, and
hyponyms.  For \emph{earthquake}, e.g., we obtain ``quake,'' ``temblor,''
``seism,'' ``aftershock,'' etc.   


\subsubsection{Language Model Features}\label{subsubsec:lm}

%Because the data in our experiments is scraped from the web, it is common to
%find sentences that contain both salient informantion and two kinds of noise:
%noisey fragments of web page structure (e.g. section titles, \emph{News},
%\emph{Sports}, etc.) and references to other news not relevant to the topic
%summary.
%
We use two trigram language models, trained using the SRILM toolkit
\cite{stolcke2002srilm}, taking as features the average log probability (i.e.
the sentence's total log probability normalized by sentence length) from each
model.  This first model is trained on 4 years (2005-2009) of articles from
the Gigaword corpus.  Specifically, we use articles from the Associated Press
and the New York Times. This model is intended to assess the general writing
quality (grammaticality, word usage) of an input sentence and helps us to
filter out text snippets which are not sentences (e.g., web page titles).  The
second model is a domain specific language model. We build a corpus of
Wikipedia articles for each event type, consisting of documents from a related
Wikipedia category. E.g. for earthquakes, we collect pages under the category
\emph{Category:Earthquakes}. This model assigns higher probability to
sentences that are focused on the given domain.

%For both models, we Finally, we extract the percentage of capitalized words,
%and sentence length as features. These last two features also help to
%identify sentences that are less likely to contain relevant content-- overly
%long and heavily capitalized sentences in our corpus were likely to be long
%strings of web-page headlines, section headers, and other irrelevant page
%structure. 

\subsubsection{Geographic Relevance Features}

Locations are identified using a named entity tagger. For each location in a
sentence, we obtain its latitude and longitude using the Google Maps API.  We
then compute its distance to that of the event location.  It is possible for a
sentence and an event to have multiple locations so we take as features the
minimum, maximum, and average distance of all sentence-event location pairs.
Distances are calculated using the Vincenty distance.

\subsubsection{Temporal Relevance Features}

Our data consists of hourly crawls of online content and so we exploit the
temporality of corpus by capturing the burstiness of a sentence, i.e.  the
change in word frequency from one hour to the next.``Bursty'' sentences often
indicate new and important data. 

Let $D_t$ be the set of web pages at time $t$ and let $s = \{w_1,\ldots,
w_n\}$ be a sentence from a page $d \in D_t$.  We calculate the 1-hour
burstiness of sentence $s$ from document $d$ at hour $t$  as \begin{align*}
\operatorname{b}_1(s,d,t) = \frac{1}{|s|} \sum_{w \in s} \Bigg( &
\operatorname{tf-idf}_t(w,d)  \\ & \left. - \frac{\sum_{d^\prime \in D_{t-1}:
w \in d^\prime } \operatorname{tf-idf}_{t-1}(w,d^\prime)}{|\{d^\prime \in
D_{t-1}: w \in d^\prime\}|} \right) \end{align*}

where \begin{align*} \operatorname{tf-idf}_t(w,d) =&
\log\left(1+\sum_{w^\prime \in d}1\{w=w^\prime\}  \right)\\ & \times
\log\left(\frac{|D_t|}{1 + \sum_{d^\prime \in D_t}1\{w \in d^\prime\}}\right).
\end{align*}
% 1\{w = w^\prime} %- \operatorname{avg-tf-idf}_{t_{i-1}}(w).
%\end{align*}


We similarly find the sentence's 5-hour burstiness.  In addition to
burstiness, we also include the sentence's average tf-idf and hours since the
event in question started as features.


\subsection{Sentence Selection}\label{subsec:SentenceSelection}

In the sentence selection stage, we use the salience predictions from our GP
model as preferences in the AP clustering algorithm. The AP algorithm is 
parameterized by a similarity matrix $\mathbf{S}$ and a vector of 
preferences $\boldsymbol{\pi}$; we found AP to be very sensitive to these
parameters, and did not perform robustly on our range of inputs.
In order to improve the quality of the clusters and exemplar selection,
we re-scaled both the raw inputs $\mathbf{S}$ and $\boldsymbol{\pi}$. 
The raw preferences are scaled to lie within the  range $(-3, -2)$

The initial matrix $\mathbf{S}$ is computed by finding the pairwise semantic
similarity between input sentences. Self-similarities and similarities below 
a threshold $\lambda$ were masked and the remaining values scaled to the range
$(-3, -1)$.

\subsubsection{Preferences}

Before rescaling, we first penalize each $\boldsymbol{\pi}_i$ based on
the aggregate similarity of input sentence $\mathbf{s}_i$ to the set of previous updates
$\mathbf{U}$. We call this the redundancy penalty 

$$\rho_i = \sum_j \frac{\boldsymbol{\pi}^{(U)}_{j}\operatorname{cos-sim}(\mathbf{s}_i, \mathbf{U}_j)}
{\sum_{j^\prime}\operatorname{cos-sim}(\mathbf{s}_i, \mathbf{U}_{j^\prime})} $$   
where $\boldsymbol{\pi}^{(U)}_{j}$ is the salience prediction of the $j$ 
previous update. We calculate our new penalized preferences 
$\boldsymbol{\pi}^{(\rho)}$ where $\boldsymbol{\pi}^{(\rho)}_i = \boldsymbol{\pi}_i - \rho_i$.
Finally, we rescale $\boldsymbol{\pi}^{(\rho)}$ such that all values lie 
within the range $(-3,-2)$.

\subsubsection{Filtering}

We run AP clustering with our rescaled and penalized sentence similarities
and preferences. When the clustering has converged we emit all exemplar
sentences (cluster centers) whose preference is $\lambda$ standard deviations
above the mean preference for the current time period and who do not belong
to singleton clusters as updates. These updates and their preferences are 
retained for penalty calculations in subsequent time steps.


\section{Run Submissions}

We submitted three different runs for official evaluation. The first system,
(AP+) used AP clustering without salience predictions or redundancy penalty.
The clustering was run every hour, and all non-singleton exemplars were 
taken as the updates.

Our second submission (AP+Sal+) uses our salience prediction model, with the
AP clustering to select sentences. We do not penalize for redundancy.

Our final run (AP+Sal+Red-) is the same as the previous run but the 
redundancy penalty is applied.

\section{Results}

Overall it appears that (AP+Sal+) is our best performing model of the three
submissions. One of the notable differences is in the average number of 
updates per event. (AP+Sal+) was by far the most terse system, with 
383.2 updates/event compared to 5997.8 and 1070.7 for (AP+) and (AP+Sal+Red-)
respectively.
Despite not returning many updates, $\mathbb{E}$[Latency Gain] was high enough
on average to outperform on the F1 score compared to the other two systems.
The differences in F1 from the baseline run for both (AP+Sal+) and 
(AP+Sal+Red-) are statistically significant; 
this is empirical validation of our salience modeling efforts.

\begin{figure}
\centering
\begin{tabular}{| c | c | c | c |}
\hline
\textbf{Run} & \textbf{nE[Lat. Gain]} & \textbf{Lat. Comp} & \textbf{F1} \\
\hline
AP+ & 0.0222  & 0.5777 & 0.0403\\
\hline
AP+Sal+ & 0.0751 &  0.4139 & $\mathbf{0.1162}$\\
\hline
AP+Sal+Red- & 0.0375 & 0.3413 & 0.0602\\
\hline
\end{tabular}
\caption{Submission results, averaged over 2014 TREC TS events.}
\end{figure}

Unfortunately, there is still much work to be done with our redundancy 
component. (AP+Sal+Red-) returned an order of magnitude more updates on 
average than (AP+Sal+) but still achieved comparatively lower comprehensiveness
scores. The redundancy penalty is perhaps too aggressive, driving the 
clustering to focus on very novel input spaces---so novel as to be 
irrelevant. 

A related issue with our current model is that our salience predictions
are relative from the point of view of the clustering algorithm. If all inputs
have low salience predictions, the AP clustering will still find exemplars, 
albeit from the most salient inputs. This becomes especially problematic 
with the redundancy component, as with every timestep, we are evaluating 
riskier and riskier sets of more novel sentences, and being forced to return
something. This behavior can be observed especially in the updates from the
Chelyabinsk meteor event, where our document retrieval stage was 
under-performing and the predicted salience of all updates was quite low.
This would suggest implementing some minimum threshold of salience. We
initially experimented with such a threshold but found tuning it across 
events of different magnitudes to be difficult. 



\section{Related Work}

A principal concern in extractive multi-document summarization is the
selection of salient sentences for inclusion in summary output
\cite{nenkova2012survey}.  This has often been approached as a ranking
problem.
%We broadly conceptualize this decision as either an intrinsic or extrinsic
%sentence evaluation process. Intrinsic approaches evaluate sentences
%individually, possibly by predicting the impact on summary quality using
%sentece level features. 
Sentences have been ranked by the average word probability, average tf-idf
score, and the number of topically related words (topic-signatures in the
summarization literature)
\cite{nenkova2005impact,hovy1998automated,lin2000automated}. The first two
statistics are easily computable from the input sentences, while the third
only requires an additional, generic background corpus.  Another ranking
approach, centroid summarization, involves creating an average bag of words
(BOW) vector, the centroid, from the input sentences and ranking sentences by
their similarity to the centroid \cite{radev2004centroid}.  Graph
\cite{erkan2004lexrank} and clustering
\cite{hatzivassiloglou2001simfinder,mckeown1999towards,siddharthan2004syntactic}
based approaches, on the other hand, make use of pair-wise similarity
comparisons amongst input sentences.  In these models, salient sentences are
more central to the input or cluster, respectively.

%identify salient regions of the input space while simultaneously coping with
%redundancy.  Graph-based algorithms have been used to rank sentences
%Clustering algorithms, e.g., are commonly used to exploit redundancy in
%input. Input sentences are clustered and summaries are generated by selecting
%the most representative sentence from each cluster.  Graph-based models have
%also been used for summarization.  E.g., the LexRank algorithm treats
%sentences as nodes in a graph, where edges are constructed by way of cosine
%similarity between sentence nodes; edges are either continuosly weighted by
%similarity or discrete, existing only when the similarity is above a
%threshold.  The PageRank algorithm is used on the graph to find the most
%important sentence nodes. In both clustering and graph-based approaches,
%sentence salience is largely determined by the pairwise relations between
%sentences.

Supervised learning has also been applied to this task. Model features are
usually derived from human generated summaries, and are non-lexical in nature
(e.g., sentence starting position, number of topic-signatures, number of
unique words, word frequencies). Seminal work in this area has employed naive
Bayes and logistic regression classifiers to identify sentences for summary
inclusion \cite{kupiec1995trainable,conroy2001using}. 

%\fdadd{
Several researchers have recognized the importance of summarization during
natural disasters.  Guo \textit{et al.} developed a system for detecting
novel, relevant, and comprehensive sentences immediately after a natural
disaster \cite{qi:temporal-summarization}.  The method uses a model of
sentence relevance and novelty in order to select appropriate updates.
Training data for regression targets is automatically generated from
retrospective Wikipedia data.  The system is evaluated on news documents
related to 197 natural and human disasters from 2009 to 2011 using variants of
Rouge modified to capture novelty, relevance, and comprehensiveness
\cite{lin2004rouge}.  Wang and Li present a clustering-based approach to
efficiency detect important updates during natural disasters
\cite{wang:update-summarization}.  The algorithm works by hierarchically
clustering sentences online, allowing the system to output a more expressive
narrative structure than Guo \textit{et al.}.  The method is evaluated on
official press releases related to Hurricane Wilma  in 2005 using Rouge score
between the system summary and a manually generated target summary.
%}

%This work uses the TREC Stream Corpus data set that is in use by the TREC
%Temporal Summarization track \cite{frank2012building,aslam2013trec} .
%Generally, last year's participants used a pipelined approach to build
%summaries, generally ranking sentences, filtering out all but the most
%relevant, and then performing some sort of deduplication/redundancy removal
%step.  Ranking approaches ranged from simple query word match to more
%sophisticated query expansion and query based language model scoring
%\cite{liu2013ictnet,xu2013trec,baruah2013univ}.  Perhaps most similar to our
%approach, the system of \cite{xu2013trec} uses a weighted combination of
%features (similarity to query, named entity frequency, predicate frequency,
%presence of numerical values, sentence novelty, etc.) to score sentences;
%sentences above a threshold are added to the summary.  Both the weights and
%the threshold are selected by hand.
%
%
%Our system seeks to combine the best of these approaches, using supervised
%learning to predict salience rankings, and directly incorporate this
%information in a clustering algorithm to bias the formation of sentence
%clusters around highly salient regions. 

%Broadly, we conceive of the above 



%Another popular graph-based model




 
%Larger clusters generally indicate more important For example 





%\cite{} also explore the use of a hidden Markov model to predict summary
%inclusion based on previously selected sentences.


 
%ranking.
%
%The field of extractive summarization has long been focused on determing
%sentence importance or salience.  Many These efforts have largely involved
%unsupervised, data-driven approaches, where systems rank input sentences
%using features easily computable from the input, and sometimes an unrelated
%background corpus.  The most basic sentence rankings are by average word
%probability and average TF-IDF score, statistics that are computable from the
%input alone.  One of the more effective techniques, topic-sigantures,
%involves identifying the most topically related words in the input sentences.
%This is done using a log-likelihood ratio test ($\chi^2$) to determine which
%of two hypotheses hold for a word in the input: (1) the probability of the
%word in the input distribution and the background distribution are identical,
%or (2) the probability of the word is greater in the input distribution than
%in the background distribution. If (2) is true with some statistical
%significance, the word is said to be a topic signature. As with the previous
%features, a common approach is to rank sentences by the number of
%topic-signatures they contain.
%
%In mulit-document summarization, clustering algorthims are often employed to
%determine sentence importance, with larger clusters corresponding to more
%important topics. 
%
%Supervised learning approaches have also been explored. Features are usually
%derived from human generated summaries, and are non-lexical in nature (e.g.,
%sentence starting position, number of topic-signatures, number of unique
%words, word frequencies). Seminal work in this area has employed naive Bayes
%and logistic regression classifiers to identify sentences for summary
%inclusion. \cite{} also explore the use of a hidden Markov model to predict
%summary inclusion based on previously selected sentences.
%
%If this  
%
%tried to rank sentences by their overlap with query words. 
%


%and models are developed to predict the likelihood of a sentences inclusion
%in a summary. Naive Bayes, hidden Markov models, and logistic regression to
%predict a sentence's summary inclusion.
%
%Researches have also experimented with supervised learning for summarization,
%however, machine learning approaches have not yielded significant advances in
%generic summarization. One difficulty is that gold data is difficult to
%obtain.
%
%
%In  
%
%
%%which remain competetive to supervised machine-learning techniques.
%
%have ranked sentences by
%
%
%their average word probability, average TF-IDF weighting, and/or topic
%signatures; these representations are all easily computable from the input
%sentences, and, for topic signatures, an unrelated background corpus.  Topic
%signatures in
%
% 
%These efforts can be broadly split into two camps: feature based ranking and
%clustering.
%
%
%
%Determing sentence importance has been A large portion of automatic
%summarization research has focused on  
%
%

%\newcommand{corpus}{}





%A common approach to automatic summarization is to identify sentences with the
%highest centrality with respect to the input sentences.  Intuitively,
%sentences with a high degree of centrality are more semantically related to
%the entire set of input sentences.  A summary can thus be obtained by
%returning the $k$ most central sentences.  This generally implies the
%calculation of pairwise distances between all sentences
%\cite{radev2004centroid, erkan2004lexrank}.  In these approaches, sentences
%are evaluated extrinsically by their distance to other sentences, either
%directly \cite{erkan2004lexrank} or through an aggregate centroid object
%\cite{radev2004centroid}. The distance between sentences is most commonly the
%cosine distance of sentence term-vectors, but in general this can be an
%arbitrary real-valued similarity function.
%%; the distance function imposes a topology on the space of input sentences.
%
%For some domains, it is very likely that we will have additional background
%knowledge that could be predictive of sentence salience for the event being
%summarized.  For example, certain kinds of information extracted from the
%sentence text (e.g., temporal or geographic proximity) can indicate relevance
%to a given event.  It would be difficult to incorporate this kind of salience
%into the measure of centrality.
%
%
%For example, consider a cluster of three sentence vectors $s_1 = (1,1,0)$,
%$s_2 = (1,1,1)$, and $s_3 = (0,1,1)$. Without any other information, $s_2$ has
%the highest degree of centrality, i.e. it has the highest average cosine
%similarity and smallest average Euclidean distance to the other sentences.
%Now, if we believe that $s_1$ is $\alpha$ times more salient than the other
%sentences, we cannot simply scale $s_1$ by $\alpha$--the average cosine
%similarity will remain unchanged, since the vector magnitude does not affect
%the angle and $s_1$ will have an even greater the average Euclidean distance
%from the rest.  Worse still, $s_2$ will still be the most representative
%sentence of the three.
%
%


%In order to predict the salience score for a sentence, 

 %\cite{guo2012simple} produces a real-valued score of the similarity We use
 %\cite{guo2012simple} produces real valued similarity scores s.
 %\cite{guo2012simple} determines the similarity of two sentences by deriving
 %low dimensional We have As a proxy measure for sentence salience, we use the
 %maximum similarity of an input sentence to a human generated summary
 %sentence, using the state-of-the-art sentence similarity module of
 %\cite{guo2012simple}. \cite{guo2012simple} derive a lower-dimensional vector
 %of latent features for a sentence. The similarity of two sentences is then
 %the cosine similarity of the latent vectors.  In order to predict this
 %similarity we extract a variety of features including language model scores,
 %geographic relevance, and temporal relevance.  We learn a predictive
 %function of sentence-summary similarity by fitting a Gaussian process model
 %on these features. \cite{preotiuc2013temporal}

%As training data for this model, we take a portion of the sentences found in
%the relevant documents and find their highest similarity to the gold nugget
%information seen thus far.  

%When assigning preference scores


%After obtaining similarity scores for sentences in our training set, we fit a
%Gaussian process model to learn 


%Our model 




%Correllating automatic measures of sentence salience with human judgments is
%a difficult task. , we use the maximum cosine similarity to 



%For example, if we scaled the BOW vector based on our belief in its salience,
%it would not effect the cosine distance since the angle between two vectors
%is not changed by the magnitude of either

%Simply scaling the sentence vector based on our belief in it's fitness does
%not with our prior beliefs-- When using the BOWs representation, the input
%space is lexically defined, but our salience metric may not be based on any
%specific lexical items. 


%These approaches make it difficult to leverage prior knowledge to guide the
%selection of salient sentences. Sentences 


%
%Centroid based algorthims are commonly employed in the summarization
%literature to compress a large number of sentences into a much smaller,
%representative set. 
%
%The $k$-medoids clustering algorithm is commonly used to partition input
%sentences into topically distinct clusters, where the most representative
%sentences (the cluster medoids) are taken to build the summary. The quality
%of the summary in this framework is dependent on the method of sentence
%representation and the ability of a similarity or distance metric to
%adequately capture pairwise comparisons between sentences. Previous work in
%event clustering has examined the use of different term weightings (tf-idf,
%named-entities)  and lower dimensional term representations (LSA, LDA) for
%use in summarization.
%
%
%A popular graph-based alternative, LexRank, treats each sentence as a node in
%a graph, and identifies the nodes that are most central by in-degree.  The
%existence of an edge between two sentences is determined by a similarity
%function (either by setting a threshold above which the edge exists, or by
%taking the similarity as a continuous edge weight). The sentences with the
%highest eigenvector centrality can be found using random walks (PageRank).
%The $k$ most central sentences are used as the summary.
%
%One drawback of these algorithms is that they do not provide an elegant way
%to incorporate prior information about sentence salience.


%Move this out!
%In our approach, the system generates clusters using an affinity propagation
%algorithm and from each cluster an exemplar sentence is selected that is added
%to the summary.  In the following sections, we show how  prior information
%representing salience can easily be incorporated into the affinity propagation
%algorithm.  We believe the incorporation of salience to be useful in noisy
%environments (e.g., a web crawl), and that it can help the formation of
%clusters around the most relevant inputs. Our current system  is trained using
%features derived from location, changes in wording across time and language
%models that characterize the language of disaster to generate summaries at
%regular intervals across time.  As we develop the system further, we will
%extend it to generate updates across time,  penalizing the salience of
%concepts already selected by the summarizer to encourage the discovery of
%novel sentences as the event unfolds. 
%









%\begin{table*}[ht] \begin{center} \begin{tabular}{| c p{7.0cm} || c p{7.0cm}
%|} \hline Preference & AP Clustering & Preference & $k$-Medoids Clustering\\
%\hline 9.010 & The magnitude-7.5 quake, about 20 miles deep, was centered off
%the town of Champerico .People fled buildings in Guatemala City , in Mexico
%City and in the capital of the Mexican state of Chiapas , across the border
%from Guatemala & 9.010 & The magnitude-7.5 quake, about 20 miles deep, was
%centered off the town of Champerico .People fled buildings in Guatemala City ,
%in Mexico City and in the capital of the Mexican state of Chiapas , across the
%border from Guatemala   \\ \hline 9.010 & A reporter in the town of San Marcos
%, about 80 miles north of the epicenter, told local radio station Emisoras
%Unidas that houses had collapsed onto residents and smashed televisions and
%other appliances had been scattered into the streets.  & 3.007 & ``Things fell
%in my kitchen .'' Perez said more than 2,000 soldiers were deployed from a
%base in San Marcos to help with disaster relief.\\ \hline 9.007 & The local
%fire department said on its Twitter account that a school had collapsed and
%eight injured people had been taken to a nearby hospital.  & 3.007 &  Ingrid
%Lopez , who went to the hospital with a 72-year-old aunt whose legs was
%crushed by a falling wall, said she had waited hours for an X-ray.\\ \hline
%7.008 & There are three confirmed dead and many missing after the strongest
%earthquake to hit Guatemala since a deadly 1976 quake that killed 23,000.  &
%1.007 & Hundreds of people crammed into the hallways of the small town
%hospital waiting for medical staff to help out hundreds of injured family
%members, some complaining they were not getting care quickly enough. \\ \hline
%\end{tabular} \end{center} \caption{Example summary using affinity propagation
%(left) and $k$-medoids (right)} \end{table*} \begin{table*}[t] \begin{center}
%\begin{tabular}{| c || c | c | c || c | c | c || c | c | c |} \hline Method &
%\multicolumn{3}{c}{ROUGE-1} & \multicolumn{3}{c}{ROUGE-2} &
%\multicolumn{3}{c}{ROUGE-3} \\ & Recall & Prec. & F-1 & Recall & Prec. & F-1 &
%Recall & Prec. & F-1 \\ \hline $k$-medoids & 0.127 & 0.414 & 0.181 & 0.025 &
%0.076 & 0.035 & 0.003 & 0.010 & 0.005\\ \hline AP & 0.117 & 0.440 & 0.173 &
%0.022 & 0.082 & 0.033 & 0.004 & 0.015 & 0.006\\ \hline \end{tabular}
%\end{center} \caption{ROUGE scores for $k$-medoids and affinity propagation
%methods} \end{table*}
%
%
%
%
%\section{Experiments}
%
%We carried out a small set of initial experiments on one event.  We collected
%a subset of pages from the TREC Stream Corpus that were relevant to a 2012
%earthquake off the coast of Guatemala, and further subdivided this collection
%by the hour they were created. For each hour we generated a summary using the
%AP clustering algorithm. 
%
% We also generated baseline summaries using the $k$-medoids (using the
% Partitioning Around Medoids algorithm), setting $k = |\mathcal{E}|$ , i.e.
% the number of exemplar sentences returned by the AP. Because $k$-medoids
% begins with a random initialization, we took the best (minimum average
% distance) result of 100 restarts.
%
%Table 1 shows example output of the AP and $k$-medoids generated summaries.
%Sentences are ranked by preference score, although preference has no effect on
%the $k$-medoids algorithm.  Quantitatively, AP exemplar sentences had higher
%predicted sentence quality scores (preferences) than the cluster medoids.
%Qualitatively, the AP method appears to select more general details about the
%earthquake. Looking at the third sentence selection in table 1, we can see
%that $k$-medoids selects a personal experience that was reported. This is
%perhaps less newsworthy or reportable compared to the third sentence in in the
%AP generated summary which reports a notable structure collapse and injuries
%related to the quake.  We believe AP results in a more readable and
%informative summary, although we have yet to perform a rigorous human
%evaluation of the summary output.
%
%
%We evaluated both algorithms with the ROUGE toolkit \cite{lin2004rouge}.
%N-ROUGE works by calculating the n-gram recall and precision of an
%automatically generated summary in reference to a model summary.  We created
%model summaries by taking the gold nugget sentences with timestamps up to and
%including the current system time as the gold summary for that hour.
%%\fdcomment{may be interesting to measure track metrics to measure latency,
%%redundancy, etc.} KM - Ask Chris
%
%Table 2 shows average recall, precision, and F-measure for various orders of
%ROUGE score. AP demonstrated consistently higher precision than our baseline.
%%scores over $k$-medoids (ROUGE evaluations are usually focused on recall).
%While not statistically significant, it is difficult to show significance with
%Rouge using a small test; we hope further tests will confirm this improvement. 
%%   , AP had higher precision on average.
%On average, the AP summaries were slightly shorter than the baseline, which
%would partially explain this difference. It is also possible that our language
%models are biased toward shorter sentences; we are more likely to have seen a
%shorter sentence in the language model input.
%%\fdcomment{is there a theory to why this is happening?}
%We are currently adapting our summarizer to add updates over time, and
%maintaining precision will be important to prevent topic drift.  
%
%
%
%%\begin{table}[h!] \tiny \begin{center} \begin{tabular}{| c p{3.5cm} |} \hline
%%Preference & Sentence\\ \hline 9.010 & The magnitude-7.5 quake, about 20
%%miles deep, was centered off the town of Champerico .People fled buildings in
%%Guatemala City , in Mexico City and in the capital of the Mexican state of
%%Chiapas , across the border from Guatemala\\ \hline 3.007 & "Things fell in
%%my kitchen ." Perez said more than 2,000 soldiers were deployed from a base
%%in San Marcos to help with disaster relief.\\ \hline 3.007 & Ingrid Lopez ,
%%who went to the hospital with a 72-year-old aunt whose legs was crushed by a
%%falling wall, said she had waited hours for an X-ray. \\ \hline 1.007 &
%%Hundreds of people crammed into the hallways of the small town hospital
%%waiting for medical staff to help out hundreds of injured family members,
%%some complaining they were not getting care quickly enough. \\ \hline
%%\end{tabular} \end{center} %  \caption{Example summary using $k$-medoids with
%%100 random restarts} \end{table}
%
%
%

%\section{Conclusions and Future Work}

%We have developed a summarizer that can generate summaries over time from web
%crawls on disasters. We show that our method of identifying exemplar sentences
%for a summary using AP clustering produces  summaries with higher precision
%    compared to those based on clustering with K-medoids. A key component of
%    our approach is the prediction of salient information using features based
%    on location, temporal changes in topic, and two different language models.
%
%Currently, we run each hour of summarization independently. In order to avoid
%repeating information, we would like to incorporate previously chosen
%exemplars in the preference computation. One possibility would be to
%down-weight a candidate exemplar's preference based on its similarity to
%previous exemplars.
%
%Secondly, we would like to do more intelligent inference of missing
%geographical information since not all sentences contain locations. Currently
%we are using mean values for missing data.
%
%Finally, we would like to experiment with non-symmetric similarity matrices,
%specifically using narrative chains\cite{chambers2009unsupervised}. Under this
%model $S(i,j)$ would express the likelihood that the events in sentence $j$
%precede the events in sentence $i$. We hope such a parameterization would
%promote more causally motivated sentences into exemplar positions, which would
%better describe the disaster event domain.
%
%
\bibliographystyle{abbrv}
%\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this
%case
\bibliography{cites.bib}
% You must have a proper ".bib" file and remember to run: latex bibtex latex
% latex to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional \balancecolumns \balancecolumns % GM June 2007 That's
%all folks!
\end{document}
