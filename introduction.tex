\section{Introduction}
\label{sec:introduction}
During crises, information is critical for first responders and those caught
in the event.  When the event is significant, as in the case of Hurricane
Sandy, the amount of information produced by traditional news outlets,
government agencies, relief organizations, and social media can vastly
overwhelm those trying to monitor the situation. Methods for identifying,
tracking, and summarizing events from text based input have been explored
extensively  (e.g.,
\cite{allan1998topic,Filatova&Hatzivassiloglou.04a,Wang&al.11}). However,
these experiments were not performed in the large and heterogeneous
environment of the modern web; there is still a need for robust and scalable 
methods for automatic summarization.



%Previous work on generating event descriptions and/or multi-document
%summarization has relied on clustering algorithms to find representative
%sentences appropriate for an event summary.  These methods impose a metric
%space on the text data that can make it difficult to incorporate external
%sources of information elegantly -- in this paper we argue that centroid
%sentences are not a priori the best candidates for inclusion in an event
%summary.

In this paper, we detail our participation in the second year of the TREC
Temporal Summarization (TS) track. Broadly, TS systems are required to monitor
a time ordered corpus for information about a specific event and ``emit'' 
updates about that event. Under the current TREC formulation of this task,
successful TS systems must synthesize techniques from information retrieval,
 extractive multi-document summarization, and update summarization.
Specifically, we must select updates to emit based on three primary criterion:
relevance, salience, and redundancy.

As this was our first time participating in TS track, we focused mostly on 
second and third criterion, relying on the TREC supplied resources for 
document retrieval and filtering heuristics for relevance.
Our system predicts sentence salience in the context of a
large-scale event and integrates these predictions into
a clustering based multi-document summarization system. We fit a Gaussian 
Process (GP) regression model to predict sentence salience and use 
these predictions to bias the
formation of sentence clusters around more salient regions in the input space
using affinity propagation (AP) clustering.  AP uses the salience predictions
as well as pairwise similarities among input sentences to identify
\emph{exemplar} sentences, which we use as our summary output.  Our approach
differs from other methods of summarization that compute salience by pairwise
comparisons alone, ignoring features of importance that are intrinsic to the
sentences themselves.

The remainder of the paper is organized as follows. We begin with a review of related work in the information retrieval and multi-document summarization literature.  In section
~\ref{sec:background} we give an overview of the semantic similarity, 
Gaussian process, and affinity propagation algorithms that make up the bulk of
our TS system. 
Next we describe the data with which we build our various models.
Then in section~\ref{sec:approach} we give a high level sketch
of our system, and then explain each component in detail. 
