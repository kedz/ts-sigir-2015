\section{Algorithms}\label{sec:background}

\subsection{Affinity Propagation}

Affinity propagation (AP) is a message passing algorithm that identifies both
exemplar data points and assignments of each point to an exemplar.  This is
done iteratively by passing \emph{responsibility} and \emph{availability}
messages between data points that quantify the fitness of one data point to
represent another, and the fitness of a data point to be represented based on
the choices of other data points respectively \cite{dueck2007non}.

AP is parameterized by an $n\times n$ similarity matrix $S$ and an $n\times 1$
preference vector $\pi$.  $S$ is a real-valued matrix where $S(i,j)$ is the
similarity of the $i$-th data point to the $j$-th data point.  $S$ does not
need to be symmetric.  $\pi$ is a real-valued vector where $\pi(i)$ expresses
our preference that the $i$-th data point can serve as an exemplar a priori of
other data points. 




AP has several useful properties that comport well with the TS track 
requirements. First, the number of clusters $k$ is not a hyper-parameter
of the model. Since we will be running this algorithm many times in per run,
the usual methods of hyperparameter search become infeasible 
(?). The number of clusters falls out of the algorithm
organically-- lower overall preference values will result in fewer clusters. 

Secondly, the arbitrary nature of the preferences
allow us to incorporate a variety of signals for
identifying the best exemplars, i.e. salience and redundancy signals. 
The preferences can be thought of as self-similarities (similarities that
pairwise-comparison based algorithms would ignore) that we can exploit to 
incorporate our prior beliefs about a data point.

Finally, cluster exemplars are guaranteed to be actual data points. Many 
clustering algorithms group data around mathematical objects (e.g., the
mean) that are not necessarily observed in the data. The extractive nature of
this TS task requires that we emit actual data points (i.e. sentences).
We are able to sidestep 
the additional requirement of selecting a most
representative cluster member, as this is computed explicitly in the AP 
algorithm. 




%Each element $R(i,k)$ expresses the fitness of the $k^{th}$ point to serve as
%the exemplar of the $i^{th}$ point relative to other potential exemplars.
%Each element $A(i, k)$ represents the $k^{th}$ point's ``availability'' to
%serve as an exemplar of the $i^{th}$ point, taking into account other points'
%preference for the $k^{th}$ point as an exemplar.



%While, AP does not set the number of exemplars before-hand, a lower overall
%preference values will result in a smaller number of exemplars.


%arbitrary pair-wise similarity function $S: \mathcal{R}^d \rightarrow
%\mathcal{R}$, where $d$ is the dimension of the data being clustered and a
%real-valued preference $\pi_i$ quantifying our belief a priori of the
%$i^{th}$ element's ability to serve as an exemplar.




\subsection{Semantic Similarity}\label{subsec:semsim}

Whenever we make a pairwise comparison between sentences, we use the weighted
textual matrix factorization (WTMF) model of \cite{guo2012simple}. This 
model can be thought of as a variant of latent semantic analysis (?), 
where words that are not present in a sentence are explicitly modeled.
More formally, we have a term-sentence matrix 
$\mathbf{X}\in\mathcal{R}^{v \times n}$ representing $n$ sentences with a 
vocabulary of $v$ words; $\mathbf{X}_{i,j}$ indicates is non-zero if sentence
$j$ contains word $i$. In the WTMF regime, we want to find an approximation
of $\mathbf{X} \approx \mathbf{P}^T\mathbf{Q}$, where 
$\mathbf{P} \in \mathcal{R}^{k \times v}$ is a latent word vector space and
$\mathbf{Q} \in \mathcal{R}^{k \times n}$ is a latent sentence vector
space. These matrices are found by minimizing the objective function

$$\sum_i^v \sum_j^n \mathbf{W}_{i,j}(\mathbf{P}_{\cdot,i}^T
\mathbf{Q}_{\cdot,j} 
- \mathbf{X}_{i,j})^2 
 + \lambda ||\mathbf{P}||_2^2 + \lambda ||\mathbf{Q}||_2^2$$

where $\mathbf{W}_{i,j} = 
\begin{cases} 1, & \textrm{if $\mathbf{X}_{i,j} \ne 0$ } \\
w_m, & \textrm{if $\mathbf{X}_{i,j} = 0$ }\\
\end{cases}$
and $\lambda$ is a hyperparameter controlling the regularization terms.

The $w_m$ term is another model hyperparameter that is set to a small constant
($\le .01$). The weight matrix $\mathbf{W}$ has the effect of discounting the
reconstruction error of missing terms (words that did not occur a sentence).

Given an unseen sentence $\hat{i}$ we can project its term vector into the
latent sentence vector space with 

$$
\mathbf{Q}_{\cdot,\hat{i}} = (\mathbf{P}\mathbf{\tilde{W}}^{(\hat{i})}
\mathbf{P}^T  + \lambda\mathbf{I} )^{-1} 
\mathbf{P}\mathbf{\tilde{W}}^{(\hat{i})} \mathbf{X}_{\cdot, \hat{i}}
$$  

where $\mathbf{\tilde{W}}^{(\hat{i})}$ is an $v\times v$ diagonal matrix
where $\mathbf{\tilde{W}}^{(\hat{i})}_{j,j}$ is equal to $1$ or $w_m$ 
depending on whether or not the $j$-th term occurs in sentence $\hat{i}$.


The WTMF model is used extensively throughout our TS system. When making any 
pairwise comparison between sentence $i$ and $j$, we first construct
their latent sentence vectors $\mathbf{Q}_{\cdot,i}$ and
$\mathbf{Q}_{\cdot,j}$ and then find the cosine similarity 
$\displaystyle \operatorname{cos-sim}
(\mathbf{Q}_{\cdot,i}, \mathbf{Q}_{\cdot,j}) = 
\frac{\mathbf{Q}_{\cdot,i}^T\mathbf{Q}_{\cdot,j}}{||\mathbf{Q}_{\cdot,i}||_2
||\mathbf{Q}_{\cdot,j}||_2   }$.


Because the events for the TS task come from different domains, we construct
domain specific latent word vector spaces for each domain using in-domain 
Wikipedia pages (see~\cref{sec:data} for more details).

\subsection{Gaussian Processes}

A Gaussian process (GP) is a distribution over functions and is a 
generalization of the multi-variate Gaussian to the infinite dimensional
setting. That is, we use the observed data to define a distribution over 
possible functions that generated this data, without having to explicitly 
parameterize the function---in this sense GPs are considered 
a non-parametric model.

Formally, let $p(f)$ be a distribution over functions where $f$ is any mapping
of an input space $\mathcal{X}$ to the reals,

$$f: \mathcal{X} \rightarrow \mathcal{R}.$$ 
Let the random variable $\mathbf{f} = (f(x_1),\ldots,f(x_n) )$ be
 an $n$-dimensional vector whose elements are evaluations of the function $f$
at points $x_i \in \mathcal{X}$.
We say $p(f)$ is a Gaussian process if for any finite subset 
$\{x_1,\ldots,x_n\} \subset \mathcal{X}$, the marginal distribution over 
that finite subset $p(\mathbf{f})$ has a multivariate Gaussian distribution.
A GP is parameterized by a mean function $\mu(\mathbf{x})$ and a 
covariance function $K(x,x^\prime)$. Generally, the mean function is simply
set to 0, leaving the distribution to be completely characterized by the
kernel function on the data.

In the regression setting, we typically have a response variable $y$ that
is the sum of our model prediction  and 
some Gaussian noise, i.e. $y = f(x) + \epsilon$ with 
$\epsilon \sim \mathcal{N}(0, \sigma^2)$. When
$f \sim \operatorname{GP}(\mathbf{0}, \mathbf{K})$, the
two distributions
of principal interest are the marginal likelihood
$p(\mathbf{y}|\mathbf{X}) = 
\mathcal{N}(\mathbf{0},\mathbf{K} + \sigma^2\mathbf{I})$ and the predictive
distribution,

$$p(\mathbf{y_*}|\mathbf{x_*},\mathbf{X},\mathbf{y}) =
\mathcal{N}(\boldsymbol{\mu}_*, \boldsymbol{\sigma}^2_*) $$

where $\mathbf{x_*}$ is a new or unseen input, $\mathbf{y_*}$ our predicted
response, and
\begin{align*}
\boldsymbol{\mu}_* & = \mathbf{K_*}(\mathbf{K} + \sigma^2\mathbf{I})^{-1}\mathbf{y} \\
\boldsymbol{\sigma}^2_* & 
= \mathbf{K}_{**} - \mathbf{K}_*(\mathbf{K} + \sigma^2\mathbf{I})^{-1}
\mathbf{K}_*^T + \sigma^2\\
\end{align*}.

Here $\mathbf{K}_* = K(\mathbf{x}_*, \mathbf{X})$, and 
$\mathbf{K}_{**} = K(\mathbf{x}_*, \mathbf{x}_*)$.


GP's are incredibly general, and are state of the art for many regression 
tasks (?). The reliance on the covariance matrix 
$\mathbf{K}$ for parameterization opens up the wide world of kernel methods
for regression, and many varieties of similarity functions can be used.
In our experiments we used a radial basis function kernel 
$$K(\mathbf{x},\mathbf{x}^\prime) = \sigma^2 \exp\bigg(- \frac{1}{2} 
\sum_{i=1}^d \frac{ (x_i-x^\prime_i)^2}{\ell_i^2} \bigg)$$ where 
$\sigma$ and the $\ell_i$ are parameters we fit to our observed training data.
The $\ell_i$ are feature dependent scaling parameters; once learned, they not
only improve the accuracy of the model, but give us some introspection 
into which features are more important.

%the formation of sentence clusters around more salient sentences. We
%introduce the affinity propagation algorithm as as an elegant way to
%incorporate our salience predictions into a

%What follows is a description of our ongoing event summarization efforts.  We
%briefly situate our approach to summarization within the broader field of
%multi-document summarization, and then introduce the affinity propagation
%algorithm which we use for clustering. This algorithm allows us to elegantly
%address the salient sentence selection problem by incorporating our prior
%beliefs about sentence quality. Next, we describe out method for modeling
%summary sentence quality, and the features used in this model.  Finally, we
%address future features and system improvements that we are incorporating
%into our summarizer.



\subsection{Temporal Summarization System}\label{sec:approach}


We first begin with some notation. For a given event, let $\corpus$ be the set
of retrieved documents. A document $\doc \in \corpus$ is an ordered sequence
of sentences $\{\sent_{1,\doc},\ldots,\sent_{|\doc|,\doc} \}$. 
Additionally, each document has a timestamp $\dtime(\doc)$. Finally, let 
$\corpus_{\hour_i}$ be the set of retrieved documents such that 
$\hour_i \le \dtime(\doc) < \hour_{i+1}$ for all $\doc \in \corpus_{\hour_i}$.

%Our TS system involves ? phases involving sentence level classification, 
%regression, and clustering. 
Figure ? outlines our general temporal summarization algorithm. The description
of our approach is as follows.
For each event, we iterate over the retrieved 
documents in hourly chunks, emitting 0 or more updates at each hour.
At each hour $\hour$, we process each document
$\doc \in \corpus_{\hour}$. First, we identify where the document content
actually is; Second, we predict the salience of all content sentences.
To account for redundancy, the predicted salience is penalized based on the 
distance of the sentence in question to the previous summary updates.

Finally, we cluster all content sentences 
for the current hour using the penalized salience predictions to bias the 
formation
of clusters around the most salient sentences. For each cluster center, or 
exemplar, that results, we check that the salience is above a threshold and 
that it does not belong to a singleton cluster; exemplars that satisfy these
conditions are emitted as an update.
Additionally, we maintain the complete set of updates in order to penalize
salience predictions in the subsequent time steps. 

% \begin{figure}
% \centering
\begin{algorithm}%[H]
 \KwData{$Query$ --- the set of event query words\\ 
        $SC$ --- the stream corpus\\

         
}
 ~\\
 Initialize empty list $\mathbf{U}$ of updates\\
 Initialize empty list $\boldsymbol{\Pref}^{(U)}$ of update preferences\\
 $\corpus \gets $ RetrieveDocuments($Query$, $SC$) (See \cref{subsec:Document Retrieval})\\
 \For{$i \gets 1,\ldots,t $}{

  Initialize empty lists $\SMat, \Pref$ \;
      

  \For{$\doc \in \corpus_{\hour_i}$}{
   \For{$\sent \in \operatorname{getContent}(\doc, Query)$ (See \cref{subsec:Content Detection})\\}{
     $\SMat.\operatorname{append}(\sent)$\;
     $\sigma \gets \operatorname{PredictSalience}(\sent)$ (See \cref{subsec:Predict})\\     
     $\Pref.\operatorname{append}(\sigma)$\;
     
   }        
  }
  %$\Sim \gets \operatorname{ComputeSimilarityMatrix}(X)$\;
  %$\operatorname{}$
  ~\\ 
  $\mathbf{U}_{h_i}, \Pref^{(U)}_{h_i} \gets \operatorname{SentenceSelection}
    (\SMat, \Pref)$ (See \cref{subsec:SentenceSelection})\\
  ~\\
  $\operatorname{Emit}(\mathbf{U}_{h_i})$ \\ 
  $\mathbf{U}\operatorname{.append}(\mathbf{U}_{h_i})$ \\
  $\mathbf{\Pref}^{(U)}\operatorname{.append}(\Pref^{(U)}_{h_i})$ \\
%gets \Updates_{cache} \cup \Updates_{\hour}$\;

 } 
 \caption{Temporal Summarization Algorithm}
\end{algorithm}
% \end{figure}

\subsubsection{Document Retrieval}\label{subsec:Document Retrieval}

The focus of our system was on salience prediction and clustering stages, and
so we relied heavily on the pre-filtered corpus provided by the track 
organizers. The TREC Temporal Summarization 2014 (TREC-TS-2014F) corpus is
a subset of the full TREC 2014 StreamCorpus and is
intended to be a high recall retrieval of documents related to all 15 of 
the 2014 TS events. 

Track participants were provided with a set of query words for each event.
For example, the event ``Costa Concordia disaster and recovery'' had query
words [``costa'', ``concordia''].
In order to construct an event specific corpus,
we retrieve all documents whose timestamps fall within the event start/stop 
times and whose raw html content contains at least one keyword from the 
event's query words. We further restricted our document set to only those 
articles from the news domain.


\subsubsection{Content Detection}\label{subsec:Content Detection}

In early versions of our summarization system, we found that structural html
artifacts and sentence tokenization errors
were negatively effecting the performance of later stages. Examples of the 
former include strings of link text like ``World Politics Sports ...'', while
examples of the latter included concatenations of various article headlines,
i.e. headlines pertaining to the event in question, as well as headlines from
other non-related events.
Both types of ``sentences'' were problematic for computing sentence similarity
as they were more likely to have higher average similarity to all input 
sentences. In turn they would be more likely to appear as cluster exemplars
in our clustering stage. From the clustering algorithm's point of view, this
is the correct decision to make---such multi-topic sentences are more general 
than a single topic, and better able to represent all aspects contained in a 
cluster. They make poor choices as updates, however, as they contain 
irrelevant information.

In order to filter out these problematic inputs, we trained a classifier to 
identify which sentences came from inside a document's main article and which
came from various headers, titles, menus, and links to other content. We collected ? random sentences and manually labeled whether the sentence came from
inside or outside the document's main article. We then trained a logistic 
regression classifier using the following features :

\begin{itemize}
 \item the position of the sentence
 \item word counts
 \item the last token in the sentence
 \item the last two tokens in the sentence
 \item the last three tokens in the sentence.
\end{itemize} 

These features were sufficient to capture 
the main difference between content and non-content sentences,
which 
was that content sentences generally ended with sentence final punctuation, 
i.e. periods or a closing quotation mark. 

Within our larger summarization framework, we process a document at a time,
identifying the subset of its sentences that are content sentences.
We then check to make sure \emph{all} event
query terms can be found within the document's content sentences. If so,
we send the content sentences on to the next stage of our pipeline; otherwise
we ignore all sentences in this document.

\subsubsection{Predicting Sentence Salience}\label{subsec:Predict}

In order to use AP clustering for summarization, we need to assign a
preference value to each input sentence.  In our approach, we equate a
sentence's salience with its preference.  A good model of sentence salience
should predict higher values for sentences that are more likely to appear in a
human generated summary of the event. 

We do not have such human judgments, but we do have last year's gold nugget
sentences.
The response variable we try to predict is a sentence's semantic similarity
(see ?) to the gold nugget sentences, i.e. we want to predict the similarity
to a gold nugget when the gold nugget is not known.


To build training data for this regression task, 
we use the TREC 2013 event/gold nuggets. For each event, we retrieve all 
sentences using the document retrieval and content selection steps outlined
in the previous selection. We then sample with replacement 200 sentences
from this collection, and extract non-lexical features (described in more 
detail below) for each sentence
to construct our design matrix $\mathbf{X}$. To build our response variables
$\mathbf{y}$ we compute the maximum semantic similarity of each input
sentence to the gold nugget sentences. We fit a $\operatorname{GP}$ with
an RBF kernel to this data, optimizing kernel parameters with the scaled 
conjugate gradient method. This sampling procedure is repeated 100 times
for each of the ? 2013 TS events, yielding ? total models. In our salience 
predictions for the 2014 events, we take the mean prediction of all models.

 
%we take a subset of sentences
%relevant to the TREC events (approximately 1000) and match them to the gold
%nugget sentence with highest similarity as determined by the sentence
%similarity system of \cite{guo2012simple}. 
%\cite{} have used this system previously to correlate sentences to meaningful
%units of information in human generated summaries. 
%We use the real-valued similarity scores as our salience scores for the
%training sentences.

We want our model to be predictive across different kinds of events so we
avoid lexical features.  Instead, we extract a variety of features including
language model scores, geographic relevance, and temporal relevance from each
sentence.  These features are used to fit a Gaussian process regression model
that can predict the similarity of a sentence to a gold summary
\cite{preotiuc2013temporal}.  
%We use the model predicted salience of each
%sentence as it's preference value in the AP clustering. 

\subsubsection{Basic Features}

We employ several basic features that have been used previously in supervised
models to rank sentence salience \cite{kupiec1995trainable,conroy2001using}.
These include sentence length, the number of capitalized words normalized by
sentence length, document position, number of named entities, etc.

\subsubsection{Query Features}

These features are derived from the event query and the event type.
These include the number of query words present in the sentence
in addition to the number of event type synonyms, hypernyms, and hyponyms
as found in WordNet \cite{miller1995wordnet}.
E.g., for event type \emph{earthquake}, e.g., we obtain ``quake,'' ``temblor,''
``seism,'' ``aftershock,'' etc.   


\subsubsection{Language Model Features}\label{subsubsec:lm}

%Because the data in our experiments is scraped from the web, it is common to
%find sentences that contain both salient informantion and two kinds of noise:
%noisey fragments of web page structure (e.g. section titles, \emph{News},
%\emph{Sports}, etc.) and references to other news not relevant to the topic
%summary.
%
We use two trigram language models, trained using the SRILM toolkit
\cite{stolcke2002srilm}, taking as features the average log probability (i.e.
the sentence's total log probability normalized by sentence length) from each
model.  This first model is trained on 4 years (2005-2009) of articles from
the Gigaword corpus.  Specifically, we use articles from the Associated Press
and the New York Times. This model is intended to assess the general writing
quality (grammaticality, word usage) of an input sentence and helps us to
filter out text snippets which are not sentences (e.g., web page titles).  The
second model is a domain specific language model. We build a corpus of
Wikipedia articles for each event type, consisting of documents from a related
Wikipedia category. E.g. for earthquakes, we collect pages under the category
\emph{Category:Earthquakes}. This model assigns higher probability to
sentences that are focused on the given domain.

%For both models, we Finally, we extract the percentage of capitalized words,
%and sentence length as features. These last two features also help to
%identify sentences that are less likely to contain relevant content-- overly
%long and heavily capitalized sentences in our corpus were likely to be long
%strings of web-page headlines, section headers, and other irrelevant page
%structure. 

\subsubsection{Geographic Relevance Features}

Locations are identified using a named entity tagger. For each location in a
sentence, we obtain its latitude and longitude using the Google Maps API.  We
then compute its distance to that of the event location.  It is possible for a
sentence and an event to have multiple locations so we take as features the
minimum, maximum, and average distance of all sentence-event location pairs.
Distances are calculated using the Vincenty distance.

\subsubsection{Temporal Relevance Features}

Our data consists of hourly crawls of online content and so we exploit the
temporality of corpus by capturing the burstiness of a sentence, i.e.  the
change in word frequency from one hour to the next.``Bursty'' sentences often
indicate new and important data. 

Let $D_t$ be the set of web pages at time $t$ and let $s = \{w_1,\ldots,
w_n\}$ be a sentence from a page $d \in D_t$.  We calculate the 1-hour
burstiness of sentence $s$ from document $d$ at hour $t$  as \begin{align*}
\operatorname{b}_1(s,d,t) = \frac{1}{|s|} \sum_{w \in s} \Bigg( &
\operatorname{tf-idf}_t(w,d)  \\ & \left. - \frac{\sum_{d^\prime \in D_{t-1}:
w \in d^\prime } \operatorname{tf-idf}_{t-1}(w,d^\prime)}{|\{d^\prime \in
D_{t-1}: w \in d^\prime\}|} \right) \end{align*}

where \begin{align*} \operatorname{tf-idf}_t(w,d) =&
\log\left(1+\sum_{w^\prime \in d}1\{w=w^\prime\}  \right)\\ & \times
\log\left(\frac{|D_t|}{1 + \sum_{d^\prime \in D_t}1\{w \in d^\prime\}}\right).
\end{align*}
% 1\{w = w^\prime} %- \operatorname{avg-tf-idf}_{t_{i-1}}(w).
%\end{align*}


We similarly find the sentence's 5-hour burstiness.  In addition to
burstiness, we also include the sentence's average tf-idf and hours since the
event in question started as features.


\subsubsection{Sentence Selection}\label{subsec:SentenceSelection}

In the sentence selection stage, we use the salience predictions from our GP
model as preferences in the AP clustering algorithm. The AP algorithm is 
parameterized by a similarity matrix $\mathbf{S}$ and a vector of 
preferences $\boldsymbol{\pi}$; we found AP to be very sensitive to these
parameters, and did not perform robustly on our range of inputs.
In order to improve the quality of the clusters and exemplar selection,
we re-scaled both the raw inputs $\mathbf{S}$ and $\boldsymbol{\pi}$. 
The raw preferences are scaled to lie within the  range $(-3, -2)$

The initial matrix $\mathbf{S}$ is computed by finding the pairwise semantic
similarity between input sentences. Self-similarities and similarities below 
a threshold $\lambda$ were masked and the remaining values scaled to the range
$(-3, -1)$.

\subsubsection{Preferences}

Before rescaling, we first penalize each $\boldsymbol{\pi}_i$ based on
the aggregate similarity of input sentence $\mathbf{s}_i$ to the set of previous updates
$\mathbf{U}$. We call this the redundancy penalty 

$$\rho_i = \sum_j \frac{\boldsymbol{\pi}^{(U)}_{j}\operatorname{cos-sim}(\mathbf{s}_i, \mathbf{U}_j)}
{\sum_{j^\prime}\operatorname{cos-sim}(\mathbf{s}_i, \mathbf{U}_{j^\prime})} $$   
where $\boldsymbol{\pi}^{(U)}_{j}$ is the salience prediction of the $j$ 
previous update. We calculate our new penalized preferences 
$\boldsymbol{\pi}^{(\rho)}$ where $\boldsymbol{\pi}^{(\rho)}_i = \boldsymbol{\pi}_i - \rho_i$.
Finally, we rescale $\boldsymbol{\pi}^{(\rho)}$ such that all values lie 
within the range $(-3,-2)$.

\subsubsection{Filtering}

We run AP clustering with our rescaled and penalized sentence similarities
and preferences. When the clustering has converged we emit all exemplar
sentences (cluster centers) whose preference is $\lambda$ standard deviations
above the mean preference for the current time period and who do not belong
to singleton clusters as updates. These updates and their preferences are 
retained for penalty calculations in subsequent time steps.


