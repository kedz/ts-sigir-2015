\section{Related Work}
\label{sec:relatedwork}
A principal concern in extractive multi-document summarization is the
selection of salient sentences for inclusion in summary output
\cite{nenkova2012survey}.  This has often been approached as a ranking
problem.
%We broadly conceptualize this decision as either an intrinsic or extrinsic
%sentence evaluation process. Intrinsic approaches evaluate sentences
%individually, possibly by predicting the impact on summary quality using
%sentece level features. 
Sentences have been ranked by the average word probability, average tf-idf
score, and the number of topically related words (topic-signatures in the
summarization literature)
\cite{nenkova2005impact,hovy1998automated,lin2000automated}. The first two
statistics are easily computable from the input sentences, while the third
only requires an additional, generic background corpus.  Another ranking
approach, centroid summarization, involves creating an average bag of words
(BOW) vector, the centroid, from the input sentences and ranking sentences by
their similarity to the centroid \cite{radev2004centroid}.  Graph
\cite{erkan2004lexrank} and clustering
\cite{hatzivassiloglou2001simfinder,mckeown1999towards,siddharthan2004syntactic}
based approaches, on the other hand, make use of pair-wise similarity
comparisons amongst input sentences.  In these models, salient sentences are
more central to the input or cluster, respectively.

%identify salient regions of the input space while simultaneously coping with
%redundancy.  Graph-based algorithms have been used to rank sentences
%Clustering algorithms, e.g., are commonly used to exploit redundancy in
%input. Input sentences are clustered and summaries are generated by selecting
%the most representative sentence from each cluster.  Graph-based models have
%also been used for summarization.  E.g., the LexRank algorithm treats
%sentences as nodes in a graph, where edges are constructed by way of cosine
%similarity between sentence nodes; edges are either continuosly weighted by
%similarity or discrete, existing only when the similarity is above a
%threshold.  The PageRank algorithm is used on the graph to find the most
%important sentence nodes. In both clustering and graph-based approaches,
%sentence salience is largely determined by the pairwise relations between
%sentences.

Supervised learning has also been applied to this task. Model features are
usually derived from human generated summaries, and are non-lexical in nature
(e.g., sentence starting position, number of topic-signatures, number of
unique words, word frequencies). Seminal work in this area has employed naive
Bayes and logistic regression classifiers to identify sentences for summary
inclusion \cite{kupiec1995trainable,conroy2001using}. 

%\fdadd{
Several researchers have recognized the importance of summarization during
natural disasters.  Guo \textit{et al.} developed a system for detecting
novel, relevant, and comprehensive sentences immediately after a natural
disaster \cite{qi:temporal-summarization}.  The method uses a model of
sentence relevance and novelty in order to select appropriate updates.
Training data for regression targets is automatically generated from
retrospective Wikipedia data.  The system is evaluated on news documents
related to 197 natural and human disasters from 2009 to 2011 using variants of
Rouge modified to capture novelty, relevance, and comprehensiveness
\cite{lin2004rouge}.  Wang and Li present a clustering-based approach to
efficiency detect important updates during natural disasters
\cite{wang:update-summarization}.  The algorithm works by hierarchically
clustering sentences online, allowing the system to output a more expressive
narrative structure than Guo \textit{et al.}.  The method is evaluated on
official press releases related to Hurricane Wilma  in 2005 using Rouge score
between the system summary and a manually generated target summary.

\fdcomment{Glasgow temporal summarization system \cite{mccreadie:temporal-summarization}.}