\label{sec:relatedwork}

A principal concern in extractive multi-document summarization is the
selection of salient sentences for inclusion in summary output
\cite{nenkova2012survey}. 
Existing approaches generally fall into one of three categories, each with 
specific trade-offs with respect to update summarization. 


First, centrality-focused approaches (including graph \cite{erkan2004lexrank},
cluster \cite{hatzivassiloglou2001simfinder}, and centroid 
\cite{radev2004centroid} methods) are very natural for retrospective analysis 
in the sense that they let the data ``speak for itself.'' These methods 
equate salience with centrality, either to the input or some other aggregate 
object (i.e. a cluster center or input centroid). However, they rely
chiefly on redundancy. When applied to an unfolding event, there may not exist
enough redundant content at the event onset for these methods to exploit.
Once the event onset has passed, however, the redundancy reduction of these 
methods is quite beneficial.


The second category, predictive approaches, includes ranking and 
classification based methods. Sentences have been ranked by the average word 
probability, average TF*IDF score, and the number of topically related words 
(topic-signatures in the summarization literature)
\cite{nenkova2005impact,hovy1998automated,lin2000automated}. The first two
statistics are easily computable from the input sentences, while the third
only requires an additional, generic background corpus. In classification 
based methods, model features are usually derived from human generated 
summaries, and are non-lexical in nature (e.g., sentence starting position, 
number of topic-signatures, number of unique words). Seminal work in this 
area has employed na\"ive Bayes and logistic regression classifiers to 
identify sentences for summary inclusion 
\cite{kupiec1995trainable,conroy2001using}. 
While these methods are less dependent on redundancy, the expressiveness of
their features is limited. Our model expands on these basic features to 
account for geographic, temporal, and language model features.


The last category includes probabilistic \cite{haghighi2009exploring}, 
information theoretic, and set cover \cite{lin2011class}
approaches. While these methods are  focused on producing diverse
summaries, they are difficult to adapt to the streaming setting, where 
we do not necessarily have a fixed summary length and the corpus to be
summarized contains many irrelevant sentences, i.e. there are large
portions of the corpora that we specifically want to avoid. 


Several researchers have recognized the importance of summarization during
natural disasters.  \cite{qi:temporal-summarization} developed a system for 
detecting novel, relevant, and comprehensive sentences immediately after a 
natural disaster.  \cite{wang:update-summarization} present a clustering-based
approach to efficiently detect important updates during natural disasters.  
The algorithm works by hierarchically clustering sentences online, allowing 
the system to output a more expressive narrative structure than 
\cite{qi:temporal-summarization}.  Our system attempts to unify these system's
approaches (predictive ranking and clustering respectively). 
