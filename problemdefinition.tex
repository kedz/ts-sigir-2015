\label{sec:problemdefinition}
In order to evaluate an update summarization system, we adopt the simulation-based approach used in the TREC Temporal Summarization Track.  We provide a brief overview of the problem.  Details on the formulation can be found in the track overview paper \cite{aslam2013trec}.  

An update summarization system takes as input 
\begin{enumerate*}[label=\itshape\alph*\upshape)]
  \item a short query $\query$ defining the event to be tracked (e.g. `Hurricane Sandy'), 
  \item an event category $\eventcategory$ defining the type of event to be tracked (e.g. `hurricane'), 
  \item a stream of time-stamped documents, $(\doc_0, \doc_1,\ldots,\doc_T)$,
  presented in temporal order, and \item an evaluation time period of interest,
      $(\stime,\etime)$.  \end{enumerate*} While processing documents
      throughout the time period of interest, the system must output sentences,
      known as updates, which are \emph{relevant} to the query,
      \emph{comprehensive} with respect to the event, \emph{novel} to the user,
      and \emph{timely} with respect to when the update occurred.  Precisely
      how to measure these properties will be discussed in Section
      \ref{sec:methods}.  In our version of this problem, we assume that that
      the system receives one batch of new sentence-segmented documents every
      hour throughout the period of interest.

We address disaster types such as terrorism and mass shootings (e.g., the 2012
shooting in Aurora, Col.), natural disasters (e.g., Hurricane Sandy), accidents
(e.g., the 2012 Pakistan garment factory fire), and social activism (e.g., the
Arab spring). The size and scope of the events varies considerably. Hurricane
Sandy, for example, effected multiple countries over weeks while the
2012 Aurora shooting was contained to a single location spanning several hours.

%\kmcomment{This was as best as I could do on this. These are the example
%    categories from the wikipedia page. However you refer in the language model
%    description to the event type earthquake which is more specific. Also, in
%    the TREC data you refer to event types that are specified in the metadata.
%So I'm wondering how many different categories you have and where they come
%from.}

%\fdcomment{include description of nuggets here.} 
%%\kmcomment{We should jointly
%    discuss what goes in data and what goes in problem definition. I have moved
%    event types here because I think knowing how many types and examples of
%    them would be helpful upfront. I think where nuggets and wikipedia pages go
%is questionable.}

Figure \ref{alg:temporal-summarization} outlines our general update
summarization algorithm.  At each hour, the system processes each input 
sentence batch $S_t$.
We predict the salience $P$ for all input sentences $s\in S_t$.
Next, we cluster $S_t$ using the AP clustering algorithm, biased by $P$,
obtaining a set of exemeplar sentences $E$. Finally we select a subset of $E$
to be updates and add those to our set of summary updates $U$.

\begin{algorithm}%[H]
 \KwData{ 
     $S_{\stime}, S_{\stime+1},\ldots,S_{\etime} $ --- time ordered sentence
     batches\\
 \KwResult{U --- a list of updates, i.e. the update summary} 
}
 ~\\
 Initialize empty list $U$ of updates\\
    \For{$t \gets \stime,\ldots,\etime $}{
        $P \gets \operatorname{PredictSalience}(S_t)$\\
        $E \gets \operatorname{APCluster}(P, S_t)$ \\
        $U_t \gets \operatorname{SelectUpdates}(E,P,U)$\\
        $U \gets U \cup U_t$
}
 \caption{Temporal Summarization Algorithm}\label{alg:temporal-summarization}
\end{algorithm}

\subsection{Salience Prediction}
%
\begin{figure}[t!]
\begin{tabular}{| l |} 
\hline
\textbf{Basic Features}\\
$\cdot$ Sent. position (normalized by doc. length) \\
$\cdot$ Sent. length \\
$\cdot$ Ratio of punc. to non-punc. chars. \\
$\cdot$ Ratio of caps. to non-caps. chars. \\
$\cdot$ Ratio of lowercase to other chars. \\
$\cdot$ Ratio of uppercase to other chars. \\
$\cdot$ \# of caps. words (normalized by \# of words)\\
$\cdot$ \# of Person, Location, Org. Date, Number,\\
$\;\;$ Ordinal, Percent, Money, Set, Misc  N.E. tags \\
$\;\;$ (normalized by \# of words)\\
\hline
\textbf{Query Features}\\
$\cdot$ \% of query words covered by sent.\\
$\cdot$ Total query matches.\\
$\cdot$ Total event-type synonyms/hypernyms/hyponyms\\
$\;\;$ coverage.\\
$\cdot$ Total event-type synonyms/hypernyms/hyponyms\\
$\;\;$ matches.\\
\hline
\textbf{Language Model Features}\\
$\cdot$ Avg. token log probability (domain lang. model)\\
$\cdot$ Avg. token log probability (background lang. model)\\
\hline
\textbf{Geo-tag Features}\\
$\cdot$ Median document distance to nearest location \\
$\;\;$ cluster (current hour).\\
$\cdot$ Median document distance to nearest location \\
$\;\;$ clusters (previous hour).\\
$\cdot$ Distance of first location in doc. to nearest location \\
$\;\;$ cluster (current hour).\\
$\cdot$ Distance of first location in doc. to nearest location \\
$\;\;$ cluster (previous hour).\\
\hline
\textbf{Temporal Features}\\
$\cdot$ Avg. tf-idf at current time.\\
$\cdot$ Change in avg. tf-idf since previous hour (up to\\
$\;\;$  24 hours).\\
$\cdot$ Time since query/event start.\\
\hline
\end{tabular}
\caption{Salience model features.}
\end{figure}

\subsubsection{Features}
We want our model to be predictive across different kinds of events so we avoid lexical features.  Instead, we extract a variety of features including language model scores, geographic relevance, and temporal relevance from each sentence.  These features are used to fit a Gaussian process regression model that can predict the similarity of a sentence to a gold summary \cite{preotiuc2013temporal}.  
\fdcomment{can we enumerate all of these?}

\paragraph{Basic Features}
%KM - Would be good to have quick justification of these features. I added a
%sentence. Feel free to edit or remove.

We employ several basic features that have been used previously in supervised models to rank sentence salience \cite{kupiec1995trainable,conroy2001using}. These include sentence length, the number of capitalized words normalized by sentence length, document position, number of named entities.  
Since training is one on grammatical English, some of these features help
to downweight sentences that are ungrammatical (e.g., have too many capitalized words or are too short).
Others help to more heavily weight important sentences (e.g., that appear in
prominent positions such as paragraph initial or article initial).

\paragraph{Query Features}

Query features measure the relationship between the sentence and the event query and type.  These include the number of query words present in the sentence in addition to the number of event type synonyms, hypernyms, and hyponyms as found in WordNet \cite{miller1995wordnet}.  For example, for event type \emph{earthquake},  we match sentence terms ``quake'', ``temblor'', ``seism'', and ``aftershock''.
\paragraph{Language Model Features}\label{subsubsec:lm}
Language models allow us to measure the likelihood of a sentence having been produced from a particular source.  We consider two types of language model features.  The first model is estimated from a corpus of generic news articles.  This model is intended to assess the general writing quality (grammaticality, word usage) of an input sentence and helps us to filter out text snippets which are not sentences (e.g., web page titles).  The second model is estimated from text specific to our event types.  For example, the language model for event type `earthquake' is estimated from Wikipedia pages under the category \emph{Category:Earthquakes}.  These models are intended to detect sentences similar to those appearing in summaries of other events in the same category (e.g. most earthquake summaries are likely to include higher probability for ngrams including the token `magnitude').  
%KM - Note: Someplace the exact list of event types should appear. Probably not
%here.
%KM - I note you have it in a later section but it is labeled as data you use
%to train language models and semantic similarity. I think it would be good to
%have up front in definition of task.


%For both models, we Finally, we extract the percentage of capitalized words,
%and sentence length as features. These last two features also help to
%identify sentences that are less likely to contain relevant content-- overly
%long and heavily capitalized sentences in our corpus were likely to be long
%strings of web-page headlines, section headers, and other irrelevant page
%structure. 

\paragraph{Geographic Relevance Features}

Locations are identified using a named entity tagger. For each location in a sentence, we obtain its latitude and longitude using the a publicly available geolocation service.  We then compute its distance to that of the event location.  It is possible for a sentence and an event to have multiple locations so we take as features the minimum, maximum, and average distance of all sentence-event location pairs.  Distances are calculated using the Vincenty distance. 
%KM - Probably should say how you determine event location. Some events move.
%KM - In your figure you only include one distance.

\paragraph{Temporal Relevance Features}

Our data consists of hourly crawls of online content and so we exploit the temporality of corpus by capturing the burstiness of a sentence, i.e.  the change in word frequency from one hour to the next.``Bursty'' sentences often indicate new and important data. 

Let $D_t$ be the set of web pages at time $t$ and let $s = \{w_1,\ldots,w_n\}$ be a sentence from a page $d \in D_t$.  We calculate the 1-hour burstiness of sentence $s$ from document $d$ at hour $t$  as 
\begin{align*}
\operatorname{b}_1(s,d,t) = \frac{1}{|s|} \sum_{w \in s} \Bigg( &
\operatorname{tf-idf}_t(w,d)  \\ & \left. - \frac{\sum_{d^\prime \in D_{t-1}:
w \in d^\prime } \operatorname{tf-idf}_{t-1}(w,d^\prime)}{|\{d^\prime \in
D_{t-1}: w \in d^\prime\}|} \right) \end{align*}

where \begin{align*} \operatorname{tf-idf}_t(w,d) =&
\log\left(1+\sum_{w^\prime \in d}1\{w=w^\prime\}  \right)\\ & \times
\log\left(\frac{|D_t|}{1 + \sum_{d^\prime \in D_t}1\{w \in d^\prime\}}\right).
\end{align*}
% 1\{w = w^\prime} %- \operatorname{avg-tf-idf}_{t_{i-1}}(w).
%\end{align*}


We similarly find the sentence's 5-hour burstiness.  In addition to burstiness, we also include the sentence's average tf-idf and hours since the event in question started as features.

\subsubsection{Model}
%KM - I think one of the words ``sentence'' below should be something else, btu
%not sure what you meant. Ah.. I think ``salience''?
%We adopt Gaussian process regression in order to predict sentence sentence
We adopt Gaussian process regression in order to predict sentence salience
\cite{rasmussen:gaussian-process-book}.  Gaussian process regressors are a
class of data-driven, non-parametric model generalizing the multi-variate
Gaussian to the infinite dimensional setting.  Gaussian processes are general
and are state of the art for many regression tasks.  A full treatment of
Gaussian process regression is beyond the scope of this article and can be
found in standard textbooks.


Being non-parametric, the Gaussian process relies on a  covariance matrix $\kernelMatrix$, measuring the affinity between pairs of instances, in our case candidate sentences.  In our experiments, we used a radial basis function (RBF) kernel.  Given two featurized sentences, $\features$ and $\features^\prime$, the RBF kernel is defined as,
\begin{align*}
        \kernelMatrixij{\features}{\features^\prime}&= \sigma^2 \exp\left(- \frac{1}{2} 
\sum_{i=1}^{\numfeatures} \frac{ (\featuresi{i}-\featuresi{i}^\prime)^2}{\ell_i^2} \right)
\end{align*}
where $\sigma$ and the $\ell_i$ are parameters we fit to our observed training data. The $\ell_i$ are feature dependent scaling parameters; once learned, they not only improve the accuracy of the model, but give us some introspection  into which features are more important.


% Formally, let $p(f)$ be a distribution over functions where $f$ is any mapping
% of an input space $\mathcal{X}$ to the reals,
%
% $$f: \mathcal{X} \rightarrow \mathcal{R}.$$
% Let the random variable $\mathbf{f} = (f(x_1),\ldots,f(x_n) )$ be
%  an $n$-dimensional vector whose elements are evaluations of the function $f$
% at points $x_i \in \mathcal{X}$.
% We say $p(f)$ is a Gaussian process if for any finite subset
% $\{x_1,\ldots,x_n\} \subset \mathcal{X}$, the marginal distribution over
% that finite subset $p(\mathbf{f})$ has a multivariate Gaussian distribution.
% A GP is parameterized by a mean function $\mu(\mathbf{x})$ and a
% covariance function $K(x,x^\prime)$. Generally, the mean function is simply
% set to 0, leaving the distribution to be completely characterized by the
% kernel function on the data.
%
% In the regression setting, we typically have a response variable $y$ that
% is the sum of our model prediction  and
% some Gaussian noise, i.e. $y = f(x) + \epsilon$ with
% $\epsilon \sim \mathcal{N}(0, \sigma^2)$. When
% $f \sim \operatorname{GP}(\mathbf{0}, \mathbf{K})$, the
% two distributions
% of principal interest are the marginal likelihood
% $p(\mathbf{y}|\mathbf{X}) =
% \mathcal{N}(\mathbf{0},\mathbf{K} + \sigma^2\mathbf{I})$ and the predictive
% distribution,
%
% $$p(\mathbf{y_*}|\mathbf{x_*},\mathbf{X},\mathbf{y}) =
% \mathcal{N}(\boldsymbol{\mu}_*, \boldsymbol{\sigma}^2_*) $$
%
% where $\mathbf{x_*}$ is a new or unseen input, $\mathbf{y_*}$ our predicted
% response, and
% \begin{align*}
% \boldsymbol{\mu}_* & = \mathbf{K_*}(\mathbf{K} + \sigma^2\mathbf{I})^{-1}\mathbf{y} \\
% \boldsymbol{\sigma}^2_* &
% = \mathbf{K}_{**} - \mathbf{K}_*(\mathbf{K} + \sigma^2\mathbf{I})^{-1}
% \mathbf{K}_*^T + \sigma^2\\
% \end{align*}.
%
% Here $\mathbf{K}_* = K(\mathbf{x}_*, \mathbf{X})$, and
% $\mathbf{K}_{**} = K(\mathbf{x}_*, \mathbf{x}_*)$.
%
%
