\subsection{Filtering Content}
\label{sec:filtering}
We simulate our system using realistic data insofar as the document stream consists of relevant and non-relevant content.  As with many information retrieval systems, we crude, first phase of high-recall document filtering.  That is, we would like to remove most of the content which we can easily detect as unlikely to contain useful updates.  

\subsubsection{Filtering Documents}
\label{sec:filtering:documents}
As a first filter, we retained only those documents whose raw html content contains at least one keyword from the event's query words. We further restricted our document set to only those articles from the news domain.\footnote{This filtered set was distributed by track organizers.}  

\subsubsection{Filtering Sentences}
\label{sec:filtering:sentences}
In early versions of our summarization system, we found that structural html
artifacts and sentence tokenization errors
were negatively effecting the performance of later stages. Examples of the 
former include strings of link text like ``World Politics Sports ...'', while
examples of the latter included concatenations of various article headlines,
i.e. headlines pertaining to the event in question, as well as headlines from
other non-related events.
Both types of ``sentences'' were problematic for computing sentence similarity
as they were more likely to have higher average similarity to all input 
sentences. In turn they would be more likely to appear as cluster exemplars
in our clustering stage. From the clustering algorithm's point of view, this
is the correct decision to make---such multi-topic sentences are more general 
than a single topic, and better able to represent all aspects contained in a 
cluster. They make poor choices as updates, however, as they contain 
irrelevant information.

In order to filter out these problematic inputs, we trained a classifier to 
identify which sentences came from inside a document's main article and which
came from various headers, titles, menus, and links to other content. We collected ? random sentences and manually labeled whether the sentence came from
inside or outside the document's main article. We then trained a logistic 
regression classifier using the following features :

\begin{itemize}
 \item the position of the sentence
 \item word counts
 \item the last token in the sentence
 \item the last two tokens in the sentence
 \item the last three tokens in the sentence.
\end{itemize} 

These features were sufficient to capture 
the main difference between content and non-content sentences,
which 
was that content sentences generally ended with sentence final punctuation, 
i.e. periods or a closing quotation mark. 

Within our larger summarization framework, we process a document at a time,
identifying the subset of its sentences that are content sentences.
We then check to make sure \emph{all} event
query terms can be found within the document's content sentences. If so,
we send the content sentences on to the next stage of our pipeline; otherwise
we ignore all sentences in this document.

