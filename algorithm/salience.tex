\subsection{Predicting Sentence Salience}
\label{subsec:Predict}
Once the input has been filtered down to a higher quality set of candidate updates, we can rank sentences according to their salience.  We adopt a model-based approach to predicting salience: a good model should predict higher values for sentences that are more likely to appear in a human generated summary of the event.   The response variable we try to predict is a sentence's semantic similarity \cite{guo:wtmf} to the gold nugget sentences, i.e. we want to predict the similarity to a gold nugget when the gold nugget is not known.  If sentence is very similar to a nugget, then it is a better candidate for emission as an update compared to a sentence that is not similar to any nuggets.


\subsubsection{Features}
We want our model to be predictive across different kinds of events so we avoid lexical features.  Instead, we extract a variety of features including language model scores, geographic relevance, and temporal relevance from each sentence.  These features are used to fit a Gaussian process regression model that can predict the similarity of a sentence to a gold summary \cite{preotiuc2013temporal}.  
\fdcomment{can we enumerate all of these?}

\paragraph{Basic Features}

We employ several basic features that have been used previously in supervised models to rank sentence salience \cite{kupiec1995trainable,conroy2001using}. These include sentence length, the number of capitalized words normalized by sentence length, document position, number of named entities.  

\paragraph{Query Features}

Query features measure the relationship between the sentence and the event query and type.  These include the number of query words present in the sentence in addition to the number of event type synonyms, hypernyms, and hyponyms as found in WordNet \cite{miller1995wordnet}.  For example, for event type \emph{earthquake},  we match sentence terms ``quake'', ``temblor'', ``seism'', and ``aftershock''.


\paragraph{Language Model Features}\label{subsubsec:lm}
Language models allow us to measure the likelihood of a sentence having been produced from a particular source.  We consider two types of language model features.  The first model is estimated from a corpus of generic news articles.  This model is intended to assess the general writing quality (grammaticality, word usage) of an input sentence and helps us to filter out text snippets which are not sentences (e.g., web page titles).  The second model is estimated from text specific to our event types.  For example, the language model for event type `earthquake' is estimated from Wikipedia pages under the category \emph{Category:Earthquakes}.  These models are intended to detect sentences similar to those appearing in summaries of other events in the same category (e.g. most earthquake summaries are likely to include higher probability for ngrams including the token `magnitude').  


%For both models, we Finally, we extract the percentage of capitalized words,
%and sentence length as features. These last two features also help to
%identify sentences that are less likely to contain relevant content-- overly
%long and heavily capitalized sentences in our corpus were likely to be long
%strings of web-page headlines, section headers, and other irrelevant page
%structure. 

\paragraph{Geographic Relevance Features}

Locations are identified using a named entity tagger. For each location in a sentence, we obtain its latitude and longitude using the a publicly available geolocation service.  We then compute its distance to that of the event location.  It is possible for a sentence and an event to have multiple locations so we take as features the minimum, maximum, and average distance of all sentence-event location pairs.  Distances are calculated using the Vincenty distance. 

\paragraph{Temporal Relevance Features}

Our data consists of hourly crawls of online content and so we exploit the temporality of corpus by capturing the burstiness of a sentence, i.e.  the change in word frequency from one hour to the next.``Bursty'' sentences often indicate new and important data. 

Let $D_t$ be the set of web pages at time $t$ and let $s = \{w_1,\ldots,w_n\}$ be a sentence from a page $d \in D_t$.  We calculate the 1-hour burstiness of sentence $s$ from document $d$ at hour $t$  as 
\begin{align*}
\operatorname{b}_1(s,d,t) = \frac{1}{|s|} \sum_{w \in s} \Bigg( &
\operatorname{tf-idf}_t(w,d)  \\ & \left. - \frac{\sum_{d^\prime \in D_{t-1}:
w \in d^\prime } \operatorname{tf-idf}_{t-1}(w,d^\prime)}{|\{d^\prime \in
D_{t-1}: w \in d^\prime\}|} \right) \end{align*}

where \begin{align*} \operatorname{tf-idf}_t(w,d) =&
\log\left(1+\sum_{w^\prime \in d}1\{w=w^\prime\}  \right)\\ & \times
\log\left(\frac{|D_t|}{1 + \sum_{d^\prime \in D_t}1\{w \in d^\prime\}}\right).
\end{align*}
% 1\{w = w^\prime} %- \operatorname{avg-tf-idf}_{t_{i-1}}(w).
%\end{align*}


We similarly find the sentence's 5-hour burstiness.  In addition to burstiness, we also include the sentence's average tf-idf and hours since the event in question started as features.

\subsubsection{Gaussian Process Regression}

\fdcomment{transition to gp description}

A Gaussian process (GP) is a distribution over functions and is a 
generalization of the multi-variate Gaussian to the infinite dimensional
setting. That is, we use the observed data to define a distribution over 
possible functions that generated this data, without having to explicitly 
parameterize the function---in this sense GPs are considered 
a non-parametric model.

Formally, let $p(f)$ be a distribution over functions where $f$ is any mapping
of an input space $\mathcal{X}$ to the reals,

$$f: \mathcal{X} \rightarrow \mathcal{R}.$$ 
Let the random variable $\mathbf{f} = (f(x_1),\ldots,f(x_n) )$ be
 an $n$-dimensional vector whose elements are evaluations of the function $f$
at points $x_i \in \mathcal{X}$.
We say $p(f)$ is a Gaussian process if for any finite subset 
$\{x_1,\ldots,x_n\} \subset \mathcal{X}$, the marginal distribution over 
that finite subset $p(\mathbf{f})$ has a multivariate Gaussian distribution.
A GP is parameterized by a mean function $\mu(\mathbf{x})$ and a 
covariance function $K(x,x^\prime)$. Generally, the mean function is simply
set to 0, leaving the distribution to be completely characterized by the
kernel function on the data.

In the regression setting, we typically have a response variable $y$ that
is the sum of our model prediction  and 
some Gaussian noise, i.e. $y = f(x) + \epsilon$ with 
$\epsilon \sim \mathcal{N}(0, \sigma^2)$. When
$f \sim \operatorname{GP}(\mathbf{0}, \mathbf{K})$, the
two distributions
of principal interest are the marginal likelihood
$p(\mathbf{y}|\mathbf{X}) = 
\mathcal{N}(\mathbf{0},\mathbf{K} + \sigma^2\mathbf{I})$ and the predictive
distribution,

$$p(\mathbf{y_*}|\mathbf{x_*},\mathbf{X},\mathbf{y}) =
\mathcal{N}(\boldsymbol{\mu}_*, \boldsymbol{\sigma}^2_*) $$

where $\mathbf{x_*}$ is a new or unseen input, $\mathbf{y_*}$ our predicted
response, and
\begin{align*}
\boldsymbol{\mu}_* & = \mathbf{K_*}(\mathbf{K} + \sigma^2\mathbf{I})^{-1}\mathbf{y} \\
\boldsymbol{\sigma}^2_* & 
= \mathbf{K}_{**} - \mathbf{K}_*(\mathbf{K} + \sigma^2\mathbf{I})^{-1}
\mathbf{K}_*^T + \sigma^2\\
\end{align*}.

Here $\mathbf{K}_* = K(\mathbf{x}_*, \mathbf{X})$, and 
$\mathbf{K}_{**} = K(\mathbf{x}_*, \mathbf{x}_*)$.


GP's are incredibly general, and are state of the art for many regression 
tasks (?). The reliance on the covariance matrix 
$\mathbf{K}$ for parameterization opens up the wide world of kernel methods
for regression, and many varieties of similarity functions can be used.
In our experiments we used a radial basis function kernel 
$$K(\mathbf{x},\mathbf{x}^\prime) = \sigma^2 \exp\bigg(- \frac{1}{2} 
\sum_{i=1}^d \frac{ (x_i-x^\prime_i)^2}{\ell_i^2} \bigg)$$ where 
$\sigma$ and the $\ell_i$ are parameters we fit to our observed training data.
The $\ell_i$ are feature dependent scaling parameters; once learned, they not
only improve the accuracy of the model, but give us some introspection 
into which features are more important.
