\subsection{Predicting Sentence Salience}
\label{subsec:Predict}
Once the input has been filtered down to a higher quality set of candidate updates, we can rank sentences according to their salience.  We adopt a model-based approach to predicting salience: a good model should predict higher values for sentences that are more likely to appear in a human generated summary of the event.   The response variable we try to predict is a sentence's semantic similarity \cite{guo:wtmf} to the gold nugget sentences, i.e. we want to predict the similarity to a gold nugget when the gold nugget is not known.  If sentence is very similar to a nugget, then it is a better candidate for emission as an update compared to a sentence that is not similar to any nuggets.

\begin{figure}[t!]
\begin{tabular}{| l |} 
\hline
\textbf{Basic Features}\\
$\cdot$ Sent. position (normalized by doc. length) \\
$\cdot$ Sent. length \\
$\cdot$ Ratio of punc. to non-punc. chars. \\
$\cdot$ Ratio of caps. to non-caps. chars. \\
$\cdot$ Ratio of lowercase to other chars. \\
$\cdot$ Ratio of uppercase to other chars. \\
$\cdot$ \# of caps. words (normalized by \# of words)\\
$\cdot$ \# of Person, Location, Org. Date, Number,\\
$\;\;$ Ordinal, Percent, Money, Set, Misc  N.E. tags \\
$\;\;$ (normalized by \# of words)\\
\hline
\textbf{Query Features}\\
$\cdot$ \% of query words covered by sent.\\
$\cdot$ Total query matches.\\
$\cdot$ Total event-type synonyms/hypernyms/hyponyms\\
$\;\;$ coverage.\\
$\cdot$ Total event-type synonyms/hypernyms/hyponyms\\
$\;\;$ matches.\\
\hline
\textbf{Language Model Features}\\
$\cdot$ Avg. token log probability (domain lang. model)\\
$\cdot$ Avg. token log probability (background lang. model)\\
\hline
\textbf{Geo-tag Features}\\
$\cdot$ Median document distance to nearest location \\
$\;\;$ cluster (current hour).\\
$\cdot$ Median document distance to nearest location \\
$\;\;$ clusters (previous hour).\\
$\cdot$ Distance of first location in doc. to nearest location \\
$\;\;$ cluster (current hour).\\
$\cdot$ Distance of first location in doc. to nearest location \\
$\;\;$ cluster (previous hour).\\
\hline
\textbf{Temporal Features}\\
$\cdot$ Avg. tf-idf at current time.\\
$\cdot$ Change in avg. tf-idf since previous hour (up to\\
$\;\;$  24 hours).\\
$\cdot$ Time since query/event start.\\
\hline
\end{tabular}
\caption{Salience model features.}
\end{figure}

\subsubsection{Features}
We want our model to be predictive across different kinds of events so we avoid lexical features.  Instead, we extract a variety of features including language model scores, geographic relevance, and temporal relevance from each sentence.  These features are used to fit a Gaussian process regression model that can predict the similarity of a sentence to a gold summary \cite{preotiuc2013temporal}.  
\fdcomment{can we enumerate all of these?}

\paragraph{Basic Features}
%KM - Would be good to have quick justification of these features. I added a
%sentence. Feel free to edit or remove.

We employ several basic features that have been used previously in supervised models to rank sentence salience \cite{kupiec1995trainable,conroy2001using}. These include sentence length, the number of capitalized words normalized by sentence length, document position, number of named entities.  
Since training is one on grammatical English, some of these features help
to downweight sentences that are ungrammatical (e.g., have too many capitalized words or are too short).
Others help to more heavily weight important sentences (e.g., that appear in
prominent positions such as paragraph initial or article initial).

\paragraph{Query Features}

Query features measure the relationship between the sentence and the event query and type.  These include the number of query words present in the sentence in addition to the number of event type synonyms, hypernyms, and hyponyms as found in WordNet \cite{miller1995wordnet}.  For example, for event type \emph{earthquake},  we match sentence terms ``quake'', ``temblor'', ``seism'', and ``aftershock''.
\paragraph{Language Model Features}\label{subsubsec:lm}
Language models allow us to measure the likelihood of a sentence having been produced from a particular source.  We consider two types of language model features.  The first model is estimated from a corpus of generic news articles.  This model is intended to assess the general writing quality (grammaticality, word usage) of an input sentence and helps us to filter out text snippets which are not sentences (e.g., web page titles).  The second model is estimated from text specific to our event types.  For example, the language model for event type `earthquake' is estimated from Wikipedia pages under the category \emph{Category:Earthquakes}.  These models are intended to detect sentences similar to those appearing in summaries of other events in the same category (e.g. most earthquake summaries are likely to include higher probability for ngrams including the token `magnitude').  
%KM - Note: Someplace the exact list of event types should appear. Probably not
%here.
%KM - I note you have it in a later section but it is labeled as data you use
%to train language models and semantic similarity. I think it would be good to
%have up front in definition of task.


%For both models, we Finally, we extract the percentage of capitalized words,
%and sentence length as features. These last two features also help to
%identify sentences that are less likely to contain relevant content-- overly
%long and heavily capitalized sentences in our corpus were likely to be long
%strings of web-page headlines, section headers, and other irrelevant page
%structure. 

\paragraph{Geographic Relevance Features}

Locations are identified using a named entity tagger. For each location in a sentence, we obtain its latitude and longitude using the a publicly available geolocation service.  We then compute its distance to that of the event location.  It is possible for a sentence and an event to have multiple locations so we take as features the minimum, maximum, and average distance of all sentence-event location pairs.  Distances are calculated using the Vincenty distance. 
%KM - Probably should say how you determine event location. Some events move.
%KM - In your figure you only include one distance.

\paragraph{Temporal Relevance Features}

Our data consists of hourly crawls of online content and so we exploit the temporality of corpus by capturing the burstiness of a sentence, i.e.  the change in word frequency from one hour to the next.``Bursty'' sentences often indicate new and important data. 

Let $D_t$ be the set of web pages at time $t$ and let $s = \{w_1,\ldots,w_n\}$ be a sentence from a page $d \in D_t$.  We calculate the 1-hour burstiness of sentence $s$ from document $d$ at hour $t$  as 
\begin{align*}
\operatorname{b}_1(s,d,t) = \frac{1}{|s|} \sum_{w \in s} \Bigg( &
\operatorname{tf-idf}_t(w,d)  \\ & \left. - \frac{\sum_{d^\prime \in D_{t-1}:
w \in d^\prime } \operatorname{tf-idf}_{t-1}(w,d^\prime)}{|\{d^\prime \in
D_{t-1}: w \in d^\prime\}|} \right) \end{align*}

where \begin{align*} \operatorname{tf-idf}_t(w,d) =&
\log\left(1+\sum_{w^\prime \in d}1\{w=w^\prime\}  \right)\\ & \times
\log\left(\frac{|D_t|}{1 + \sum_{d^\prime \in D_t}1\{w \in d^\prime\}}\right).
\end{align*}
% 1\{w = w^\prime} %- \operatorname{avg-tf-idf}_{t_{i-1}}(w).
%\end{align*}


We similarly find the sentence's 5-hour burstiness.  In addition to burstiness, we also include the sentence's average tf-idf and hours since the event in question started as features.

\subsubsection{Model}
%KM - I think one of the words ``sentence'' below should be something else, btu
%not sure what you meant. Ah.. I think ``salience''?
%We adopt Gaussian process regression in order to predict sentence sentence
We adopt Gaussian process regression in order to predict sentence salience
\cite{rasmussen:gaussian-process-book}.  Gaussian process regressors are a
class of data-driven, non-parametric model generalizing the multi-variate
Gaussian to the infinite dimensional setting.  Gaussian processes are general
and are state of the art for many regression tasks.  A full treatment of
Gaussian process regression is beyond the scope of this article and can be
found in standard textbooks.


Being non-parametric, the Gaussian process relies on a  covariance matrix $\kernelMatrix$, measuring the affinity between pairs of instances, in our case candidate sentences.  In our experiments, we used a radial basis function (RBF) kernel.  Given two featurized sentences, $\features$ and $\features^\prime$, the RBF kernel is defined as,
\begin{align*}
        \kernelMatrixij{\features}{\features^\prime}&= \sigma^2 \exp\left(- \frac{1}{2} 
\sum_{i=1}^{\numfeatures} \frac{ (\featuresi{i}-\featuresi{i}^\prime)^2}{\ell_i^2} \right)
\end{align*}
where $\sigma$ and the $\ell_i$ are parameters we fit to our observed training data. The $\ell_i$ are feature dependent scaling parameters; once learned, they not only improve the accuracy of the model, but give us some introspection  into which features are more important.


% Formally, let $p(f)$ be a distribution over functions where $f$ is any mapping
% of an input space $\mathcal{X}$ to the reals,
%
% $$f: \mathcal{X} \rightarrow \mathcal{R}.$$
% Let the random variable $\mathbf{f} = (f(x_1),\ldots,f(x_n) )$ be
%  an $n$-dimensional vector whose elements are evaluations of the function $f$
% at points $x_i \in \mathcal{X}$.
% We say $p(f)$ is a Gaussian process if for any finite subset
% $\{x_1,\ldots,x_n\} \subset \mathcal{X}$, the marginal distribution over
% that finite subset $p(\mathbf{f})$ has a multivariate Gaussian distribution.
% A GP is parameterized by a mean function $\mu(\mathbf{x})$ and a
% covariance function $K(x,x^\prime)$. Generally, the mean function is simply
% set to 0, leaving the distribution to be completely characterized by the
% kernel function on the data.
%
% In the regression setting, we typically have a response variable $y$ that
% is the sum of our model prediction  and
% some Gaussian noise, i.e. $y = f(x) + \epsilon$ with
% $\epsilon \sim \mathcal{N}(0, \sigma^2)$. When
% $f \sim \operatorname{GP}(\mathbf{0}, \mathbf{K})$, the
% two distributions
% of principal interest are the marginal likelihood
% $p(\mathbf{y}|\mathbf{X}) =
% \mathcal{N}(\mathbf{0},\mathbf{K} + \sigma^2\mathbf{I})$ and the predictive
% distribution,
%
% $$p(\mathbf{y_*}|\mathbf{x_*},\mathbf{X},\mathbf{y}) =
% \mathcal{N}(\boldsymbol{\mu}_*, \boldsymbol{\sigma}^2_*) $$
%
% where $\mathbf{x_*}$ is a new or unseen input, $\mathbf{y_*}$ our predicted
% response, and
% \begin{align*}
% \boldsymbol{\mu}_* & = \mathbf{K_*}(\mathbf{K} + \sigma^2\mathbf{I})^{-1}\mathbf{y} \\
% \boldsymbol{\sigma}^2_* &
% = \mathbf{K}_{**} - \mathbf{K}_*(\mathbf{K} + \sigma^2\mathbf{I})^{-1}
% \mathbf{K}_*^T + \sigma^2\\
% \end{align*}.
%
% Here $\mathbf{K}_* = K(\mathbf{x}_*, \mathbf{X})$, and
% $\mathbf{K}_{**} = K(\mathbf{x}_*, \mathbf{x}_*)$.
%
%
