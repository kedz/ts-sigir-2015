\subsection{Selecting Sentences}
\label{sec:ap}
In the sentence selection stage, we use the salience predictions from our GP
model as preferences in the AP clustering algorithm. The AP algorithm is 
parameterized by a similarity matrix $\mathbf{S}$ and a vector of 
preferences $\boldsymbol{\pi}$; we found AP to be very sensitive to these
parameters, and did not perform robustly on our range of inputs.
In order to improve the quality of the clusters and exemplar selection,
we re-scaled both the raw inputs $\mathbf{S}$ and $\boldsymbol{\pi}$. 
The raw preferences are scaled to lie within the  range $(-3, -2)$

The initial matrix $\mathbf{S}$ is computed by finding the pairwise semantic
similarity between input sentences. Self-similarities and similarities below 
a threshold $\lambda$ were masked and the remaining values scaled to the range
$(-3, -1)$.

\subsubsection{Semantic Similarity}\label{subsec:semsim}

Whenever we make a pairwise comparison between sentences, we use the weighted
textual matrix factorization (WTMF) model of \cite{guo2012simple}. This 
model can be thought of as a variant of latent semantic analysis (?), 
where words that are not present in a sentence are explicitly modeled.
More formally, we have a term-sentence matrix 
$\mathbf{X}\in\mathcal{R}^{v \times n}$ representing $n$ sentences with a 
vocabulary of $v$ words; $\mathbf{X}_{i,j}$ indicates is non-zero if sentence
$j$ contains word $i$. In the WTMF regime, we want to find an approximation
of $\mathbf{X} \approx \mathbf{P}^T\mathbf{Q}$, where 
$\mathbf{P} \in \mathcal{R}^{k \times v}$ is a latent word vector space and
$\mathbf{Q} \in \mathcal{R}^{k \times n}$ is a latent sentence vector
space. These matrices are found by minimizing the objective function

$$\sum_i^v \sum_j^n \mathbf{W}_{i,j}(\mathbf{P}_{\cdot,i}^T
\mathbf{Q}_{\cdot,j} 
- \mathbf{X}_{i,j})^2 
 + \lambda ||\mathbf{P}||_2^2 + \lambda ||\mathbf{Q}||_2^2$$

where $\mathbf{W}_{i,j} = 
\begin{cases} 1, & \textrm{if $\mathbf{X}_{i,j} \ne 0$ } \\
w_m, & \textrm{if $\mathbf{X}_{i,j} = 0$ }\\
\end{cases}$
and $\lambda$ is a hyperparameter controlling the regularization terms.

The $w_m$ term is another model hyperparameter that is set to a small constant
($\le .01$). The weight matrix $\mathbf{W}$ has the effect of discounting the
reconstruction error of missing terms (words that did not occur a sentence).

Given an unseen sentence $\hat{i}$ we can project its term vector into the
latent sentence vector space with 

$$
\mathbf{Q}_{\cdot,\hat{i}} = (\mathbf{P}\mathbf{\tilde{W}}^{(\hat{i})}
\mathbf{P}^T  + \lambda\mathbf{I} )^{-1} 
\mathbf{P}\mathbf{\tilde{W}}^{(\hat{i})} \mathbf{X}_{\cdot, \hat{i}}
$$  

where $\mathbf{\tilde{W}}^{(\hat{i})}$ is an $v\times v$ diagonal matrix
where $\mathbf{\tilde{W}}^{(\hat{i})}_{j,j}$ is equal to $1$ or $w_m$ 
depending on whether or not the $j$-th term occurs in sentence $\hat{i}$.


The WTMF model is used extensively throughout our TS system. When making any 
pairwise comparison between sentence $i$ and $j$, we first construct
their latent sentence vectors $\mathbf{Q}_{\cdot,i}$ and
$\mathbf{Q}_{\cdot,j}$ and then find the cosine similarity 
$\displaystyle \operatorname{cos-sim}
(\mathbf{Q}_{\cdot,i}, \mathbf{Q}_{\cdot,j}) = 
\frac{\mathbf{Q}_{\cdot,i}^T\mathbf{Q}_{\cdot,j}}{||\mathbf{Q}_{\cdot,i}||_2
||\mathbf{Q}_{\cdot,j}||_2   }$.


Because the events for the TS task come from different domains, we construct
domain specific latent word vector spaces for each domain using in-domain 
Wikipedia pages (see~\cref{sec:data} for more details).


\subsubsection{Affinity Propagation}
Affinity propagation (AP) is a message passing algorithm that identifies both
exemplar data points and assignments of each point to an exemplar.  This is
done iteratively by passing \emph{responsibility} and \emph{availability}
messages between data points that quantify the fitness of one data point to
represent another, and the fitness of a data point to be represented based on
the choices of other data points respectively \cite{dueck2007non}.

AP is parameterized by an $n\times n$ similarity matrix $S$ and an $n\times 1$
preference vector $\pi$.  $S$ is a real-valued matrix where $S(i,j)$ is the
similarity of the $i$-th data point to the $j$-th data point.  $S$ does not
need to be symmetric.  $\pi$ is a real-valued vector where $\pi(i)$ expresses
our preference that the $i$-th data point can serve as an exemplar a priori of
other data points. 




AP has several useful properties that comport well with the TS track 
requirements. First, the number of clusters $k$ is not a hyper-parameter
of the model. Since we will be running this algorithm many times in per run,
the usual methods of hyperparameter search become infeasible 
(?). The number of clusters falls out of the algorithm
organically-- lower overall preference values will result in fewer clusters. 

Secondly, the arbitrary nature of the preferences
allow us to incorporate a variety of signals for
identifying the best exemplars, i.e. salience and redundancy signals. 
The preferences can be thought of as self-similarities (similarities that
pairwise-comparison based algorithms would ignore) that we can exploit to 
incorporate our prior beliefs about a data point.

Finally, cluster exemplars are guaranteed to be actual data points. Many 
clustering algorithms group data around mathematical objects (e.g., the
mean) that are not necessarily observed in the data. The extractive nature of
this TS task requires that we emit actual data points (i.e. sentences).
We are able to sidestep 
the additional requirement of selecting a most
representative cluster member, as this is computed explicitly in the AP 
algorithm. 

\subsubsection{Preferences}

Before rescaling, we first penalize each $\boldsymbol{\pi}_i$ based on
the aggregate similarity of input sentence $\mathbf{s}_i$ to the set of previous updates
$\mathbf{U}$. We call this the redundancy penalty 

$$\rho_i = \sum_j \frac{\boldsymbol{\pi}^{(U)}_{j}\operatorname{cos-sim}(\mathbf{s}_i, \mathbf{U}_j)}
{\sum_{j^\prime}\operatorname{cos-sim}(\mathbf{s}_i, \mathbf{U}_{j^\prime})} $$   
where $\boldsymbol{\pi}^{(U)}_{j}$ is the salience prediction of the $j$ 
previous update. We calculate our new penalized preferences 
$\boldsymbol{\pi}^{(\rho)}$ where $\boldsymbol{\pi}^{(\rho)}_i = \boldsymbol{\pi}_i - \rho_i$.
Finally, we rescale $\boldsymbol{\pi}^{(\rho)}$ such that all values lie 
within the range $(-3,-2)$.

